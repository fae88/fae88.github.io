{"pages":[{"title":"About Me","date":"2017-11-26T03:42:51.000Z","updated":"2018-10-02T12:59:31.889Z","comments":true,"path":"about/index.html","permalink":"http://www.fufan.me/about/index.html","excerpt":"","text":"I AM WHAT I AM ……"},{"title":"tags","date":"2017-11-16T09:08:20.000Z","updated":"2018-10-02T07:52:02.919Z","comments":true,"path":"tags/index.html","permalink":"http://www.fufan.me/tags/index.html","excerpt":"","text":""},{"title":"Categories","date":"2017-11-14T05:59:36.000Z","updated":"2018-10-02T07:50:02.801Z","comments":true,"path":"categories/index.html","permalink":"http://www.fufan.me/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"java多线程系列（九）—— JUC中的阻塞队列","slug":"java多线程系列（九）——-JUC中的阻塞队列","date":"2018-12-06T04:00:00.000Z","updated":"2018-12-06T04:02:23.008Z","comments":true,"path":"2018/12/06/java多线程系列（九）——-JUC中的阻塞队列/","link":"","permalink":"http://www.fufan.me/2018/12/06/java多线程系列（九）——-JUC中的阻塞队列/","excerpt":"","text":"0. 阻塞队列概要阻塞队列与我们平常接触的普通队列(LinkedList或ArrayList等)的最大不同点，在于阻塞队列支出阻塞添加和阻塞删除方法。阻塞添加所谓的阻塞添加是指当阻塞队列元素已满时，队列会阻塞加入元素的线程，直队列元素不满时才重新唤醒线程执行元素加入操作。阻塞删除阻塞删除是指在队列元素为空时，删除队列元素的线程将被阻塞，直到队列不为空再执行删除操作(一般都会返回被删除的元素)由于Java中的阻塞队列接口BlockingQueue继承自Queue接口，因此先来看看阻塞队列接口为我们提供的主要方法12345678910111213141516171819202122232425262728293031323334public interface BlockingQueue&lt;E&gt; extends Queue&lt;E&gt; &#123; //将指定的元素插入到此队列的尾部（如果立即可行且不会超过该队列的容量） //在成功时返回 true，如果此队列已满，则抛IllegalStateException。 boolean add(E e); //将指定的元素插入到此队列的尾部（如果立即可行且不会超过该队列的容量） // 将指定的元素插入此队列的尾部，如果该队列已满， //则在到达指定的等待时间之前等待可用的空间,该方法可中断 boolean offer(E e, long timeout, TimeUnit unit) throws InterruptedException; //将指定的元素插入此队列的尾部，如果该队列已满，则一直等到（阻塞）。 void put(E e) throws InterruptedException; //获取并移除此队列的头部，如果没有元素则等待（阻塞）， //直到有元素将唤醒等待线程执行该操作 E take() throws InterruptedException; //获取并移除此队列的头部，在指定的等待时间前一直等到获取元素， //超过时间方法将结束 E poll(long timeout, TimeUnit unit) throws InterruptedException; //从此队列中移除指定元素的单个实例（如果存在）。 boolean remove(Object o); &#125; //除了上述方法还有继承自Queue接口的方法 //获取但不移除此队列的头元素,没有则跑异常NoSuchElementException E element(); //获取但不移除此队列的头；如果此队列为空，则返回 null。 E peek(); //获取并移除此队列的头，如果此队列为空，则返回 null。 E poll();插入方法：add(E e) : 添加成功返回true，失败抛IllegalStateException异常offer(E e) : 成功返回 true，如果此队列已满，则返回 false。put(E e) :将元素插入此队列的尾部，如果该队列已满，则一直阻塞删除方法:remove(Object o) :移除指定元素,成功返回true，失败返回falsepoll() : 获取并移除此队列的头元素，若队列为空，则返回 nulltake()：获取并移除此队列头元素，若没有元素则一直阻塞。检查方法element() ：获取但不移除此队列的头元素，没有元素则抛异常peek() :获取但不移除此队列的头；若队列为空，则返回 null。这里主要介绍阻塞队列中的两个实现类ArrayBlockingQueue和LinkedBlockingQueue的简单使用和实现原理一. ArrayBlockingQueue1. 特点一个用数组实现的有界阻塞队列，其内部按先进先出的原则对元素进行排序2. 原理ArrayBlockingQueue的内部是通过一个可重入锁ReentrantLock和两个Condition条件对象来实现阻塞12345678910111213141516171819202122232425262728293031public class ArrayBlockingQueue&lt;E&gt; extends AbstractQueue&lt;E&gt; implements BlockingQueue&lt;E&gt;, java.io.Serializable &#123; /** 存储数据的数组 */ final Object[] items; /**获取数据的索引，主要用于take，poll，peek，remove方法 */ int takeIndex; /**添加数据的索引，主要用于 put, offer, or add 方法*/ int putIndex; /** 队列元素的个数 */ int count; /** 控制并非访问的锁 */ final ReentrantLock lock; /**notEmpty条件对象，用于通知take方法队列已有元素，可执行获取操作 */ private final Condition notEmpty; /**notFull条件对象，用于通知put方法队列未满，可执行添加操作 */ private final Condition notFull; /** 迭代器 */ transient Itrs itrs = null;&#125;元素的添加这里的add方法和offer方法实现比较简单，其中需要注意的是enqueue(E x)方法，其方法内部通过putIndex索引直接将元素添加到数组items中，这里可能会疑惑的是当putIndex索引大小等于数组长度时，需要将putIndex重新设置为0，这是因为当前队列执行元素获取时总是从队列头部获取，而添加元素从中从队列尾部获取所以当队列索引（从0开始）与数组长度相等时，下次我们就需要从数组头部开始添加了add、offer、put123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657//add方法实现，间接调用了offer(e)public boolean add(E e) &#123; if (offer(e)) return true; else throw new IllegalStateException(\"Queue full\"); &#125;//offer方法public boolean offer(E e) &#123; checkNotNull(e);//检查元素是否为null final ReentrantLock lock = this.lock; lock.lock();//加锁 try &#123; if (count == items.length)//判断队列是否满 return false; else &#123; enqueue(e);//添加元素到队列 return true; &#125; &#125; finally &#123; lock.unlock(); &#125; &#125;//入队操作private void enqueue(E x) &#123; //获取当前数组 final Object[] items = this.items; //通过putIndex索引对数组进行赋值 items[putIndex] = x; //索引自增，如果已是最后一个位置，重新设置 putIndex = 0; if (++putIndex == items.length) putIndex = 0; count++;//队列中元素数量加1 //唤醒调用take()方法的线程，执行元素获取操作。 notEmpty.signal();&#125;/**put方法是一个阻塞的方法，如果队列元素已满，那么当前线程将会被notFull条件对象挂起加到等待队列中，直到队列有空档才会唤醒执行添加操作。但如果队列没有满，那么就直接调用enqueue(e)方法将元素加入到数组队列中。到此我们对三个添加方法即put，offer，add都分析完毕，其中offer，add在正常情况下都是无阻塞的添加，而put方法是阻塞添加。这就是阻塞队列的添加过程。说白了就是当队列满时通过条件对象Condtion来阻塞当前调用put方法的线程，直到线程又再次被唤醒执行。总得来说添加线程的执行存在以下两种情况，一是，队列已满，那么新到来的put线程将添加到notFull的条件队列中等待，二是，有移除线程执行移除操作，移除成功同时唤醒put线程**///put方法，阻塞时可中断 public void put(E e) throws InterruptedException &#123; checkNotNull(e); final ReentrantLock lock = this.lock; lock.lockInterruptibly();//该方法可中断 try &#123; //当队列元素个数与数组长度相等时，无法添加元素 while (count == items.length) //将当前调用线程挂起，添加到notFull条件队列中等待唤醒 notFull.await(); enqueue(e);//如果队列没有满直接添加。。 &#125; finally &#123; lock.unlock(); &#125; &#125;移除元素poll、remove、peek123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150public E poll() &#123; final ReentrantLock lock = this.lock; lock.lock(); try &#123; //判断队列是否为null，不为null执行dequeue()方法，否则返回null return (count == 0) ? null : dequeue(); &#125; finally &#123; lock.unlock(); &#125; &#125; //删除队列头元素并返回 private E dequeue() &#123; //拿到当前数组的数据 final Object[] items = this.items; @SuppressWarnings(\"unchecked\") //获取要删除的对象 E x = (E) items[takeIndex]; 将数组中takeIndex索引位置设置为null items[takeIndex] = null; //takeIndex索引加1并判断是否与数组长度相等， //如果相等说明已到尽头，恢复为0 if (++takeIndex == items.length) takeIndex = 0; count--;//队列个数减1 if (itrs != null) itrs.elementDequeued();//同时更新迭代器中的元素数据 //删除了元素说明队列有空位，唤醒notFull条件对象添加线程，执行添加操作 notFull.signal(); return x; &#125; /**poll()，获取并删除队列头元素，队列没有数据就返回null，内部通过dequeue()方法删除头元素，注释很清晰，这里不重复了。接着看remove(Object o)方法**/public boolean remove(Object o) &#123; if (o == null) return false; //获取数组数据 final Object[] items = this.items; final ReentrantLock lock = this.lock; lock.lock();//加锁 try &#123; //如果此时队列不为null，这里是为了防止并发情况 if (count &gt; 0) &#123; //获取下一个要添加元素时的索引 final int putIndex = this.putIndex; //获取当前要被删除元素的索引 int i = takeIndex; //执行循环查找要删除的元素 do &#123; //找到要删除的元素 if (o.equals(items[i])) &#123; removeAt(i);//执行删除 return true;//删除成功返回true &#125; //当前删除索引执行加1后判断是否与数组长度相等 //若为true，说明索引已到数组尽头，将i设置为0 if (++i == items.length) i = 0; &#125; while (i != putIndex);//继承查找 &#125; return false; &#125; finally &#123; lock.unlock(); &#125;&#125;//根据索引删除元素，实际上是把删除索引之后的元素往前移动一个位置void removeAt(final int removeIndex) &#123; final Object[] items = this.items; //先判断要删除的元素是否为当前队列头元素 if (removeIndex == takeIndex) &#123; //如果是直接删除 items[takeIndex] = null; //当前队列头元素加1并判断是否与数组长度相等，若为true设置为0 if (++takeIndex == items.length) takeIndex = 0; count--;//队列元素减1 if (itrs != null) itrs.elementDequeued();//更新迭代器中的数据 &#125; else &#123; //如果要删除的元素不在队列头部， //那么只需循环迭代把删除元素后面的所有元素往前移动一个位置 //获取下一个要被添加的元素的索引，作为循环判断结束条件 final int putIndex = this.putIndex; //执行循环 for (int i = removeIndex;;) &#123; //获取要删除节点索引的下一个索引 int next = i + 1; //判断是否已为数组长度，如果是从数组头部（索引为0）开始找 if (next == items.length) next = 0; //如果查找的索引不等于要添加元素的索引，说明元素可以再移动 if (next != putIndex) &#123; items[i] = items[next];//把后一个元素前移覆盖要删除的元 i = next; &#125; else &#123; //在removeIndex索引之后的元素都往前移动完毕后清空最后一个元素 items[i] = null; this.putIndex = i; break;//结束循环 &#125; &#125; count--;//队列元素减1 if (itrs != null) itrs.removedAt(removeIndex);//更新迭代器数据 &#125; notFull.signal();//唤醒添加线程 &#125; /**remove(Object o)方法的删除过程相对复杂些，因为该方法并不是直接从队列头部删除元素。首先线程先获取锁，再一步判断队列count&gt;0,这点是保证并发情况下删除操作安全执行。接着获取下一个要添加源的索引putIndex以及takeIndex索引 ，作为后续循环的结束判断，因为只要putIndex与takeIndex不相等就说明队列没有结束。然后通过while循环找到要删除的元素索引，执行removeAt(i)方法删除，在removeAt(i)方法中实际上做了两件事，一是首先判断队列头部元素是否为删除元素，如果是直接删除，并唤醒添加线程，二是如果要删除的元素并不是队列头元素，那么执行循环操作，从要删除元素的索引removeIndex之后的元素都往前移动一个位置，那么要删除的元素就被removeIndex之后的元素替换，从而也就完成了删除操作。接着看take()方法，是一个阻塞方法，直接获取队列头元素并删除。**///从队列头部删除，队列没有元素就阻塞，可中断 public E take() throws InterruptedException &#123; final ReentrantLock lock = this.lock; lock.lockInterruptibly();//中断 try &#123; //如果队列没有元素 while (count == 0) //执行阻塞操作 notEmpty.await(); return dequeue();//如果队列有元素执行删除操作 &#125; finally &#123; lock.unlock(); &#125; &#125;/**peek方法非常简单，直接返回当前队列的头元素但不删除任何元素**/public E peek() &#123; final ReentrantLock lock = this.lock; lock.lock(); try &#123; //直接返回当前队列的头元素，但不删除 return itemAt(takeIndex); // null when queue is empty &#125; finally &#123; lock.unlock(); &#125; &#125;final E itemAt(int i) &#123; return (E) items[i]; &#125;二. LinkedBlockingQueue1. 特点是一个由链表实现的有界队列阻塞队列，但大小默认值为Integer.MAX_VALUE，如果存在添加速度大于删除速度时候，有可能会内存溢出，所以我们在使用LinkedBlockingQueue时建议手动传值，为其提供我们所需的大小，避免队列过大造成机器负载或者内存爆满等情况。(和前者最大的不同是他通过插入和弹出的分离锁方式来提高吞吐量，即添加和删除操作并不是互斥操作，可以同时进行）2. 原理1234567891011121314151617181920212223242526272829303132//默认大小为Integer.MAX_VALUEpublic LinkedBlockingQueue() &#123; this(Integer.MAX_VALUE);&#125;//创建指定大小为capacity的阻塞队列public LinkedBlockingQueue(int capacity) &#123; if (capacity &lt;= 0) throw new IllegalArgumentException(); this.capacity = capacity; last = head = new Node&lt;E&gt;(null); &#125;//创建大小默认值为Integer.MAX_VALUE的阻塞队列并添加c中的元素到阻塞队列public LinkedBlockingQueue(Collection&lt;? extends E&gt; c) &#123; this(Integer.MAX_VALUE); final ReentrantLock putLock = this.putLock; putLock.lock(); // Never contended, but necessary for visibility try &#123; int n = 0; for (E e : c) &#123; if (e == null) throw new NullPointerException(); if (n == capacity) throw new IllegalStateException(\"Queue full\"); enqueue(new Node&lt;E&gt;(e)); ++n; &#125; count.set(n); &#125; finally &#123; putLock.unlock(); &#125; &#125;从源码看，有三种方式可以构造LinkedBlockingQueue，通常情况下，我们建议创建指定大小的LinkedBlockingQueue阻塞队列。LinkedBlockingQueue队列也是按 FIFO（先进先出）排序元素。队列的头部是在队列中时间最长的元素，队列的尾部 是在队列中时间最短的元素，新元素插入到队列的尾部，而队列执行获取操作会获得位于队列头部的元素。在正常情况下，链接队列的吞吐量要高于基于数组的队列（ArrayBlockingQueue），因为其内部实现添加和删除操作使用的两个ReenterLock来控制并发执行，而ArrayBlockingQueue内部只是使用一个ReenterLock控制并发，因此LinkedBlockingQueue的吞吐量要高于ArrayBlockingQueue。注意LinkedBlockingQueue和ArrayBlockingQueue的API几乎是一样的，但它们的内部实现原理不太相同，这点稍后会分析。使用LinkedBlockingQueue，我们同样也能实现生产者消费者模式。只需把前面ArrayBlockingQueue案例中的阻塞队列对象换成LinkedBlockingQueue即可。这里限于篇幅就不贴重复代码了。接下来我们重点分析LinkedBlockingQueue的内部实现原理，最后我们将对ArrayBlockingQueue和LinkedBlockingQueue 做总结，阐明它们间的不同之处。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class LinkedBlockingQueue&lt;E&gt; extends AbstractQueue&lt;E&gt; implements BlockingQueue&lt;E&gt;, java.io.Serializable &#123; /** * 节点类，用于存储数据 */ static class Node&lt;E&gt; &#123; E item; /** * One of: * - the real successor Node * - this Node, meaning the successor is head.next * - null, meaning there is no successor (this is the last node) */ Node&lt;E&gt; next; Node(E x) &#123; item = x; &#125; &#125; /** 阻塞队列的大小，默认为Integer.MAX_VALUE */ private final int capacity; /** 当前阻塞队列中的元素个数 */ private final AtomicInteger count = new AtomicInteger(); /** * 阻塞队列的头结点 */ transient Node&lt;E&gt; head; /** * 阻塞队列的尾节点 */ private transient Node&lt;E&gt; last; /** 获取并移除元素时使用的锁，如take, poll, etc */ private final ReentrantLock takeLock = new ReentrantLock(); /** notEmpty条件对象，当队列没有数据时用于挂起执行删除的线程 */ private final Condition notEmpty = takeLock.newCondition(); /** 添加元素时使用的锁如 put, offer, etc */ private final ReentrantLock putLock = new ReentrantLock(); /** notFull条件对象，当队列数据已满时用于挂起执行添加的线程 */ private final Condition notFull = putLock.newCondition();&#125;从上述可看成，每个添加到LinkedBlockingQueue队列中的数据都将被封装成Node节点，添加的链表队列中，其中head和last分别指向队列的头结点和尾结点。与ArrayBlockingQueue不同的是，LinkedBlockingQueue内部分别使用了takeLock 和 putLock 对并发进行控制，也就是说，添加和删除操作并不是互斥操作，可以同时进行，这样也就可以大大提高吞吐量。这里再次强调如果没有给LinkedBlockingQueue指定容量大小，其默认值将是Integer.MAX_VALUE，如果存在添加速度大于删除速度时候，有可能会内存溢出，这点在使用前希望慎重考虑。至于LinkedBlockingQueue的实现原理图与ArrayBlockingQueue是类似的，除了对添加和移除方法使用单独的锁控制外，两者都使用了不同的Condition条件对象作为等待队列，用于挂起take线程和put线程。添加元素add、offer、put123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/**add方法间接调用的是offer方法，如果add方法添加失败将抛出IllegalStateException异常，添加成功则返回true**/public boolean add(E e) &#123; if (offer(e)) return true; else throw new IllegalStateException(\"Queue full\");&#125;public boolean offer(E e) &#123; //添加元素为null直接抛出异常 if (e == null) throw new NullPointerException(); //获取队列的个数 final AtomicInteger count = this.count; //判断队列是否已满 if (count.get() == capacity) return false; int c = -1; //构建节点 Node&lt;E&gt; node = new Node&lt;E&gt;(e); final ReentrantLock putLock = this.putLock; putLock.lock(); try &#123; //再次判断队列是否已满，考虑并发情况 if (count.get() &lt; capacity) &#123; enqueue(node);//添加元素 c = count.getAndIncrement();//拿到当前未添加新元素时的队列长度 //如果容量还没满 if (c + 1 &lt; capacity) notFull.signal();//唤醒下一个添加线程，执行添加操作 &#125; &#125; finally &#123; putLock.unlock(); &#125; // 由于存在添加锁和消费锁，而消费锁和添加锁都会持续唤醒等到线程，因此count肯定会变化。 //这里的if条件表示如果队列中还有1条数据 if (c == 0) signalNotEmpty();//如果还存在数据那么就唤醒消费锁 return c &gt;= 0; // 添加成功返回true，否则返回false &#125;//入队操作private void enqueue(Node&lt;E&gt; node) &#123; //队列尾节点指向新的node节点 last = last.next = node;&#125;//signalNotEmpty方法private void signalNotEmpty() &#123; final ReentrantLock takeLock = this.takeLock; takeLock.lock(); //唤醒获取并删除元素的线程 notEmpty.signal(); &#125; finally &#123; takeLock.unlock(); &#125; &#125;这里的Offer()方法做了两件事，第一件事是判断队列是否满，满了就直接释放锁，没满就将节点封装成Node入队，然后再次判断队列添加完成后是否已满，不满就继续唤醒等到在条件对象notFull上的添加线程。第二件事是，判断是否需要唤醒等到在notEmpty条件对象上的消费线程。这里我们可能会有点疑惑，为什么添加完成后是继续唤醒在条件对象notFull上的添加线程而不是像ArrayBlockingQueue那样直接唤醒notEmpty条件对象上的消费线程？而又为什么要当if (c == 0)时才去唤醒消费线程呢？唤醒添加线程的原因，在添加新元素完成后，会判断队列是否已满，不满就继续唤醒在条件对象notFull上的添加线程，这点与前面分析的ArrayBlockingQueue很不相同，在ArrayBlockingQueue内部完成添加操作后，会直接唤醒消费线程对元素进行获取，这是因为ArrayBlockingQueue只用了一个ReenterLock同时对添加线程和消费线程进行控制，这样如果在添加完成后再次唤醒添加线程的话，消费线程可能永远无法执行，而对于LinkedBlockingQueue来说就不一样了，其内部对添加线程和消费线程分别使用了各自的ReenterLock锁对并发进行控制，也就是说添加线程和消费线程是不会互斥的，所以添加锁只要管好自己的添加线程即可，添加线程自己直接唤醒自己的其他添加线程，如果没有等待的添加线程，直接结束了。如果有就直到队列元素已满才结束挂起，当然offer方法并不会挂起，而是直接结束，只有put方法才会当队列满时才执行挂起操作。注意消费线程的执行过程也是如此。这也是为什么LinkedBlockingQueue的吞吐量要相对大些的原因。为什么要判断if (c == 0)时才去唤醒消费线程呢，这是因为消费线程一旦被唤醒是一直在消费的（前提是有数据），所以c值是一直在变化的，c值是添加完元素前队列的大小，此时c只可能是0或c&gt;0，如果是c=0，那么说明之前消费线程已停止，条件对象上可能存在等待的消费线程，添加完数据后应该是c+1，那么有数据就直接唤醒等待消费线程，如果没有就结束啦，等待下一次的消费操作。如果c&gt;0那么消费线程就不会被唤醒，只能等待下一个消费操作（poll、take、remove）的调用，那为什么不是条件c&gt;0才去唤醒呢？我们要明白的是消费线程一旦被唤醒会和添加线程一样，一直不断唤醒其他消费线程，如果添加前c&gt;0，那么很可能上一次调用的消费线程后，数据并没有被消费完，条件队列上也就不存在等待的消费线程了，所以c&gt;0唤醒消费线程得意义不是很大，当然如果添加线程一直添加元素，那么一直c&gt;0，消费线程执行的换就要等待下一次调用消费操作了（poll、take、remove）。移除元素remove、poll、take123456789101112131415161718192021222324252627282930313233/**remove方法删除指定的对象，这里我们可能会诧异，为什么同时对putLock和takeLock加锁？这是因为remove方法删除的数据的位置不确定，为了避免造成并非安全问题，所以需要对2个锁同时加锁。**/public boolean remove(Object o) &#123; if (o == null) return false; fullyLock();//同时对putLock和takeLock加锁 try &#123; //循环查找要删除的元素 for (Node&lt;E&gt; trail = head, p = trail.next; p != null; trail = p, p = p.next) &#123; if (o.equals(p.item)) &#123;//找到要删除的节点 unlink(p, trail);//直接删除 return true; &#125; &#125; return false; &#125; finally &#123; fullyUnlock();//解锁 &#125; &#125;//两个同时加锁void fullyLock() &#123; putLock.lock(); takeLock.lock(); &#125;void fullyUnlock() &#123; takeLock.unlock(); putLock.unlock(); &#125;123456789101112131415161718192021222324252627282930313233343536373839404142/**poll方法也比较简单，如果队列没有数据就返回null，如果队列有数据，那么就取出来，如果队列还有数据那么唤醒等待在条件对象notEmpty上的消费线程。然后判断if (c == capacity)为true就唤醒添加线程，这点与前面分析if(c==0)是一样的道理。因为只有可能队列满了，notFull条件对象上才可能存在等待的添加线程。**/public E poll() &#123; //获取当前队列的大小 final AtomicInteger count = this.count; if (count.get() == 0)//如果没有元素直接返回null return null; E x = null; int c = -1; final ReentrantLock takeLock = this.takeLock; takeLock.lock(); try &#123; //判断队列是否有数据 if (count.get() &gt; 0) &#123; //如果有，直接删除并获取该元素值 x = dequeue(); //当前队列大小减一 c = count.getAndDecrement(); //如果队列未空，继续唤醒等待在条件对象notEmpty上的消费线程 if (c &gt; 1) notEmpty.signal(); &#125; &#125; finally &#123; takeLock.unlock(); &#125; //判断c是否等于capacity，这是因为如果满说明NotFull条件对象上 //可能存在等待的添加线程 if (c == capacity) signalNotFull(); return x; &#125; private E dequeue() &#123; Node&lt;E&gt; h = head;//获取头结点 Node&lt;E&gt; first = h.next; 获取头结的下一个节点（要删除的节点） h.next = h; // help GC//自己next指向自己，即被删除 head = first;//更新头结点 E x = first.item;//获取删除节点的值 first.item = null;//清空数据，因为first变成头结点是不能带数据的，这样也就删除队列的带数据的第一个节点 return x; &#125;12345678910111213141516171819202122232425262728/**take方法是一个可阻塞可中断的移除方法，主要做了两件事，一是，如果队列没有数据就挂起当前线程到 notEmpty条件对象的等待队列中一直等待，如果有数据就删除节点并返回数据项，同时唤醒后续消费线程，二是尝试唤醒条件对象notFull上等待队列中的添加线程。 **/public E take() throws InterruptedException &#123; E x; int c = -1; //获取当前队列大小 final AtomicInteger count = this.count; final ReentrantLock takeLock = this.takeLock; takeLock.lockInterruptibly();//可中断 try &#123; //如果队列没有数据，挂机当前线程到条件对象的等待队列中 while (count.get() == 0) &#123; notEmpty.await(); &#125; //如果存在数据直接删除并返回该数据 x = dequeue(); c = count.getAndDecrement();//队列大小减1 if (c &gt; 1) notEmpty.signal();//还有数据就唤醒后续的消费线程 &#125; finally &#123; takeLock.unlock(); &#125; //满足条件，唤醒条件对象上等待队列中的添加线程 if (c == capacity) signalNotFull(); return x; &#125;peek、element到此关于remove、poll、take的实现也分析完了，其中只有take方法具备阻塞功能。remove方法则是成功返回true失败返回false，poll方法成功返回被移除的值，失败或没数据返回null。下面再看看两个检查方法，即peek和element12345678910111213141516171819202122232425262728293031//构造方法，head 节点不存放数据 public LinkedBlockingQueue(int capacity) &#123; if (capacity &lt;= 0) throw new IllegalArgumentException(); this.capacity = capacity; last = head = new Node&lt;E&gt;(null); &#125; public E element() &#123; E x = peek();//直接调用peek if (x != null) return x; else throw new NoSuchElementException();//没数据抛异常 &#125; public E peek() &#123; if (count.get() == 0) return null; final ReentrantLock takeLock = this.takeLock; takeLock.lock(); try &#123; //获取头结节点的下一个节点 Node&lt;E&gt; first = head.next; if (first == null) return null;//为null就返回null else return first.item;//返回值 &#125; finally &#123; takeLock.unlock(); &#125; &#125;从代码来看，head头结节点在初始化时是本身不带数据的，仅仅作为头部head方便我们执行链表的相关操作。peek返回直接获取头结点的下一个节点返回其值，如果没有值就返回null，有值就返回节点对应的值。element方法内部调用的是peek，有数据就返回，没数据就抛异常。下面我们最后来看两个根据时间阻塞的方法，比较有意思，利用的Conditin来实现的。offer(E e, long timeout, TimeUnit unit)方法内部正是利用这样的Codition的超时等待awaitNanos方法实现添加方法的超时阻塞操作。同样对于poll(long timeout, TimeUnit unit)方法也是一样的道理。3. LinkedBlockingQueue和ArrayBlockingQueue的异同1.队列大小有所不同，ArrayBlockingQueue是有界的初始化必须指定大小，而LinkedBlockingQueue可以是有界的也可以是无界的(Integer.MAX_VALUE)，对于后者而言，当添加速度大于移除速度时，在无界的情况下，可能会造成内存溢出等问题。2.数据存储容器不同，ArrayBlockingQueue采用的是数组作为数据存储容器，而LinkedBlockingQueue采用的则是以Node节点作为连接对象的链表。3.由于ArrayBlockingQueue采用的是数组的存储容器，因此在插入或删除元素时不会产生或销毁任何额外的对象实例，而LinkedBlockingQueue则会生成一个额外的Node对象。这可能在长时间内需要高效并发地处理大批量数据的时，对于GC可能存在较大影响。4.两者的实现队列添加或移除的锁不一样，ArrayBlockingQueue实现的队列中的锁是没有分离的，即添加操作和移除操作采用的同一个ReenterLock锁，而LinkedBlockingQueue实现的队列中的锁是分离的，其添加采用的是putLock，移除采用的则是takeLock，这样能大大提高队列的吞吐量，也意味着在高并发的情况下生产者和消费者可以并行地操作队列中的数据，以此来提高整个队列的并发性能。参考博客深入剖析java并发之阻塞队列LinkedBlockingQueue与ArrayBlockingQueue","categories":[{"name":"多线程","slug":"多线程","permalink":"http://www.fufan.me/categories/多线程/"}],"tags":[{"name":"多线程","slug":"多线程","permalink":"http://www.fufan.me/tags/多线程/"}]},{"title":"java多线程系列（八）——执行器Executor并发框架","slug":"java多线程系列（八）——执行器Executor并发框架","date":"2018-12-04T03:53:00.000Z","updated":"2018-12-05T03:54:22.143Z","comments":true,"path":"2018/12/04/java多线程系列（八）——执行器Executor并发框架/","link":"","permalink":"http://www.fufan.me/2018/12/04/java多线程系列（八）——执行器Executor并发框架/","excerpt":"","text":"线程池的架构图如下：1. Executor它是”执行者”接口，它是来执行任务的。准确的说，Executor提供了execute()接口来执行已提交的 Runnable 任务的对象。Executor存在的目的是提供一种将”任务提交”与”任务如何运行”分离开来的机制。它只包含一个函数接口：2. ExecutorServicexecutorService继承于Executor。它是”执行者服务”接口，它是为”执行者接口Executor”服务而存在的；准确的话，ExecutorService提供了”将任务提交给执行者的接口(submit方法)”，”让执行者执行任务(invokeAll, invokeAny方法)”的接口等等。3. AbstractExecutorServiceAbstractExecutorService是一个抽象类，它实现了ExecutorService接口。AbstractExecutorService存在的目的是为ExecutorService中的函数接口提供了默认实现。其实基本上抽象方法的作用一般也就是为了提供默认的实现，不过我们在jdk1.8开始，可以使用接口的默认方法，即default关键字。4. ThreadPoolExecutorThreadPoolExecutor就是大名鼎鼎的”线程池”。它继承于AbstractExecutorService抽象类。构造方法的核心参数corePoolSize：核心线程数量，当有新任务在execute()方法提交时，会执行以下判断：如果运行的线程少于 corePoolSize，则创建新线程来处理任务，即使线程池中的其他线程是空闲的；如果线程池中的线程数量大于等于 corePoolSize 且小于 maximumPoolSize，则只有当workQueue满时才创建新的线程去处理任务；如果设置的corePoolSize 和 maximumPoolSize相同，则创建的线程池的大小是固定的，这时如果有新任务提交，若workQueue未满，则将请求放入workQueue中，等待有空闲的线程去从workQueue中取任务并处理；如果运行的线程数量大于等于maximumPoolSize，这时如果workQueue已经满了，则通过handler所指定的策略来处理任务；所以，任务提交时，判断的顺序为 corePoolSize –&gt; workQueue –&gt; maximumPoolSize。maximumPoolSize：最大线程数量；workQueue：等待队列，当任务提交时，如果线程池中的线程数量大于等于corePoolSize的时候，把该任务封装成一个Worker对象放入等待队列；workQueue：保存等待执行的任务的阻塞队列，当提交一个新的任务到线程池以后, 线程池会根据当前线程池中正在运行着的线程的数量来决定对该任务的处理方式，主要有以下几种处理方式:直接切换：这种方式常用的队列是SynchronousQueue，但现在还没有研究过该队列，这里暂时还没法介绍；使用无界队列：一般使用基于链表的阻塞队列LinkedBlockingQueue。如果使用这种方式，那么线程池中能够创建的最大线程数就是corePoolSize，而maximumPoolSize就不会起作用了（后面也会说到）。当线程池中所有的核心线程都是RUNNING状态时，这时一个新的任务提交就会放入等待队列中。使用有界队列：一般使用ArrayBlockingQueue。使用该方式可以将线程池的最大线程数量限制为maximumPoolSize，这样能够降低资源的消耗，但同时这种方式也使得线程池对线程的调度变得更困难，因为线程池和队列的容量都是有限的值，所以要想使线程池处理任务的吞吐率达到一个相对合理的范围，又想使线程调度相对简单，并且还要尽可能的降低线程池对资源的消耗，就需要合理的设置这两个数量。如果要想降低系统资源的消耗（包括CPU的使用率，操作系统资源的消耗，上下文环境切换的开销等）, 可以设置较大的队列容量和较小的线程池容量, 但这样也会降低线程处理任务的吞吐量。如果提交的任务经常发生阻塞，那么可以考虑通过调用 setMaximumPoolSize() 方法来重新设定线程池的容量。如果队列的容量设置的较小，通常需要将线程池的容量设置大一点，这样CPU的使用率会相对的高一些。但如果线程池的容量设置的过大，则在提交的任务数量太多的情况下，并发量会增加，那么线程之间的调度就是一个要考虑的问题，因为这样反而有可能降低处理任务的吞吐量。keepAliveTime：线程池维护线程所允许的空闲时间。当线程池中的线程数量大于corePoolSize的时候，如果这时没有新的任务提交，核心线程外的线程不会立即销毁，而是会等待，直到等待的时间超过了keepAliveTime；threadFactory：它是ThreadFactory类型的变量，用来创建新线程。默认使用Executors.defaultThreadFactory() 来创建线程。使用默认的ThreadFactory来创建线程时，会使新创建的线程具有相同的NORM_PRIORITY优先级并且是非守护线程，同时也设置了线程的名称。handler：它是RejectedExecutionHandler类型的变量，表示线程池的饱和策略。如果阻塞队列满了并且没有空闲的线程，这时如果继续提交任务，就需要采取一种策略处理该任务。线程池提供了4种策略：AbortPolicy：直接抛出异常，这是默认策略；CallerRunsPolicy：用调用者所在的线程来执行任务；DiscardOldestPolicy：丢弃阻塞队列中靠最前的任务，并执行当前任务；DiscardPolicy：直接丢弃任务；execute()execute()方法用来提交任务，代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public void execute(Runnable command) &#123; if (command == null) throw new NullPointerException(); /* * clt记录着runState和workerCount */ int c = ctl.get(); /* * workerCountOf方法取出低29位的值，表示当前活动的线程数； * 如果当前活动线程数小于corePoolSize，则新建一个线程放入线程池中； * 并把任务添加到该线程中。 */ if (workerCountOf(c) &lt; corePoolSize) &#123; /* * addWorker中的第二个参数表示限制添加线程的数量是根据corePoolSize来判断还是maximumPoolSize来判断； * 如果为true，根据corePoolSize来判断； * 如果为false，则根据maximumPoolSize来判断 */ if (addWorker(command, true)) return; /* * 如果添加失败，则重新获取ctl值 */ c = ctl.get(); &#125; /* * 如果当前线程池是运行状态并且任务添加到队列成功 */ if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; // 重新获取ctl值 int recheck = ctl.get(); // 再次判断线程池的运行状态，如果不是运行状态，由于之前已经把command添加到workQueue中了， // 这时需要移除该command // 执行过后通过handler使用拒绝策略对该任务进行处理，整个方法返回 if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); /* * 获取线程池中的有效线程数，如果数量是0，则执行addWorker方法 * 这里传入的参数表示： * 1. 第一个参数为null，表示在线程池中创建一个线程，但不去启动； * 2. 第二个参数为false，将线程池的有限线程数量的上限设置为maximumPoolSize，添加线程时根据maximumPoolSize来判断； * 如果判断workerCount大于0，则直接返回，在workQueue中新增的command会在将来的某个时刻被执行。 */ else if (workerCountOf(recheck) == 0) addWorker(null, false); &#125; /* * 如果执行到这里，有两种情况： * 1. 线程池已经不是RUNNING状态； * 2. 线程池是RUNNING状态，但workerCount &gt;= corePoolSize并且workQueue已满。 * 这时，再次调用addWorker方法，但第二个参数传入为false，将线程池的有限线程数量的上限设置为maximumPoolSize； * 如果失败则拒绝该任务 */ else if (!addWorker(command, false)) reject(command);&#125;简单来说，在执行execute()方法时如果状态一直是RUNNING时，的执行过程如下：如果workerCount &lt; corePoolSize，则创建并启动一个线程来执行新提交的任务；如果workerCount &gt;= corePoolSize，且线程池内的阻塞队列未满，则将任务添加到该阻塞队列中；如果workerCount &gt;= corePoolSize &amp;&amp; workerCount &lt; maximumPoolSize，且线程池内的阻塞队列已满，则创建并启动一个线程来执行新提交的任务；如果workerCount &gt;= maximumPoolSize，并且线程池内的阻塞队列已满, 则根据拒绝策略来处理该任务, 默认的处理方式是直接抛异常。这里要注意一下addWorker(null, false);，也就是创建一个线程，但并没有传入任务，因为任务已经被添加到workQueue中了，所以worker在执行的时候，会直接从workQueue中获取任务。所以，在workerCountOf(recheck) == 0时执行addWorker(null, false);也是为了保证线程池在RUNNING状态下必须要有一个线程来执行任务。execute方法执行流程如下：addWorker方法addWorker方法的主要工作是在线程池中创建一个新的线程并执行，firstTask参数 用于指定新增的线程执行的第一个任务，core参数为true表示在新增线程时会判断当前活动线程数是否少于corePoolSize，false表示新增线程前需要判断当前活动线程数是否少于maximumPoolSize123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293private boolean addWorker(Runnable firstTask, boolean core) &#123; retry: for (;;) &#123; int c = ctl.get(); // 获取运行状态 int rs = runStateOf(c); /* * 这个if判断 * 如果rs &gt;= SHUTDOWN，则表示此时不再接收新任务； * 接着判断以下3个条件，只要有1个不满足，则返回false： * 1. rs == SHUTDOWN，这时表示关闭状态，不再接受新提交的任务，但却可以继续处理阻塞队列中已保存的任务 * 2. firsTask为空 * 3. 阻塞队列不为空 * * 首先考虑rs == SHUTDOWN的情况 * 这种情况下不会接受新提交的任务，所以在firstTask不为空的时候会返回false； * 然后，如果firstTask为空，并且workQueue也为空，则返回false， * 因为队列中已经没有任务了，不需要再添加线程了 */ // Check if queue empty only if necessary. if (rs &gt;= SHUTDOWN &amp;&amp; ! (rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp; ! workQueue.isEmpty())) return false; for (;;) &#123; // 获取线程数 int wc = workerCountOf(c); // 如果wc超过CAPACITY，也就是ctl的低29位的最大值（二进制是29个1），返回false； // 这里的core是addWorker方法的第二个参数，如果为true表示根据corePoolSize来比较， // 如果为false则根据maximumPoolSize来比较。 // if (wc &gt;= CAPACITY || wc &gt;= (core ? corePoolSize : maximumPoolSize)) return false; // 尝试增加workerCount，如果成功，则跳出第一个for循环 if (compareAndIncrementWorkerCount(c)) break retry; // 如果增加workerCount失败，则重新获取ctl的值 c = ctl.get(); // Re-read ctl // 如果当前的运行状态不等于rs，说明状态已被改变，返回第一个for循环继续执行 if (runStateOf(c) != rs) continue retry; // else CAS failed due to workerCount change; retry inner loop &#125; &#125; boolean workerStarted = false; boolean workerAdded = false; Worker w = null; try &#123; // 根据firstTask来创建Worker对象 w = new Worker(firstTask); // 每一个Worker对象都会创建一个线程 final Thread t = w.thread; if (t != null) &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; // Recheck while holding lock. // Back out on ThreadFactory failure or if // shut down before lock acquired. int rs = runStateOf(ctl.get()); // rs &lt; SHUTDOWN表示是RUNNING状态； // 如果rs是RUNNING状态或者rs是SHUTDOWN状态并且firstTask为null，向线程池中添加线程。 // 因为在SHUTDOWN时不会在添加新的任务，但还是会执行workQueue中的任务 if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == null)) &#123; if (t.isAlive()) // precheck that t is startable throw new IllegalThreadStateException(); // workers是一个HashSet workers.add(w); int s = workers.size(); // largestPoolSize记录着线程池中出现过的最大线程数量 if (s &gt; largestPoolSize) largestPoolSize = s; workerAdded = true; &#125; &#125; finally &#123; mainLock.unlock(); &#125; if (workerAdded) &#123; // 启动线程 t.start(); workerStarted = true; &#125; &#125; &#125; finally &#123; if (! workerStarted) addWorkerFailed(w); &#125; return workerStarted;&#125;注意一下这里的t.start()这个语句，启动时会调用Worker类中的run方法，Worker本身实现了Runnable接口，所以一个Worker类型的对象也是一个线程。Worker类线程池中的每一个线程被封装成一个Worker对象，ThreadPool维护的其实就是一组Worker对象，看一下Worker的定义：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647final void runWorker(Worker w) &#123; Thread wt = Thread.currentThread(); // 获取第一个任务 Runnable task = w.firstTask; w.firstTask = null; // 允许中断 w.unlock(); // allow interrupts // 是否因为异常退出循环 boolean completedAbruptly = true; try &#123; // 如果task为空，则通过getTask来获取任务 while (task != null || (task = getTask()) != null) &#123; w.lock(); // If pool is stopping, ensure thread is interrupted; // if not, ensure thread is not interrupted. This // requires a recheck in second case to deal with // shutdownNow race while clearing interrupt if ((runStateAtLeast(ctl.get(), STOP) || (Thread.interrupted() &amp;&amp; runStateAtLeast(ctl.get(), STOP))) &amp;&amp; !wt.isInterrupted()) wt.interrupt(); try &#123; beforeExecute(wt, task); Throwable thrown = null; try &#123; task.run(); &#125; catch (RuntimeException x) &#123; thrown = x; throw x; &#125; catch (Error x) &#123; thrown = x; throw x; &#125; catch (Throwable x) &#123; thrown = x; throw new Error(x); &#125; finally &#123; afterExecute(task, thrown); &#125; &#125; finally &#123; task = null; w.completedTasks++; w.unlock(); &#125; &#125; completedAbruptly = false; &#125; finally &#123; processWorkerExit(w, completedAbruptly); &#125;&#125;这里说明一下第一个if判断，目的是：如果线程池正在停止，那么要保证当前线程是中断状态；如果不是的话，则要保证当前线程不是中断状态；这里要考虑在执行该if语句期间可能也执行了shutdownNow方法，shutdownNow方法会把状态设置为STOP，回顾一下STOP状态：不能接受新任务，也不处理队列中的任务，会中断正在处理任务的线程。在线程池处于 RUNNING 或 SHUTDOWN 状态时，调用 shutdownNow() 方法会使线程池进入到该状态。 STOP状态要中断线程池中的所有线程，而这里使用Thread.interrupted()来判断是否中断是为了确保在RUNNING或者SHUTDOWN状态时线程是非中断状态的，因为Thread.interrupted()方法会复位中断的状态。总结一下runWorker方法的执行过程：while循环不断地通过getTask()方法获取任务；getTask()方法从阻塞队列中取任务；如果线程池正在停止，那么要保证当前线程是中断状态，否则要保证当前线程不是中断状态；调用task.run()执行任务；如果task为null则跳出循环，执行processWorkerExit()方法；runWorker方法执行完毕，也代表着Worker中的run方法执行完毕，销毁线程。这里的beforeExecute方法和afterExecute方法在ThreadPoolExecutor类中是空的，留给子类来实现。completedAbruptly变量来表示在执行任务过程中是否出现了异常，在processWorkerExit方法中会对该变量的值进行判断。getTaskgetTask方法用来从阻塞队列中取任务，代码如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758private Runnable getTask() &#123; // timeOut变量的值表示上次从阻塞队列中取任务时是否超时 boolean timedOut = false; // Did the last poll() time out? for (;;) &#123; int c = ctl.get(); int rs = runStateOf(c); // Check if queue empty only if necessary. /* * 如果线程池状态rs &gt;= SHUTDOWN，也就是非RUNNING状态，再进行以下判断： * 1. rs &gt;= STOP，线程池是否正在stop； * 2. 阻塞队列是否为空。 * 如果以上条件满足，则将workerCount减1并返回null。 * 因为如果当前线程池状态的值是SHUTDOWN或以上时，不允许再向阻塞队列中添加任务。 */ if (rs &gt;= SHUTDOWN &amp;&amp; (rs &gt;= STOP || workQueue.isEmpty())) &#123; decrementWorkerCount(); return null; &#125; int wc = workerCountOf(c); // Are workers subject to culling? // timed变量用于判断是否需要进行超时控制。 // allowCoreThreadTimeOut默认是false，也就是核心线程不允许进行超时； // wc &gt; corePoolSize，表示当前线程池中的线程数量大于核心线程数量； // 对于超过核心线程数量的这些线程，需要进行超时控制 boolean timed = allowCoreThreadTimeOut || wc &gt; corePoolSize; /* * wc &gt; maximumPoolSize的情况是因为可能在此方法执行阶段同时执行了setMaximumPoolSize方法； * timed &amp;&amp; timedOut 如果为true，表示当前操作需要进行超时控制，并且上次从阻塞队列中获取任务发生了超时 * 接下来判断，如果有效线程数量大于1，或者阻塞队列是空的，那么尝试将workerCount减1； * 如果减1失败，则返回重试。 * 如果wc == 1时，也就说明当前线程是线程池中唯一的一个线程了。 */ if ((wc &gt; maximumPoolSize || (timed &amp;&amp; timedOut)) &amp;&amp; (wc &gt; 1 || workQueue.isEmpty())) &#123; if (compareAndDecrementWorkerCount(c)) return null; continue; &#125; try &#123; /* * 根据timed来判断，如果为true，则通过阻塞队列的poll方法进行超时控制，如果在keepAliveTime时间内没有获取到任务，则返回null； * 否则通过take方法，如果这时队列为空，则take方法会阻塞直到队列不为空。 * */ Runnable r = timed ? workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) : workQueue.take(); if (r != null) return r; // 如果 r == null，说明已经超时，timedOut设置为true timedOut = true; &#125; catch (InterruptedException retry) &#123; // 如果获取任务时当前线程发生了中断，则设置timedOut为false并返回循环重试 timedOut = false; &#125; &#125;&#125;这里重要的地方是第二个if判断，目的是控制线程池的有效线程数量。由上文中的分析可以知道，在执行execute方法时，如果当前线程池的线程数量超过了corePoolSize且小于maximumPoolSize，并且workQueue已满时，则可以增加工作线程，但这时如果超时没有获取到任务，也就是timedOut为true的情况，说明workQueue已经为空了，也就说明了当前线程池中不需要那么多线程来执行任务了，可以把多于corePoolSize数量的线程销毁掉，保持线程数量在corePoolSize即可。什么时候会销毁？当然是runWorker方法执行完之后，也就是Worker中的run方法执行完，由JVM自动回收。getTask方法返回null时，在runWorker方法中会跳出while循环，然后会执行processWorkerExit方法。processWorkerExit方法12345678910111213141516171819202122232425262728293031323334private void processWorkerExit(Worker w, boolean completedAbruptly) &#123; // 如果completedAbruptly值为true，则说明线程执行时出现了异常，需要将workerCount减1； // 如果线程执行时没有出现异常，说明在getTask()方法中已经已经对workerCount进行了减1操作，这里就不必再减了。 if (completedAbruptly) // If abrupt, then workerCount wasn't adjusted decrementWorkerCount(); final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; //统计完成的任务数 completedTaskCount += w.completedTasks; // 从workers中移除，也就表示着从线程池中移除了一个工作线程 workers.remove(w); &#125; finally &#123; mainLock.unlock(); &#125; // 根据线程池状态进行判断是否结束线程池 tryTerminate(); int c = ctl.get(); /* * 当线程池是RUNNING或SHUTDOWN状态时，如果worker是异常结束，那么会直接addWorker； * 如果allowCoreThreadTimeOut=true，并且等待队列有任务，至少保留一个worker； * 如果allowCoreThreadTimeOut=false，workerCount不少于corePoolSize。 */ if (runStateLessThan(c, STOP)) &#123; if (!completedAbruptly) &#123; int min = allowCoreThreadTimeOut ? 0 : corePoolSize; if (min == 0 &amp;&amp; ! workQueue.isEmpty()) min = 1; if (workerCountOf(c) &gt;= min) return; // replacement not needed &#125; addWorker(null, false); &#125;&#125;至此，processWorkerExit执行完之后，工作线程被销毁，以上就是整个工作线程的生命周期，从execute方法开始，Worker使用ThreadFactory创建新的工作线程，runWorker通过getTask获取任务，然后执行任务，如果getTask返回null，进入processWorkerExit方法，整个线程结束，如图所示：tryTerminate方法tryTerminate方法根据线程池状态进行判断是否结束线程池，代码如下：123456789101112131415161718192021222324252627282930313233343536373839final void tryTerminate() &#123; for (;;) &#123; int c = ctl.get(); /* * 当前线程池的状态为以下几种情况时，直接返回： * 1. RUNNING，因为还在运行中，不能停止； * 2. TIDYING或TERMINATED，因为线程池中已经没有正在运行的线程了； * 3. SHUTDOWN并且等待队列非空，这时要执行完workQueue中的task； */ if (isRunning(c) || runStateAtLeast(c, TIDYING) || (runStateOf(c) == SHUTDOWN &amp;&amp; ! workQueue.isEmpty())) return; // 如果线程数量不为0，则中断一个空闲的工作线程，并返回 if (workerCountOf(c) != 0) &#123; // Eligible to terminate interruptIdleWorkers(ONLY_ONE); return; &#125; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; // 这里尝试设置状态为TIDYING，如果设置成功，则调用terminated方法 if (ctl.compareAndSet(c, ctlOf(TIDYING, 0))) &#123; try &#123; // terminated方法默认什么都不做，留给子类实现 terminated(); &#125; finally &#123; // 设置状态为TERMINATED ctl.set(ctlOf(TERMINATED, 0)); termination.signalAll(); &#125; return; &#125; &#125; finally &#123; mainLock.unlock(); &#125; // else retry on failed CAS &#125;&#125;interruptIdleWorkers(ONLY_ONE);的作用是因为在getTask方法中执行workQueue.take()时，如果不执行中断会一直阻塞。在下面介绍的shutdown方法中，会中断所有空闲的工作线程，如果在执行shutdown时工作线程没有空闲，然后又去调用了getTask方法，这时如果workQueue中没有任务了，调用workQueue.take()时就会一直阻塞。所以每次在工作线程结束时调用tryTerminate方法来尝试中断一个空闲工作线程，避免在队列为空时取任务一直阻塞的情况。shutdown方法shutdown方法要将线程池切换到SHUTDOWN状态，并调用interruptIdleWorkers方法请求中断所有空闲的worker，最后调用tryTerminate尝试结束线程池。123456789101112131415161718public void shutdown() &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; // 安全策略判断 checkShutdownAccess(); // 切换状态为SHUTDOWN advanceRunState(SHUTDOWN); // 中断空闲线程 interruptIdleWorkers(); onShutdown(); // hook for ScheduledThreadPoolExecutor &#125; finally &#123; mainLock.unlock(); &#125; // 尝试结束线程池 tryTerminate();&#125;这里思考一个问题：在runWorker方法中，执行任务时对Worker对象w进行了lock操作，为什么要在执行任务的时候对每个工作线程都加锁呢？下面仔细分析一下：在getTask方法中，如果这时线程池的状态是SHUTDOWN并且workQueue为空，那么就应该返回null来结束这个工作线程，而使线程池进入SHUTDOWN状态需要调用shutdown方法；shutdown方法会调用interruptIdleWorkers来中断空闲的线程，interruptIdleWorkers持有mainLock，会遍历workers来逐个判断工作线程是否空闲。但getTask方法中没有mainLock；在getTask中，如果判断当前线程池状态是RUNNING，并且阻塞队列为空，那么会调用workQueue.take()进行阻塞；如果在判断当前线程池状态是RUNNING后，这时调用了shutdown方法把状态改为了SHUTDOWN，这时如果不进行中断，那么当前的工作线程在调用了workQueue.take()后会一直阻塞而不会被销毁，因为在SHUTDOWN状态下不允许再有新的任务添加到workQueue中，这样一来线程池永远都关闭不了了；由上可知，shutdown方法与getTask方法（从队列中获取任务时）存在竞态条件；解决这一问题就需要用到线程的中断，也就是为什么要用interruptIdleWorkers方法。在调用workQueue.take()时，如果发现当前线程在执行之前或者执行期间是中断状态，则会抛出InterruptedException，解除阻塞的状态；但是要中断工作线程，还要判断工作线程是否是空闲的，如果工作线程正在处理任务，就不应该发生中断；所以Worker继承自AQS，在工作线程处理任务时会进行lock，interruptIdleWorkers在进行中断时会使用tryLock来判断该工作线程是否正在处理任务，如果tryLock返回true，说明该工作线程当前未执行任务，这时才可以被中断。interruptIdleWorkers方法123456789101112131415161718192021222324private void interruptIdleWorkers() &#123; interruptIdleWorkers(false);&#125;private void interruptIdleWorkers(boolean onlyOne) &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; for (Worker w : workers) &#123; Thread t = w.thread; if (!t.isInterrupted() &amp;&amp; w.tryLock()) &#123; try &#123; t.interrupt(); &#125; catch (SecurityException ignore) &#123; &#125; finally &#123; w.unlock(); &#125; &#125; if (onlyOne) break; &#125; &#125; finally &#123; mainLock.unlock(); &#125;&#125;nterruptIdleWorkers遍历workers中所有的工作线程，若线程没有被中断tryLock成功，就中断该线程。为什么需要持有mainLock？因为workers是HashSet类型的，不能保证线程安全。shutdownNow方法1234567891011121314151617public List&lt;Runnable&gt; shutdownNow() &#123; List&lt;Runnable&gt; tasks; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; checkShutdownAccess(); advanceRunState(STOP); // 中断所有工作线程，无论是否空闲 interruptWorkers(); // 取出队列中没有被执行的任务 tasks = drainQueue(); &#125; finally &#123; mainLock.unlock(); &#125; tryTerminate(); return tasks;&#125;shutdownNow方法与shutdown方法类似，不同的地方在于：设置状态为STOP；中断所有工作线程，无论是否是空闲的；取出阻塞队列中没有被执行的任务并返回。shutdownNow方法执行完之后调用tryTerminate方法，该方法在上文已经分析过了，目的就是使线程池的状态设置为TERMINATED。线程池的监控通过线程池提供的参数进行监控。线程池里有一些属性在监控线程池的时候可以使用getTaskCount：线程池已经执行的和未执行的任务总数；getCompletedTaskCount：线程池已完成的任务数量，该值小于等于taskCount；getLargestPoolSize：线程池曾经创建过的最大线程数量。通过这个数据可以知道线程池是否满过，也就是达到了maximumPoolSize；getPoolSize：线程池当前的线程数量；getActiveCount：当前线程池中正在执行任务的线程数量。通过这些方法，可以对线程池进行监控，在ThreadPoolExecutor类中提供了几个空方法，如beforeExecute方法，afterExecute方法和terminated方法，可以扩展这些方法在执行前或执行后增加一些新的操作，例如统计线程池的执行任务的时间等，可以继承自ThreadPoolExecutor来进行扩展。5. ScheduledExecutorServiceScheduledExecutorService是一个接口，它继承于于ExecutorService。它相当于提供了”延时”和”周期执行”功能的ExecutorService。ScheduledExecutorService提供了相应的函数接口，可以安排任务在给定的延迟后执行，也可以让任务周期的执行。6. ScheduledThreadPoolExecutorScheduledThreadPoolExecutor继承于ThreadPoolExecutor，并且实现了ScheduledExecutorService接口。它相当于提供了”延时”和”周期执行”功能的ScheduledExecutorService。ScheduledThreadPoolExecutor类似于Timer，但是在高并发程序中，ScheduledThreadPoolExecutor的性能要优于Timer。7. ExecutorsExecutors是个静态工厂类。它通过静态工厂方法返回ExecutorService、ScheduledExecutorService、ThreadFactory 和 Callable 等类的对象。这里要注意，虽然Executors可以用静态方法来创建很多方便的线程池，但是我们在实际工作中发现，因为如果使用像newFixedThreadPool这种方式创建的线程池，会因为队列无限大，导致无法控制而出现内存溢出的问题，所以，我们创建线程池最优雅的方式是通过继承ThreadPoolExecutor，并重写相应的方法来处理。talk is cheap, let me show the code:12345678910111213/** * 任务处理线程池 */ private class WorkerPool extends ThreadPoolExecutor &#123; public WorkerPool(int coreSize, int maxSize, long keepAlive, TimeUnit timeUnit, BlockingQueue&lt;Runnable&gt; queue, ThreadFactory threadFactory) &#123; super(coreSize, maxSize, keepAlive, timeUnit, queue, threadFactory); &#125; @Override protected void afterExecute(Runnable runnable, Throwable throwable) &#123; super.afterExecute(runnable, throwable); &#125; &#125;这是我在项目中用到的手动构建线程池的简单示例，通过这种方式可以调用对应的构造方法，来构建你需要的线程池，包括参数的配置和策略的更换，只要你理解他是如何运行的，就可以轻松驾驭线程池，并发处理业务代码。博文参考Java多线程系列目录(共43篇)深入理解Java线程池：ThreadPoolExecutor","categories":[{"name":"多线程","slug":"多线程","permalink":"http://www.fufan.me/categories/多线程/"}],"tags":[{"name":"多线程","slug":"多线程","permalink":"http://www.fufan.me/tags/多线程/"}]},{"title":"python分布式爬虫利器——Scrapy+Scrapy-redis+Scrapyd+ScrapyWeb","slug":"python分布式爬虫利器——Scrapy-Scrapy-redis-Scrapyd-ScrapyWeb","date":"2018-12-03T09:28:08.000Z","updated":"2018-12-03T09:54:18.295Z","comments":true,"path":"2018/12/03/python分布式爬虫利器——Scrapy-Scrapy-redis-Scrapyd-ScrapyWeb/","link":"","permalink":"http://www.fufan.me/2018/12/03/python分布式爬虫利器——Scrapy-Scrapy-redis-Scrapyd-ScrapyWeb/","excerpt":"","text":"由于最近公司海外项目需要接一个泰国二手车平台的报价信息的客户需求，公网爬虫的话用python来做是最方便的，因此技术选型用Scrapy+Scrapy-redis+Scrapyd+Gerapy。优势如下：* 简单、易维护 * 分布式爬虫，更快（由于海外代理少且贵） * scrapy文档内容丰富 * 等等。。 Scrapy：是一个基于Twisted的异步IO框架，有了这个框架，我们就不需要等待当前URL抓取完毕之后在进行下一个URL的抓取，抓取效率可以提高很多。Scrapy-redis：虽然Scrapy框架是异步加多线程的，但是我们只能在一台主机上运行，爬取效率还是有限的，Scrapy-redis库为我们提供了Scrapy分布式的队列，调度器，去重等等功能，有了它，我们就可以将多台主机组合起来，共同完成一个爬取任务，抓取的效率又提高了。Scrapyd：分布式爬虫完成之后，接下来就是代码部署，如果我们有很多主机，那就要逐个登录服务器进行部署，万一代码有所改动……….可以想象，这个过程是多么繁琐。Scrapyd是专门用来进行分布式部署的工具，它提供HTTP接口来帮助我们部署，启动，停止，删除爬虫程序，利用它我们可以很方便的完成Scrapy爬虫项目的部署。ScrapyWeb：是一个基于Scrapyd，Scrapyd API，Django，nodejs搭建的分布式爬虫管理框架。简单点说，就是用上述的Scrapyd工具是在命令行进行操作，而Gerapy将命令行和图形界面进行了对接，我们只需要点击按钮就可完成部署，启动，停止，删除的操作。并且支持节点管理、爬虫监控，邮件发送等功能。1. 创建Scrapy项目2. 添加Scrapy-redis配置3. 安装Scrapyd4. 强大的界面分布式管理scrapy进程——ScrapyWeb待续……","categories":[],"tags":[]},{"title":"学会写脚本系列（二）—— Python之dict(或对象)与json之间的互相转化","slug":"学会写脚本系列（二）——-Python之dict-或对象-与json之间的互相转化","date":"2018-11-22T09:52:00.000Z","updated":"2018-11-22T09:52:30.480Z","comments":true,"path":"2018/11/22/学会写脚本系列（二）——-Python之dict-或对象-与json之间的互相转化/","link":"","permalink":"http://www.fufan.me/2018/11/22/学会写脚本系列（二）——-Python之dict-或对象-与json之间的互相转化/","excerpt":"","text":"在Python语言中，json数据与dict字典以及对象之间的转化，是必不可少的操作。在Python中自带json库。通过import json导入。在json模块有2个方法，loads()：将json数据转化成dict数据dumps()：将dict数据转化成json数据load()：读取json文件数据，转成dict数据dump()：将dict数据转化成json数据后写入json文件示例如下：1. dict字典转json数据123456789101112131415import jsondef dict_to_json(): dict = &#123;&#125; dict[&apos;name&apos;] = &apos;fufan&apos; dict[&apos;age&apos;] = 25 dict[&apos;sex&apos;] = &apos;male&apos; print(dict) # 输出：&#123;&apos;name&apos;: &apos;fufan&apos;, &apos;age&apos;: 25, &apos;sex&apos;: &apos;male&apos;&#125; j = json.dumps(dict) print(j) # 输出：&#123;&quot;name&quot;: &quot;fufan&quot;, &quot;age&quot;: 25, &quot;sex&quot;: &quot;male&quot;&#125;if __name__ == &apos;__main__&apos;: dict_to_json()2. 对象转json数据12345678910111213141516171819202122232425262728293031import jsonclass Student(object): id = &apos;&apos; name = &apos;&apos; age = 0 gender = &apos;&apos; phone = &apos;&apos; email = &apos;&apos; def __init__(self, id, name, age, gender, phone, email): self.id = id self.name = name self.age = age self.gender = gender self.phone = phone self.email = emaildef obj_to_json(): stu = Student(&apos;fufan&apos;, &apos;fufan&apos;, 28, &apos;male&apos;, &apos;13000000000&apos;, &apos;fufan@51dojo.com&apos;) print(type(stu)) # &lt;class &apos;json_test.student.Student&apos;&gt; stu = stu.__dict__ # 将对象转成dict字典 print(type(stu)) # &lt;class &apos;dict&apos;&gt; print(stu) # &#123;&apos;id&apos;: &apos;fufan&apos;, &apos;name&apos;: &apos;fufan&apos;, &apos;age&apos;: 28, &apos;gender&apos;: &apos;male&apos;, &apos;phone&apos;: &apos;13000000000&apos;, &apos;email&apos;: &apos;fufan@51dojo.com&apos;&#125; j = json.dumps(obj=stu) print(j) # &#123;&quot;id&quot;: &quot;fufan&quot;, &quot;name&quot;: &quot;fufan&quot;, &quot;age&quot;: 28, &quot;gender&quot;: &quot;male&quot;, &quot;phone&quot;: &quot;13000000000&quot;, &quot;email&quot;: &quot;fufan@51dojo.com&quot;&#125;if __name__ == &apos;__main__&apos;: obj_to_json()3. json数据转成dict字典1234567891011import jsondef json_to_dict(): j = &apos;&#123;&quot;id&quot;: &quot;fufan&quot;, &quot;name&quot;: &quot;fufan&quot;, &quot;age&quot;: 28, &quot;gender&quot;: &quot;male&quot;, &quot;phone&quot;: &quot;13000000000&quot;, &quot;email&quot;: &quot;fufan@51dojo.com&quot;&#125;&apos; dict = json.loads(s=j) print(dict) # &#123;&apos;id&apos;: &apos;fufan&apos;, &apos;name&apos;: &apos;fufan&apos;, &apos;age&apos;: 28, &apos;gender&apos;: &apos;male&apos;, &apos;phone&apos;: &apos;13000000000&apos;, &apos;email&apos;: &apos;fufan@51dojo.com&apos;&#125;if __name__ == &apos;__main__&apos;: json_to_dict()4. json数据转成对象123456789101112131415161718192021import jsonclass Student(object): id = &apos;&apos; name = &apos;&apos; age = 0 gender = &apos;&apos; phone = &apos;&apos; email = &apos;&apos;def json_to_obj(): j = &apos;&#123;&quot;id&quot;: &quot;fufan&quot;, &quot;name&quot;: &quot;fufan&quot;, &quot;age&quot;: 28, &quot;gender&quot;: &quot;male&quot;, &quot;phone&quot;: &quot;13000000000&quot;, &quot;email&quot;: &quot;fufan@51dojo.com&quot;&#125;&apos; dict = json.loads(s=j) stu = Student() stu.__dict__ = dict print(&apos;id: &apos; + stu.id + &apos; name: &apos; + stu.name + &apos; age: &apos; + str(stu.age) + &apos; gender: &apos; + str( stu.gender) + &apos; phone: &apos; + stu.phone + &apos; email: &apos; + stu.email)if __name__ == &apos;__main__&apos;: json_to_obj()5. dump()方法的使用1234567891011121314import jsondef dict_to_json_write_file(): dict = &#123;&#125; dict[&apos;name&apos;] = &apos;fufan&apos; dict[&apos;age&apos;] = 26 dict[&apos;gender&apos;] = &apos;male&apos; print(dict) # &#123;&apos;name&apos;: &apos;fufan&apos;, &apos;age&apos;: 26, &apos;gender&apos;: &apos;male&apos;&#125; with open(&apos;test.json&apos;, &apos;w&apos;) as f: json.dump(dict, f) # 会在目录下生成一个test.json的文件，文件内容是dict数据转成的json数据if __name__ == &apos;__main__&apos;: dict_to_json_write_file()6. load()的使用12345678910import jsondef json_file_to_dict(): with open(&apos;test.json&apos;, &apos;r&apos;) as f: dict = json.load(fp=f) print(dict) # &#123;&apos;name&apos;: &apos;fufan&apos;, &apos;age&apos;: 26, &apos;gender&apos;: &apos;male&apos;&#125;if __name__ == &apos;__main__&apos;: json_file_to_dict()","categories":[],"tags":[]},{"title":"Spring获取bean的几种方式","slug":"Spring获取bean的几种方式","date":"2018-11-21T07:08:42.000Z","updated":"2018-11-21T07:09:22.255Z","comments":true,"path":"2018/11/21/Spring获取bean的几种方式/","link":"","permalink":"http://www.fufan.me/2018/11/21/Spring获取bean的几种方式/","excerpt":"","text":"Spring获取bean的几种方式在初始化时保存ApplicationContext对象通过Spring提供的utils类获取ApplicationContext对象继承自抽象类ApplicationObjectSupport继承自抽象类WebApplicationObjectSupport实现接口ApplicationContextAware （推荐）通过Spring提供的ContextLoader在初始化时保存ApplicationContext对象12ApplicationContext ac = new FileSystemXmlApplicationContext(&quot;applicationContext.xml&quot;); ac.getBean(&quot;userService&quot;);//比如：&lt;bean id=&quot;userService&quot; class=&quot;com.cloud.service.impl.UserServiceImpl&quot;&gt;&lt;/bean&gt;这样的方式适用于採用Spring框架的独立应用程序，须要程序通过配置文件手工初始化Spring的情况。通过Spring提供的工具类获取ApplicationContext对象1234ApplicationContext ac1 = WebApplicationContextUtils.getRequiredWebApplicationContext(ServletContext sc); ApplicationContext ac2 = WebApplicationContextUtils.getWebApplicationContext(ServletContext sc); ac1.getBean(&quot;beanId&quot;); ac2.getBean(&quot;beanId&quot;);这样的方式适合于採用Spring框架的B/S系统，通过ServletContext对象获取ApplicationContext对象。然后在通过它获取须要的类实例。上面两个工具方式的差别是，前者在获取失败时抛出异常。后者返回null。继承自抽象类ApplicationObjectSupport抽象类ApplicationObjectSupport提供getApplicationContext()方法。能够方便的获取ApplicationContext。Spring初始化时。会通过该抽象类的setApplicationContext(ApplicationContext context)方法将ApplicationContext 对象注入。继承自抽象类WebApplicationObjectSupport类似上面方法。调用getWebApplicationContext()获取WebApplicationContext实现接口ApplicationContextAware实现该接口的setApplicationContext(ApplicationContext context)方法，并保存ApplicationContext 对象。Spring初始化时，会通过该方法将ApplicationContext对象注入。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849@Component@Slf4jpublic class ApplicationContextProvider implements ApplicationContextAware &#123; /** * 上下文对象实例 */ private ApplicationContext applicationContext; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; this.applicationContext = applicationContext; &#125; public ApplicationContext getApplicationContext() &#123; return applicationContext; &#125; /** * 通过name获取 Bean. * @param name * @return */ public Object getBean(String name)&#123; return getApplicationContext().getBean(name); &#125; /** * 通过class获取Bean. * @param clazz * @param &lt;T&gt; * @return */ public &lt;T&gt; T getBean(Class&lt;T&gt; clazz)&#123; return getApplicationContext().getBean(clazz); &#125; /** * 通过name,以及Clazz返回指定的Bean * @param name * @param clazz * @param &lt;T&gt; * @return */ public &lt;T&gt; T getBean(String name,Class&lt;T&gt; clazz)&#123; return getApplicationContext().getBean(name, clazz); &#125;&#125;此方法在工作中用的比较多，也是比较好的一种方法通过Spring提供的ContextLoader12WebApplicationContext wac = ContextLoader.getCurrentWebApplicationContext();wac.getBean(beanID);","categories":[],"tags":[]},{"title":"学会写脚本系列（一）—— Python Requests包的使用","slug":"学会写脚本系列（一）——-Python-Requests包的使用","date":"2018-11-19T09:39:00.000Z","updated":"2018-11-22T09:40:11.818Z","comments":true,"path":"2018/11/19/学会写脚本系列（一）——-Python-Requests包的使用/","link":"","permalink":"http://www.fufan.me/2018/11/19/学会写脚本系列（一）——-Python-Requests包的使用/","excerpt":"","text":"由于公司的需求，平时需要测试一些公网爬虫，不过java在写小demo的时候太重的，所以必须要学点python、js、shell等语言来写一些脚本。所以平时抽空需要准备一些工具脚本，来支持临时测试。今天我介绍用python来做批量跑http请求的脚本，刚好搜到了requests这个包，感觉用起来还挺方便的，具体的文档的话可以参考中文官方文档和英文官方文档requests是python的一个HTTP客户端库，跟urllib，urllib2类似，那为什么要用requests而不用urllib2呢？官方文档中是这样说明的：1python的标准库urllib2提供了大部分需要的HTTP功能，但是API太逆天了，一个简单的功能就需要一大堆代码。官方网站设计的原则如下：Beautiful is better than ugly.(美丽优于丑陋)Explicit is better than implicit.(清楚优于含糊)Simple is better than complex.(简单优于复杂)Complex is better than complicated.(复杂优于繁琐)Readability counts.(重要的是可读性)1. 安装Requestpip安装1pip install requests源码安装123$ git clone git://github.com/kennethreitz/requests.git$ cd requests$ python setup.py installpycharm安装2. 快速上手既然官网的文档写的那么详细了，我这里就直接上例子了1234567891011121314151617181920212223import requestsdef do_http_test(): # 请求url url = &apos;https://s.weibo.com/ajax_User/follow&apos; # header头，Cookie可以放在这里 headers = &#123;&apos;content-type&apos;: &quot;application/x-www-form-urlencoded&quot;, &apos;Referer&apos;: &apos;https://s.weibo.com/user?q=%E6%B8%B8%E6%88%8F&apos;, &apos;Cookie&apos;: &apos;SUB=_2A2528mffDeRhGeBJ4lUR9S3EyzyIHXVVht4XrDV8PUNbmtBeLUfekW9NRjmhG0VwLiyjpeOPdOk01GeypRYWWaEf&apos;&#125; # post请求的请求data payload = &#123;&apos;uid&apos;: &apos;5126161537&apos;, &apos;type&apos;: &apos;followed&apos;, &apos;action_code&apos;: &apos;71&apos;&#125; # 具体调用的.post方法 r = requests.post(url, data = payload, headers = headers) # 打印结果 print(r.text)if __name__ == &apos;__main__&apos;: do_http_test()3. 高级用法未完待续……","categories":[{"name":"python","slug":"python","permalink":"http://www.fufan.me/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://www.fufan.me/tags/python/"}]},{"title":"聚簇索引和非聚簇索引","slug":"聚簇索引和非聚簇索引","date":"2018-11-16T09:37:00.000Z","updated":"2018-11-16T09:37:40.595Z","comments":true,"path":"2018/11/16/聚簇索引和非聚簇索引/","link":"","permalink":"http://www.fufan.me/2018/11/16/聚簇索引和非聚簇索引/","excerpt":"","text":"1、大多数表都应该有聚簇索引或使用分区来降低对表尾页的竞争，在一个高事务的环境中，对最后一页的封锁严重影响系统的吞吐量。2、在聚簇索引下，数据在物理上按顺序排在数据页上，重复值也排在一起，因而在那些包含范围检查 (between、&lt;、&lt;=、&gt;、&gt;=)或使用group by或orderby的查询时，一旦找到具有范围中第一个键值的行，具有后续索引值的行保证物理上毗连在一起而不必进一步搜索，避免了大范围扫描，可以大 大提高查询速度。3、在一个频繁发生插入操作的表上建立聚簇索引时，不要建在具有单调上升值的列(如IDENTITY)上，否则会经常引起封锁冲突。4、在聚簇索引中不要包含经常修改的列，因为码值修改后，数据行必须移动到新的位置。5、选择聚簇索引应基于where子句和连接操作的类型。不知从什么角度来对比，只能说说各自的特点，希望对你有用。1、聚簇索引a) 一个索引项直接对应实际数据记录的存储页，可谓“直达”b) 主键缺省使用它c) 索引项的排序和数据行的存储排序完全一致，利用这一点，想修改数据的存储顺序，可以通过改变主键的方法（撤销原有主键，另找也能满足主键要求的一个字段或一组字段，重建主键）d) 一个表只能有一个聚簇索引（理由：数据一旦存储，顺序只能有一种）2、非聚簇索引a) 不能“直达”，可能链式地访问多级页表后，才能定位到数据页b) 一个表可以有多个非聚簇索引","categories":[{"name":"数据库","slug":"数据库","permalink":"http://www.fufan.me/categories/数据库/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"http://www.fufan.me/tags/数据库/"}]},{"title":"从session到token","slug":"从session到token","date":"2018-11-14T06:15:00.000Z","updated":"2018-11-14T07:34:31.747Z","comments":true,"path":"2018/11/14/从session到token/","link":"","permalink":"http://www.fufan.me/2018/11/14/从session到token/","excerpt":"","text":"很久以前30年前，互联网刚起步的时候，大部分的web请求基本都是在浏览网页，既然是浏览，作为一个服务器， 为什么要记住谁在一段时间里都浏览了什么文档呢？开始记录session但是好日子没持续多久， 很快大家就不满足于静态的Html 文档了， 交互式的Web应用开始兴起， 尤其是论坛， 在线购物等网站。那么必须记住哪些人登录系统， 哪些人往自己的购物车中放了商品， 也就是说必须把每个人区分开。由于HTTP协议的无状态特性， 我必须加点小手段，才能完成会话管理。所以在这个时候，服务器端一般会使用sessionId（会话标识）来标识该用户在一段时间内的请求，其实就是一个随机的字符串。这样每次请求过来，服务器端需要区分谁是谁。沉重的负担由于互联网的飞速发展，web请求量剧增，导致服务器压力增大，同时开销也增大。比如每个人只需要保存自己的session id，而服务器需要保存所有人的session id 。虽然可以通过分布式集群扩展服务能力，但是这里又会出现session分布的问题。比如说我用两个机器组成了一个集群， 小F通过机器A登录了系统， 那session id会保存在机器A上， 假设小F的下一次请求被转发到机器B怎么办？ 机器B可没有小F的 session id啊。虽然session sticky、缓存可以在一定程度上解决该问题，但是都会存在单点故障、机器宕机的问题。导致可靠性降低，同时增大了服务器的架构复杂度，不便于后期的扩展和维护。时间换空间同时，session的安全性也是一个隐患，不怀好意的人可以通过伪造sessionId来请求。渐渐地，很多网站开始使用JWT、UUID的方式登录或者保存用户状态信息。JWT是json web token缩写。它将用户信息加密到token里，服务器不保存任何用户信息。服务器通过使用保存的密钥验证token的正确性，只要正确即通过验证。优点是在分布式系统中，很好地解决了单点登录问题，很容易解决了session共享的问题。缺点是无法作废已颁布的令牌/不易应对数据过期。这样的话，就无状态的用token来替代了session，用CPU的计算时间来替换session的存储空间JWT 的实践其实还是挺简单。安全性也是得到了保证，后端只需要保存着密匙，其他数据可以保存在token，由前端携带，这样可以减低后端的内心消耗。虽然token是加密的，但是携带的验证数据还是不要是敏感数据.思想的迁移目前公司爬虫业务也有类似的问题，我们每一个任务的taskId是通过生成一个UUID来绑定和溯源的，这样可以保证全局唯一性，是很好的对分布式系统任务分离的一个实践。UUID生成的规则:如图所示，这里第1位不可用，前41位表示时间，中间10位用来表示工作机器的id，后12位的序列号.其中时间比较好理解，工作机器id则是机器标识，序列号是一个自增序列。有多少位表示在这一个单位时间内，此机器最多可以支持2^12个并发。在进入下一个时间单位后，序列号归0。虽然这种模式再生产上单日百万量级的服务已经实践了一年多，微服务的切分也是达到了40-50个之多，但是会发现有一个问题，就是token里未带任何任务相关的信息，而随着我们的业务量的增长，遇到的单点故障及其他复杂问题也增加（特别是aliyun中redis中间件常常会出现闪断等问题，包括db、oss、ots等都会有类似的情况）。所以要完全解决这种方式的话，就必须要有主备环境，随时切换，以应对单点故障带来对服务的影响（毕竟是24*7的服务，出一次事故对客户带来损失巨大）。而切换环境的瞬间，情况复杂度也是非常复杂的，因为你首先要保证之前的任务是正常在老环境里跑的，而新的任务也要按比例分流到新环境中来，但是需要让用户是无感知切换的话，入口是不能变的。所以我们就会考虑后端其他的微服务如何去将gateway中生成的taskId分配到哪个环境中。因为环境完全是两套，假设我们分为C区任务和D区任务，而如果能够在taskId中添加一位来表示是哪个区的话，就可以路由到哪个环境中去正常的执行下去。","categories":[{"name":"分布式","slug":"分布式","permalink":"http://www.fufan.me/categories/分布式/"}],"tags":[{"name":"jwt","slug":"jwt","permalink":"http://www.fufan.me/tags/jwt/"}]},{"title":"JVM专题（八）---强、软、弱、虚的知识点总结","slug":"Java四种引用-强、软、弱、虚的知识点总结","date":"2018-11-13T09:47:00.000Z","updated":"2018-11-14T03:27:48.331Z","comments":true,"path":"2018/11/13/Java四种引用-强、软、弱、虚的知识点总结/","link":"","permalink":"http://www.fufan.me/2018/11/13/Java四种引用-强、软、弱、虚的知识点总结/","excerpt":"","text":"本文会按照以下思路进行：（1）Java的四种对象引用的基本概念（2）四种对象引用的差异对比（3）对象可及性的判断以及与垃圾回收机制的关系（4）引用队列ReferenceQueue的介绍（5）WeakHashMap的相关介绍Java的四种对象引用的基本概念从JDK1.2版本开始，把对象的引用分为四种级别，从而使程序更加灵活的控制对象的生命周期。这四种级别由高到低依次为：强引用、软引用、弱引用和虚引用。1、强引用1Object obj =new Object();上述Object这类对象就具有强引用，属于不可回收的资源，垃圾回收器绝不会回收它。当内存空间不足，Java虚拟机宁愿抛出OutOfMemoryError错误，使程序异常终止，也不会靠回收具有强引用的对象，来解决内存不足的问题。值得注意的是：如果想中断或者回收强引用对象，可以显式地将引用赋值为null，这样的话JVM就会在合适的时间，进行垃圾回收。下图是堆区的内存示意图，分为新生代，老生代，而垃圾回收主要也是在这部分区域中进行。2、软引用（SoftReference）如果一个对象只具有软引用，那么它的性质属于可有可无的那种。如果此时内存空间足够，垃圾回收器就不会回收它，如果内存空间不足了，就会回收这些对象的内存。只要垃圾回收器没有回收它，该对象就可以被程序使用。软引用可用来实现内存敏感的告诉缓存。软引用可以和一个引用队列联合使用，如果软件用所引用的对象被垃圾回收，Java虚拟机就会把这个软引用加入到与之关联的引用队列中。12345Object obj = new Object(); ReferenceQueue queue = new ReferenceQueue(); SoftReference reference = new SoftReference(obj, queue); //强引用对象滞空，保留软引用 obj = null;当内存不足时，软引用对象被回收时，reference.get()为null，此时软引用对象的作用已经发挥完毕，这时将其添加进ReferenceQueue 队列中如果要判断哪些软引用对象已经被清理：1234SoftReference ref = null; while ((ref = (SoftReference) queue.poll()) != null) &#123; //清除软引用对象 &#125;3、弱引用(WeakReference)如果一个对象具有弱引用，那其的性质也是可有可无的状态。而弱引用和软引用的区别在于：弱引用的对象拥有更短的生命周期，只要垃圾回收器扫描到它，不管内存空间充足与否，都会回收它的内存。同样的弱引用也可以和引用队列一起使用。12345Object obj = new Object(); ReferenceQueue queue = new ReferenceQueue(); WeakReference reference = new WeakReference(obj, queue); //强引用对象滞空，保留软引用 obj = null;4、虚引用（PhantomReference）虚引用和前面的软引用、弱引用不同，它并不影响对象的生命周期。如果一个对象与虚引用关联，则跟没有引用与之关联一样，在任何时候都可能被垃圾回收器回收。注意：虚引用必须和引用队列关联使用，当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会把这个虚引用加入到与之关联的引用队列中。程序可以通过判断引用队列中是否已经加入了虚引用，来了解被引用的对象是否将要被垃圾回收。如果程序发现某个虚引用已经被加入到引用队列，那么就可以在所引用的对象的内存被回收之前采取必要的行动。12345Object obj = new Object(); ReferenceQueue queue = new ReferenceQueue(); PhantomReference reference = new PhantomReference(obj, queue); //强引用对象滞空，保留软引用 obj = null;引用总结1.对于强引用，平时在编写代码时会经常使用。2.而其他三种类型的引用，使用得最多就是软引用和弱引用，这两种既有相似之处又有区别，他们都来描述非必须对象。3.被软引用关联的对象只有在内存不足时才会被回收，而被弱引用关联的对象在JVM进行垃圾回收时总会被回收。四种对象引用的差异对比1强引用 &gt; 软引用 &gt; 弱引用 &gt; 虚引用最后总结成一张表格：引用类型被垃圾回收时间用途生存时间强引用从来不会对象的一般状态JVM停止运行时终止软引用在内存不足时对象缓存内存不足时终止弱引用在垃圾回收时对象缓存垃圾回收时终止虚引用UnkonwnUnkonwnUnkonwn对象可及性的判断在很多的时候，一个对象并不是从根集直接引用的，而是一个对象被其他对象引用，甚至同时被几个对象所引用，从而构成一个以根集为顶的树形结构。在这个树形的引用链中，箭头的方向代表了引用的方向，所指向的对象是被引用对象。由图可以看出，从根集到一个对象可以由很多条路径。比如到达对象5的路径就有① -&gt; ⑤，③ -&gt;⑦两条路径。由此带来了一个问题，那就是某个对象的可及性如何判断：（1）单条引用路径可及性判断：在这条路径中，最弱的一个引用决定对象的可及性。（2）多条引用路径可及性判断：几条路径中，最强的一条的引用决定对象的可及性。比如，我们假设图2中引用①和③为强引用，⑤为软引用，⑦为弱引用，对于对象5按照这两个判断原则，路径①-⑤取最弱的引用⑤，因此该路径对对象5的引用为软引用。同样，③-⑦为弱引用。在这两条路径之间取最强的引用，于是对象5是一个软可及对象。另外两个重要的点：**1. 强可达的对象一定不会被清理JVM保证抛出out of memory之前，清理所有的软引用对象**引用队列ReferenceQueue的介绍引用队列配合Reference的子类等使用,当引用对象所指向的对象被垃圾回收后,该Reference则被追加到引用队列的末尾.WeakHashMap的相关介绍在Java集合中有一种特殊的Map类型即WeakHashMap,在这种Map中存放了键对象的弱引用,当一个键对象被垃圾回收器回收时,那么相应的值对象的引用会从Map中删除.WeakHashMap能够节约储存空间,可用来缓存那些非必须存在的数据.而WeakHashMap是主要通过expungeStaleEntries()这个方法来实现的,而WeakHashMap也内置了一个ReferenceQueue,来获取键对象的引用情况.这个方法,相当于遍历ReferenceQueue然后,将已经被回收的键对象,对应的值对象滞空.1234567891011121314151617181920212223242526272829private void expungeStaleEntries() &#123; for (Object x; (x = queue.poll()) != null; ) &#123; synchronized (queue) &#123; @SuppressWarnings(&quot;unchecked&quot;) Entry&lt;K,V&gt; e = (Entry&lt;K,V&gt;) x; int i = indexFor(e.hash, table.length); Entry&lt;K,V&gt; prev = table[i]; Entry&lt;K,V&gt; p = prev; while (p != null) &#123; Entry&lt;K,V&gt; next = p.next; if (p == e) &#123; if (prev == e) table[i] = next; else prev.next = next; // Must not null out e.next; // stale entries may be in use by a HashIterator //通过滞空,来帮助垃圾回收 e.value = null; size--; break; &#125; prev = p; p = next; &#125; &#125; &#125; &#125;而且需要注意的是:expungeStaleEntries()并不是自动调用的,需要外部对WeakHashMap对象进行查询或者操作,才会进行自动释放的操作来一张总结图:参考博文Java四种引用—强、软、弱、虚的知识点总结","categories":[{"name":"jvm","slug":"jvm","permalink":"http://www.fufan.me/categories/jvm/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://www.fufan.me/tags/jvm/"}]},{"title":"QPS/TPS/并发量/系统吞吐量的概念","slug":"QPS-TPS-并发量-系统吞吐量的概念","date":"2018-11-11T07:52:32.000Z","updated":"2018-11-11T07:53:00.332Z","comments":true,"path":"2018/11/11/QPS-TPS-并发量-系统吞吐量的概念/","link":"","permalink":"http://www.fufan.me/2018/11/11/QPS-TPS-并发量-系统吞吐量的概念/","excerpt":"","text":"系统吞吐量的关键参数一个系统的吞度量（承压能力）与request对CPU的消耗、外部接口、IO等等紧密关联。单个reqeust 对CPU消耗越高，外部系统接口、IO影响速度越慢。系统吞吐能力越低，反之越高。QPS: 每秒钟处理完请求的次数；注意这里是处理完。具体是指发出请求到服务器处理完成功返回结果。可以理解在server中有个counter，每处理一个请求加1，1秒后counter=QPS。TPS：每秒钟处理完的事务次数，一般TPS是对整个系统来讲的。一个应用系统1s能完成多少事务处理，一个事务在分布式处理中，可能会对应多个请求，对于衡量单个接口服务的处理能力，用QPS比较多。并发量：系统能同时处理的请求数RT：响应时间，处理一次请求所需要的平均处理时间计算关系：QPS = 并发量 / 平均响应时间并发量 = QPS * 平均响应时间一个典型的上班签到系统，早上8点上班。7点半到8点这30分钟的时间里用户会登录签到系统进行签到。公司员工为1000人，平均每一个员上登录签到系统的时长为5分钟。能够用以下的方法计算：QPS = 1000/(3060) 事务/秒平均响应时间为 = 5 60 秒并发数= QPS平均响应时间 = 1000/(30 60) (5 60)=166.7","categories":[],"tags":[]},{"title":"centos安装python3.6（亲测爬坑）","slug":"centos安装python3-6（亲测爬坑）","date":"2018-11-03T09:26:00.000Z","updated":"2018-12-03T09:27:35.374Z","comments":true,"path":"2018/11/03/centos安装python3-6（亲测爬坑）/","link":"","permalink":"http://www.fufan.me/2018/11/03/centos安装python3-6（亲测爬坑）/","excerpt":"","text":"1. 安装可能用到的依赖1$ yum install -y openssl-devel bzip2-devel expat-devel gdbm-devel readline-devel sqlite-devel zlib-devel2. 下载Python3.6.5源码1$ wget https://www.python.org/ftp/python/3.6.5/Python-3.6.5.tgz3. 解压到当前目录1$ tar -xzvf Python-3.6.5.tgz4. 进入解压后的目录1$ cd Python-3.6.55. 安装到/usr/local/python目录，不用事先创建python目录1$ ./configure --prefix=/usr/local/python6. 编译1$ make7. 安装1$ make altinstall8. 进入/usr/bin目录1$ cd /usr/bin9. 重命名python2的快捷方式12$ mv python python.bak$ mv pip pip.bak10. 创建python3与pip3软连接12$ ln -s /usr/local/python/bin/python3.6 /usr/bin/python$ ln -s /usr/local/python/bin/pip3.6 /usr/bin/pip10. 修改环境变量123$ vi ~/.bash_profile# 然后在.bash_profile中添加一行 export PATH=$PATH:/usr/local/python/bin，保存$ source ~/.bash_profile11. 测试是否安装成功12[root@server83 ~]# python -VPython 3.6.512[root@server83 ~]# pip -Vpip 18.1 from /usr/local/python/lib/python3.6/site-packages/pip (python 3.6)12. 修改yum的配置由于通过此方法来安装会导致yum的执行命令有问题，需要修改yum的python版本配置问题出现原因：yum包管理是使用python2.x写的，将python2.x升级到python3.1.3以后，由于python版本语法兼容性导致问题出现解决办法：修改yum配置文件，将python版本指向以前的旧版本12$ vi /usr/bin/yum#!/usr/bin/python2.7修改urlgrabber-ext-down文件，更改python版本12$ vi /usr/libexec/urlgrabber-ext-down#!/usr/bin/python2.713. 升级pip1$ pip install --upgrade pip","categories":[{"name":"python","slug":"python","permalink":"http://www.fufan.me/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://www.fufan.me/tags/python/"}]},{"title":"Java常用集合源码分析之map遍历效率比较（六）","slug":"Java常用集合源码分析之map遍历效率比较（六）","date":"2018-06-19T14:21:00.000Z","updated":"2018-11-11T14:21:41.799Z","comments":true,"path":"2018/06/19/Java常用集合源码分析之map遍历效率比较（六）/","link":"","permalink":"http://www.fufan.me/2018/06/19/Java常用集合源码分析之map遍历效率比较（六）/","excerpt":"","text":"遍历方式1. 遍历key123for (String key : map.keySet()) &#123;&#125;2. 遍历value123for (String value : map.values()) &#123;&#125;3. 遍历key+value1234567for (Entry&lt;String, String&gt; entry: map.entrySet()) &#123; key = entry.getKey(); value = entry.getValue();&#125;如果你使用HashMap同时遍历key和value时，keySet与entrySet方法的性能差异取决于key的复杂度，总体来说还是推荐使用entrySet。换言之，取决于HashMap查找value的开销。entrySet一次性取出所有key和value的操作是有性能开销的，当这个损失小于HashMap查找value的开销时，entrySet的性能优势就会体现出来。例如上述对比测试中，当key是最简单的数值字符串时，keySet可能反而会更高效，耗时比entrySet少10%。总体来说还是推荐使用entrySet。因为当key很简单时，其性能或许会略低于keySet，但却是可控的；而随着key的复杂化，entrySet的优势将会明显体现出来。当然，我们可以根据实际情况进行选择只遍历key时，keySet方法更为合适，因为entrySet将无用的value也给取出来了，浪费了性能和空间。在上述测试结果中，keySet比entrySet方法耗时少23%。只遍历value时，使用vlaues方法是最佳选择，entrySet会略好于keySet方法。如果你使用TreeMap同时遍历key和value时，entrySet的性能远远高于keySet。这是由TreeMap的查询效率决定的，也就是说，TreeMap查找value的开销较大，明显高于entrySet一次性取出所有key和value的开销。因此，遍历TreeMap时强烈推荐使用entrySet方法。只遍历key时，keySet方法更为合适，因为entrySet将无用的value也给取出来了，浪费了性能和空间。在上述测试结果中，keySet比entrySet方法耗时少24%。只遍历value时，使用vlaues方法是最佳选择，entrySet也明显优于keySet方法。","categories":[],"tags":[{"name":"集合","slug":"集合","permalink":"http://www.fufan.me/tags/集合/"}]},{"title":"JVM专题（四）—— 图解垃圾回收","slug":"JVM专题（四）——-图解垃圾回收","date":"2018-06-09T09:00:00.000Z","updated":"2018-11-09T09:01:08.471Z","comments":true,"path":"2018/06/09/JVM专题（四）——-图解垃圾回收/","link":"","permalink":"http://www.fufan.me/2018/06/09/JVM专题（四）——-图解垃圾回收/","excerpt":"","text":"对于调优之前，我们必须要了解其运行原理，java 的垃圾收集Garbage Collection 通常被称为“GC”，它诞生于1960年 MIT 的 Lisp 语言，经过半个多世纪，目前已经十分成熟了。因此本篇主要从这三个方面来了解:123451. 哪些对象需要被回收？2. 什么时候回收？3. 如何回收？一、谁要被回收java虚拟机在执行java程序的过程中会把它所管理的内存划分为若干个不同是数据区域，这些区域有各自各自的用途。主要包含以下几个部分组成：1、程序计数器程序计数器占用的内存空间我们可以忽略不计，它是每个线程所执行的字节码的行号指示器。2、虚拟机栈java的虚拟机栈是线程私有的，生命周期和线程相同。它描述的是方法执行的内存模型。同时用于存储局部变量、操作数栈、动态链接、方法出口等。3、本地方法栈本地方法栈，类似虚拟机栈，它调用的是是native方法。4、堆堆是jvm中管理内存中最大一块。它是被共享，存放对象实例。也被称为“gc堆”。垃圾回收的主要管理区域5、方法区方法区也是共享的内存区域。它主要存储已被虚拟机加载的类信息、常量、静态变量、即时编译器（jit）编译后的代码数据。以上就是jvm在运行时期主要的内存组成，我们看到常见的内存使用不但存在于堆中，还会存在于其他区域，虽然堆的管理对程序的管理至关重要，但我们不能只局限于这一个区域，特别是当出现内存泄露的时候，我们除了要排查堆内存的情况，还得考虑虚拟机栈的以及方法区域的情况。知道了要对谁以及那些区域进行内存管理，我还需要知道什么时候对这些区域进行垃圾回收。二、什么时候回收在垃圾回收之前，我们必须确定的一件事就是对象是否存活？这就牵扯到了判断对象是否存活的算法了。引用计数算法：给对象中添加一个引用计数器，每当有一个地方引用它时，计数器+1，当引用失效，计数器-1.任何时刻计数器为0的对象就是不可能再被使用的。1给对象中添加一个引用计数器，每当有一个地方引用它时，计数器+1，当引用失效，计数器-1.任何时刻计数器为0的对象就是不可能再被使用的。可达性分析算法：通过一系列称为“GC Roots”的对象作为起始点，从这些节点开始向下搜索，搜索所走过的路径称为引用链，当一个对象到GCRoots没有任何引用链相连的时候，则证明此对象是不可用的。比如如下，右侧的对象是到GCRoot时不可达的，可以判定为可回收对象。在java中，可以作为GCRoot的对象包括以下几种：1234567* 虚拟机栈中引用的对象。* 方法区中静态属性引用的对象。* 方法区中常量引用的对象。* 本地方法中JNI引用的对象。基于以上，我们可以知道，当当前对象到GCRoot中不可达时候，即会满足被垃圾回收的可能。那么是不是这些对象就非死不可，也不一定，此时只能宣判它们存在于一种“缓刑”的阶段，要真正的宣告一个对象死亡。至少要经历两次标记：1234第一次：对象可达性分析之后，发现没有与GCRoots相连接，此时会被第一次标记并筛选。第二次：对象没有覆盖finalize（）方法，或者finalize（）方法已经被虚拟机调用过，此时会被认定为没必要执行。 (大致描述一下finalize流程：当对象变成(GC Roots)不可达时，GC会判断该对象是否覆盖了finalize方法，若未覆盖，则直接将其回收。否则，若对象未执行过finalize方法，将其放入F-Queue队列，由一低优先级线程执行该队列中对象的finalize方法。执行finalize方法完毕后，GC会再次判断该对象是否可达，若不可达，则进行回收，否则，对象“复活”。)三、如何回收上述的两点讲解之后，我们大概明白了，哪些对象会被回收，以及回收的依据是什么，但回收的这个工作实现起来并不简单，首先它需要扫描所有的对象，鉴别谁能够被回收，其次在扫描期间需要 ”stop the world“ 对象能被冻结，不然你刚扫描，他的引用信息有变化，你就等于白做了。分代回收我们从一个object1来说明其在分代垃圾回收算法中的回收轨迹。1、object1新建，出生于新生代的Eden区域。2、minor GC，object1 还存活，移动到Fromsuvivor空间，此时还在新生代。3、minor GC，object1 仍然存活，此时会通过复制算法，将object1移动到ToSuv区域，此时object1的年龄age+1。4、minor GC，object1 仍然存活，此时survivor中和object1同龄的对象并没有达到survivor的一半，所以此时通过复制算法，将fromSuv和Tosuv 区域进行互换，存活的对象被移动到了Tosuv。5、minor GC，object1 仍然存活，此时survivor中和object1同龄的对象已经达到survivor的一半以上（toSuv的区域已经满了），object1被移动到了老年代区域。6、object1存活一段时间后，发现此时object1不可达GcRoots，而且此时老年代空间比率已经超过了阈值,触发了majorGC（也可以认为是fullGC，但具体需要垃圾收集器来联系），此时object1被回收了。fullGC会触发 stop the world。在以上的新生代中，我们有提到对象的age，对象存活于survivor状态下，不会立即晋升为老生代对象，以避免给老生代造成过大的影响，它们必须要满足以下条件才可以晋升：1231、minor gc 之后，存活于survivor 区域的对象的age会+1，当超过（默认）15的时候，转移到老年代。2、动态对象，如果survivor空间中相同年龄所有的对象大小的综合和大于survivor空间的一半，年级大于或等于该年级的对象就可以直接进入老年代。以上采用分代垃圾收集的思想，对一个对象从存活到死亡所经历的历程。期间，在新生代的时刻，会用到复制算法，在老年代时，有可能会用到标记-清楚算法（mark-sweep）算法或者标记-整理算法，这些都是垃圾回收算法基于不同区域的实现，我们看下这几种回收算法的实现原理。四、垃圾收集器垃圾收集器是内存回收的具体实现，不同的厂商提供的垃圾收集器有很大的差别，一般的垃圾收集器都会作用于不同的分代，需要搭配使用。以下是各种垃圾收集器的组合方式：","categories":[],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://www.fufan.me/tags/jvm/"}]},{"title":"java常用集合源码分析之ArrayList遍历方式以及效率比较（五）","slug":"java常用集合源码分析之ArrayList遍历方式以及效率比较（五）","date":"2018-06-08T13:51:00.000Z","updated":"2018-11-11T13:53:57.516Z","comments":true,"path":"2018/06/08/java常用集合源码分析之ArrayList遍历方式以及效率比较（五）/","link":"","permalink":"http://www.fufan.me/2018/06/08/java常用集合源码分析之ArrayList遍历方式以及效率比较（五）/","excerpt":"","text":"一、遍历方式ArrayList支持三种遍历方式。1、第一种，随机访问，它是通过索引值去遍历2、第二种，foreach语句3、第三种，Iterator迭代器方式迭代器是一种模式，它可以使得对于序列类型的数据结构的遍历行为与被遍历的对象分离，即我们无需关心该序列的底层结构是什么样子的。只要拿到这个对象,使用迭代器就可以遍历这个对象的内部。二、几种遍历方式效率的比较从实验结果来看，在遍历ArrayList中，效率最高的是普通for循环遍历，foreach遍历和Iterator方式之间关系不明确，但在增大运行次数时，iterator效率高于foreach。三、效率分析1. 为什么基本的for循环效率高于Iterator遍历？ArrayList实现了RandomAccess接口，RandomAccess接口为ArrayList带来了什么好处呢？123456789As a rule of thumb, a List implementation should implement this interface if, for typical instances of the class, this loop: for (int i=0, n=list.size(); i &lt; n; i++) list.get(i); runs faster than this loop: for (Iterator i=list.iterator(); i.hasNext(); ) i.next();从描述中，可以看出实现RandomAccess接口的集合类，使用for循环的效率会比Iterator高。RandomAccess接口为ArrayList带来的好处：1、可以快速随机访问集合。2、使用快速随机访问（for循环）效率可以高于Iterator。2. 为什么foreach循环效率与Iterator效率有点相似从底层实现中可以看出：foreach不是关键字，它的关键字是for，它的语句是由iterator实现的。forEach就是为了让用iterator循环访问的形式简单，写起来更方便。四、扩展1、基本的for循环的效率一定比iterator迭代器的高吗？不一定，主要还要看集合的数据结构组成。例如，ArrayList和LinkedList中就不同ArrayList实现了RandomAccess随机访问接口，因此它对随机访问的速度快，而基本的for循环中的get()方法，采用的即是随机访问的方法，因而在ArrayList中，for循环速度快。LinkedList采取的是顺序访问方式，iterator中的next()方法，采用的即是顺序访问方法，因此在LinkedList中，使用iterator的速度较快。2、for、foreach、iterator之间的差别1）形式差别2）条件差别for：需要知道集合或数组的大小，而且需要是有序的，不然无法遍历；foreach、iterator：都不需要知道集合或数组的大小，他们都是得到集合内的每个元素然后进行处理。3）多态差别for、foreach：都需要先知道集合的类型，甚至是集合内元素的类型，即需要访问内部的成员，不能实现态；iterator：是一个接口类型，它不关心集合或者数组的类型，而且它还能随时修改和删除集合的元素。4）用法差别for循环：一般用来处理比较简单的有序的，可预知大小的集合或数组foreach：可用于遍历任何集合或数组，而且操作简单易懂，他唯一的不好就是需要了解集合内部类型iterator：是最强大的，它可以随时修改或者删除集合内部的元素，并且是在不需要知道元素和集合的类型的情况下进行的（原因可参考第三点：多态差别），当你需要对不同的容器实现同样的遍历方式时，迭代器是最好的选择！","categories":[{"name":"集合","slug":"集合","permalink":"http://www.fufan.me/categories/集合/"}],"tags":[{"name":"集合","slug":"集合","permalink":"http://www.fufan.me/tags/集合/"}]},{"title":"Java8语法特性讲解（三）—— Examples：Strings, Numbers, Math and Files","slug":"Java8语法特性讲解（三）——-Examples：Strings-Numbers-Math-and-Files-1","date":"2018-05-11T17:21:00.000Z","updated":"2018-11-11T17:21:52.570Z","comments":true,"path":"2018/05/12/Java8语法特性讲解（三）——-Examples：Strings-Numbers-Math-and-Files-1/","link":"","permalink":"http://www.fufan.me/2018/05/12/Java8语法特性讲解（三）——-Examples：Strings-Numbers-Math-and-Files-1/","excerpt":"","text":"Slicing StringsTwo new methods are available on the String class: join and chars. The first method joins any number of strings into a single string with the given delimiter:12String.join(&quot;:&quot;, &quot;foobar&quot;, &quot;foo&quot;, &quot;bar&quot;);// =&gt; foobar:foo:barThe second method chars creates a stream for all characters of the string, so you can use stream operations upon those characters:1234567&quot;foobar:foo:bar&quot; .chars() .distinct() .mapToObj(c -&gt; String.valueOf((char)c)) .sorted() .collect(Collectors.joining());// =&gt; :abforNot only strings but also regex patterns now benefit from streams. Instead of splitting strings into streams for each character we can split strings for any pattern and create a stream to work upon as shown in this example:123456Pattern.compile(&quot;:&quot;) .splitAsStream(&quot;foobar:foo:bar&quot;) .filter(s -&gt; s.contains(&quot;bar&quot;)) .sorted() .collect(Collectors.joining(&quot;:&quot;));// =&gt; bar:foobarAdditionally regex patterns can be converted into predicates. Those predicates can for example be used to filter a stream of strings:12345Pattern pattern = Pattern.compile(&quot;.*@gmail\\\\.com&quot;);Stream.of(&quot;bob@gmail.com&quot;, &quot;alice@hotmail.com&quot;) .filter(pattern.asPredicate()) .count();// =&gt; 1The above pattern accepts any string which ends with @gmail.com and is then used as a Java 8 Predicate to filter a stream of email addresses.Crunching NumbersJava 8 adds additional support for working with unsigned numbers. Numbers in Java had always been signed. Let’s look at Integer for example:An int represents a maximum of 2³² binary digits. Numbers in Java are per default signed, so the last binary digit represents the sign (0 = positive, 1 = negative). Thus the maximum positive signed int is 2³¹ - 1 starting with the decimal zero.You can access this value via Integer.MAX_VALUE:12System.out.println(Integer.MAX_VALUE); // 2147483647System.out.println(Integer.MAX_VALUE + 1); // -2147483648Java 8 adds support for parsing unsigned ints. Let’s see how this works:1234long maxUnsignedInt = (1l &lt;&lt; 32) - 1;String string = String.valueOf(maxUnsignedInt);int unsignedInt = Integer.parseUnsignedInt(string, 10);String string2 = Integer.toUnsignedString(unsignedInt, 10);As you can see it’s now possible to parse the maximum possible unsigned number 2³² - 1 into an integer. And you can also convert this number back into a string representing the unsigned number.This wasn’t possible before with parseInt as this example demonstrates:123456try &#123; Integer.parseInt(string, 10);&#125;catch (NumberFormatException e) &#123; System.err.println(&quot;could not parse signed int of &quot; + maxUnsignedInt);&#125;The number is not parseable as a signed int because it exceeds the maximum of 2³¹ - 1.Do the MathThe utility class Math has been enhanced by a couple of new methods for handling number overflows. What does that mean? We’ve already seen that all number types have a maximum value. So what happens when the result of an arithmetic operation doesn’t fit into its size?12System.out.println(Integer.MAX_VALUE); // 2147483647System.out.println(Integer.MAX_VALUE + 1); // -2147483648As you can see a so called integer overflow happens which is normally not the desired behavior.Java 8 adds support for strict math to handle this problem. Math has been extended by a couple of methods who all ends with exact, e.g. addExact. Those methods handle overflows properly by throwing an ArithmeticException when the result of the operation doesn’t fit into the number type:1234567try &#123; Math.addExact(Integer.MAX_VALUE, 1);&#125;catch (ArithmeticException e) &#123; System.err.println(e.getMessage()); // =&gt; integer overflow&#125;The same exception might be thrown when trying to convert longs to int via toIntExact:1234567try &#123; Math.toIntExact(Long.MAX_VALUE);&#125;catch (ArithmeticException e) &#123; System.err.println(e.getMessage()); // =&gt; integer overflow&#125;Working with FilesThe utility class Files was first introduced in Java 7 as part of Java NIO. The JDK 8 API adds a couple of additional methods which enables us to use functional streams with files. Let’s deep-dive into a couple of code samples.Listing filesThe method Files.list streams all paths for a given directory, so we can use stream operations like filter and sorted upon the contents of the file system.12345678try (Stream&lt;Path&gt; stream = Files.list(Paths.get(&quot;&quot;))) &#123; String joined = stream .map(String::valueOf) .filter(path -&gt; !path.startsWith(&quot;.&quot;)) .sorted() .collect(Collectors.joining(&quot;; &quot;)); System.out.println(&quot;List: &quot; + joined);&#125;The above example lists all files for the current working directory, then maps each path to it’s string representation. The result is then filtered, sorted and finally joined into a string. If you’re not yet familiar with functional streams you should read my Java 8 Stream Tutorial.You might have noticed that the creation of the stream is wrapped into a try/with statement. Streams implement AutoCloseable and in this case we really have to close the stream explicitly since it’s backed by IO operations.The returned stream encapsulates a DirectoryStream. If timely disposal of file system resources is required, the try-with-resources construct should be used to ensure that the stream&apos;s close method is invoked after the stream operations are completed. Finding filesThe next example demonstrates how to find files in a directory or it’s sub-directories.Path start = Paths.get(“”);int maxDepth = 5;try (Streamstream = Files.find(start, maxDepth, (path, attr) -&gt;String.valueOf(path).endsWith(“.js”))) {String joined = stream.sorted().map(String::valueOf).collect(Collectors.joining(“; “));System.out.println(“Found: “ + joined);}The method find accepts three arguments: The directory path start is the initial starting point and maxDepth defines the maximum folder depth to be searched. The third argument is a matching predicate and defines the search logic. In the above example we search for all JavaScript files (filename ends with .js).We can achieve the same behavior by utilizing the method Files.walk. Instead of passing a search predicate this method just walks over any file.12345678910Path start = Paths.get(&quot;&quot;);int maxDepth = 5;try (Stream&lt;Path&gt; stream = Files.walk(start, maxDepth)) &#123; String joined = stream .map(String::valueOf) .filter(path -&gt; path.endsWith(&quot;.js&quot;)) .sorted() .collect(Collectors.joining(&quot;; &quot;)); System.out.println(&quot;walk(): &quot; + joined);&#125;In this example we use the stream operation filter to achieve the same behavior as in the previous example.Reading and writing filesReading text files into memory and writing strings into a text file in Java 8 is finally a simple task. No messing around with readers and writers. The method Files.readAllLines reads all lines of a given file into a list of strings. You can simply modify this list and write the lines into another file via Files.write:123List&lt;String&gt; lines = Files.readAllLines(Paths.get(&quot;res/nashorn1.js&quot;));lines.add(&quot;print(&apos;foobar&apos;);&quot;);Files.write(Paths.get(&quot;res/nashorn1-modified.js&quot;), lines);Please keep in mind that those methods are not very memory-efficient because the whole file will be read into memory. The larger the file the more heap-size will be used.As an memory-efficient alternative you could use the method Files.lines. Instead of reading all lines into memory at once, this method reads and streams each line one by one via functional streams.123456try (Stream&lt;String&gt; stream = Files.lines(Paths.get(&quot;res/nashorn1.js&quot;))) &#123; stream .filter(line -&gt; line.contains(&quot;print&quot;)) .map(String::trim) .forEach(System.out::println);&#125;If you need more fine-grained control you can instead construct a new buffered reader:1234Path path = Paths.get(&quot;res/nashorn1.js&quot;);try (BufferedReader reader = Files.newBufferedReader(path)) &#123; System.out.println(reader.readLine());&#125;Or in case you want to write to a file simply construct a buffered writer instead:1234Path path = Paths.get(&quot;res/output.js&quot;);try (BufferedWriter writer = Files.newBufferedWriter(path)) &#123; writer.write(&quot;print(&apos;Hello World&apos;);&quot;);&#125;Buffered readers also have access to functional streams. The method lines construct a functional stream upon all lines denoted by the buffered reader:12345678Path path = Paths.get(&quot;res/nashorn1.js&quot;);try (BufferedReader reader = Files.newBufferedReader(path)) &#123; long countPrints = reader .lines() .filter(line -&gt; line.contains(&quot;print&quot;)) .count(); System.out.println(countPrints);&#125;code in GITHUB","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://www.fufan.me/tags/java/"}]},{"title":"分布式锁的实现方式（一）——数据库和Redis锁","slug":"分布式锁的实现方式（一）——数据库和Redis锁","date":"2018-04-07T02:29:00.000Z","updated":"2018-11-07T06:07:36.483Z","comments":true,"path":"2018/04/07/分布式锁的实现方式（一）——数据库和Redis锁/","link":"","permalink":"http://www.fufan.me/2018/04/07/分布式锁的实现方式（一）——数据库和Redis锁/","excerpt":"","text":"分布式锁一般有三种实现方式：1. 数据库乐观锁；2. 基于Redis的分布式锁；3. 基于ZooKeeper的分布式锁。4.基于consul的分布式锁。在之前的java多线程系列中（java多线程系列（三）——锁），我已经学习到了各种各样的锁在jdk中的使用。如今大部分互联网系统都是分布式系统，所以实现支持具体业务的高可用分布式锁是我们经常要做的事情。问题什么是锁在单进程的系统中，当存在多个线程可以同时改变某个变量（可变共享变量）时，就需要对变量或代码块做同步，使其在修改这种变量时能够线性执行消除并发修改变量。而同步的本质是通过锁来实现的。为了实现多个线程在一个时刻同一个代码块只能有一个线程可执行，那么需要在某个地方做个标记，这个标记必须每个线程都能看到，当标记不存在时可以设置该标记，其余后续线程发现已经有标记了则等待拥有标记的线程结束同步代码块取消标记后再去尝试设置标记。这个标记可以理解为锁。不同地方实现锁的方式也不一样，只要能满足所有线程都能看得到标记即可。如 Java 中 synchronize 是在对象头设置标记，Lock 接口的实现类基本上都只是某一个 volitile 修饰的 int 型变量其保证每个线程都能拥有对该 int 的可见性和原子修改，linux 内核中也是利用互斥量或信号量等内存数据做标记。除了利用内存数据做锁其实任何互斥的都能做锁（只考虑互斥情况），如流水表中流水号与时间结合做幂等校验可以看作是一个不会释放的锁，或者使用某个文件是否存在作为锁等。只需要满足在对标记进行修改能保证原子性和内存可见性即可。什么是分布式？分布式的 CAP 理论告诉我们:1任何一个分布式系统都无法同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance），最多只能同时满足两项。目前很多大型网站及应用都是分布式部署的，分布式场景中的数据一致性问题一直是一个比较重要的话题。基于 CAP理论，很多系统在设计之初就要对这三者做出取舍。在互联网领域的绝大多数的场景中，都需要牺牲强一致性来换取系统的高可用性，系统往往只需要保证最终一致性。在许多的场景中，我们为了保证数据的最终一致性，需要很多的技术方案来支持，比如分布式事务、分布式锁等。很多时候我们需要保证一个方法在同一时间内只能被同一个线程执行。在单机环境中，通过 Java 提供的并发 API 我们可以解决，但是在分布式环境下，就没有那么简单啦。分布式与单机情况下最大的不同在于其不是多线程而是多进程。多线程由于可以共享堆内存，因此可以简单的采取内存作为标记存储位置。而进程之间甚至可能都不在同一台物理机上，因此需要将标记存储在一个所有进程都能看到的地方。什么是分布式锁？当在分布式模型下，数据只有一份（或有限制），此时需要利用锁的技术控制某一时刻修改数据的进程数。与单机模式下的锁不仅需要保证进程可见，还需要考虑进程与锁之间的网络问题。（我觉得分布式情况下之所以问题变得复杂，主要就是需要考虑到网络的延时和不可靠。。。一个大坑）分布式锁还是可以将标记存在内存，只是该内存不是某个进程分配的内存而是公共内存如 Redis、Memcache。至于利用数据库、文件等做锁与单机的实现是一样的，只要保证标记能互斥就行。我们需要怎样的分布式锁？可以保证在分布式部署的应用集群中，同一个方法在同一时间只能被一台机器-上的一个线程执行。这把锁要是一把可重入锁（避免死锁）这把锁最好是一把阻塞锁（根据业务需求考虑要不要这条）这把锁最好是一把公平锁（根据业务需求考虑要不要这条）有高可用的获取锁和释放锁功能获取锁和释放锁的性能要好基于数据库做分布式锁乐观锁基于表主键唯一做分布式锁思路： 利用主键唯一的特性，如果有多个请求同时提交到数据库的话，数据库会保证只有一个操作可以成功，那么我们就可以认为操作成功的那个线程获得了该方法的锁，当方法执行完毕之后，想要释放锁的话，删除这条数据库记录即可。上面这种简单的实现有以下几个问题：这把锁强依赖数据库的可用性，数据库是一个单点，一旦数据库挂掉，会导致业务系统不可用。这把锁没有失效时间，一旦解锁操作失败，就会导致锁记录一直在数据库中，其他线程无法再获得到锁。这把锁只能是非阻塞的，因为数据的 insert操作，一旦插入失败就会直接报错。没有获得锁的线程并不会进入排队队列，要想再次获得锁就要再次触发获得锁操作。这把锁是非重入的，同一个线程在没有释放锁之前无法再次获得该锁。因为数据中数据已经存在了。这把锁是非公平锁，所有等待锁的线程凭运气去争夺锁。在 MySQL 数据库中采用主键冲突防重，在大并发情况下有可能会造成锁表现象。当然，我们也可以有其他方式解决上面的问题。数据库是单点？搞两个数据库，数据之前双向同步，一旦挂掉快速切换到备库上。没有失效时间？只要做一个定时任务，每隔一定时间把数据库中的超时数据清理一遍。非阻塞的？搞一个 while 循环，直到 insert 成功再返回成功。非重入的？在数据库表中加个字段，记录当前获得锁的机器的主机信息和线程信息，那么下次再获取锁的时候先查询数据库，如果当前机器的主机信息和线程信息在数据库可以查到的话，直接把锁分配给他就可以了。非公平的？再建一张中间表，将等待锁的线程全记录下来，并根据创建时间排序，只有最先创建的允许获取锁。比较好的办法是在程序中生产主键进行防重。悲观锁基于表字段版本号做分布式锁这个策略源于 mysql 的 mvcc 机制，使用这个策略其实本身没有什么问题，唯一的问题就是对数据表侵入较大，我们要为每个表设计一个版本号字段，然后写一条判断 sql 每次进行判断，增加了数据库操作的次数，在高并发的要求下，对数据库连接的开销也是无法忍受的。基于数据库排他锁做分布式锁在查询语句后面增加for update，数据库会在查询过程中给数据库表增加排他锁 (注意： InnoDB 引擎在加锁的时候，只有通过索引进行检索的时候才会使用行级锁，否则会使用表级锁。这里我们希望使用行级锁，就要给要执行的方法字段名添加索引，值得注意的是，这个索引一定要创建成唯一索引，否则会出现多个重载方法之间无法同时被访问的问题。重载方法的话建议把参数类型也加上。)。当某条记录被加上排他锁之后，其他线程无法再在该行记录上增加排他锁。我们可以认为获得排他锁的线程即可获得分布式锁，当获取到锁之后，可以执行方法的业务逻辑，执行完方法之后，通过connection.commit()操作来释放锁。这种方法可以有效的解决上面提到的无法释放锁和阻塞锁的问题。阻塞锁？ for update语句会在执行成功后立即返回，在执行失败时一直处于阻塞状态，直到成功。锁定之后服务宕机，无法释放？使用这种方式，服务宕机之后数据库会自己把锁释放掉。但是还是无法直接解决数据库单点和可重入问题。这里还可能存在另外一个问题，虽然我们对方法字段名使用了唯一索引，并且显示使用 for update 来使用行级锁。但是，MySQL 会对查询进行优化，即便在条件中使用了索引字段，但是否使用索引来检索数据是由 MySQL 通过判断不同执行计划的代价来决定的，如果 MySQL 认为全表扫效率更高，比如对一些很小的表，它就不会使用索引，这种情况下 InnoDB 将使用表锁，而不是行锁。如果发生这种情况就悲剧了。。。还有一个问题，就是我们要使用排他锁来进行分布式锁的 lock，那么一个排他锁长时间不提交，就会占用数据库连接。一旦类似的连接变得多了，就可能把数据库连接池撑爆。优缺点优点：简单，易于理解缺点：会有各种各样的问题（操作数据库需要一定的开销，使用数据库的行级锁并不一定靠谱，性能不靠谱）基于 Redis 做分布式锁使用redis的setNX命令实现分布式锁实现的原理Redis为单进程单线程模式，采用队列模式将并发访问变成串行访问，且多客户端对Redis的连接并不存在竞争关系。redis的SETNX命令可以方便的实现分布式锁。基本命令解析1）setNX（SET if Not eXists）语法：1SETNX key value将 key 的值设为 value ，当且仅当 key 不存在。若给定的 key 已经存在，则 SETNX 不做任何动作。SETNX 是『SET if Not eXists』(如果不存在，则 SET)的简写返回值：设置成功，返回 1 。设置失败，返回 0 。所以我们使用执行下面的命令1SETNX lock.foo &lt;current Unix time + lock timeout + 1&gt;如返回1，则该客户端获得锁，把lock.foo的键值设置为时间值表示该键已被锁定，该客户端最后可以通过DEL lock.foo来释放该锁。如返回0，表明该锁已被其他客户端取得，这时我们可以先返回或进行重试等对方完成或等待锁超时。2）getSET语法：1GETSET key value将给定 key 的值设为 value ，并返回 key 的旧值(old value)。当 key 存在但不是字符串类型时，返回一个错误。返回值：返回给定 key 的旧值。当 key 没有旧值时，也即是， key 不存在时，返回 nil 。3）get语法：1GET key返回值：当 key 不存在时，返回 nil ，否则，返回 key 的值。如果 key 不是字符串类型，那么返回一个错误解决死锁问题如果一个持有锁的客户端失败或崩溃了不能释放锁，该怎么解决？设置超时时间来解决参考博文Java分布式锁看这篇就够了","categories":[{"name":"分布式","slug":"分布式","permalink":"http://www.fufan.me/categories/分布式/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"http://www.fufan.me/tags/分布式/"},{"name":"锁","slug":"锁","permalink":"http://www.fufan.me/tags/锁/"}]},{"title":"Kafka学习笔记（三）——kafka设计的要点","slug":"Kafka学习笔记（三）——kafka设计的要点","date":"2018-04-02T10:36:00.000Z","updated":"2018-11-08T10:37:39.328Z","comments":true,"path":"2018/04/02/Kafka学习笔记（三）——kafka设计的要点/","link":"","permalink":"http://www.fufan.me/2018/04/02/Kafka学习笔记（三）——kafka设计的要点/","excerpt":"","text":"kafka的特点1、吞吐量高吞吐是kafka需要实现的核心目标之一，为此kafka做了以下一些设计：数据磁盘持久化：消息不在内存中cache，直接写入到磁盘，充分利用磁盘的顺序读写性能zero-copy：减少IO操作步骤数据批量发送数据压缩Topic划分为多个partition，提高parallelism2、负载均衡producer根据用户指定的算法，将消息发送到指定的partition存在多个partiiton，每个partition有自己的replica，每个replica分布在不同的Broker节点上多个partition需要选取出lead partition，lead partition负责读写，并由zookeeper负责fail over通过zookeeper管理broker与consumer的动态加入与离开3、拉取系统由于kafka broker会持久化数据，broker没有内存压力，因此，consumer非常适合采取pull的方式消费数据，具有以下几点好处：简化kafka设计consumer根据消费能力自主控制消息拉取速度consumer根据自身情况自主选择消费模式，例如批量，重复消费，从尾端开始消费等4、可扩展性当需要增加broker结点时，新增的broker会向zookeeper注册，而producer及consumer会根据注册在zookeeper上的watcher感知这些变化，并及时作出调整。kafka的使用场景1、消息队列比起大多数的消息系统来说，Kafka有更好的吞吐量，内置的分区，冗余及容错性，这让Kafka成为了一个很好的大规模消息处理应用的解决方案。消息系统一般吞吐量相对较低，但是需要更小的端到端延时，并尝尝依赖于Kafka提供的强大的持久性保障。在这个领域，Kafka足以媲美传统消息系统，如ActiveMR或RabbitMQ。2、行为跟踪Kafka的另一个应用场景是跟踪用户浏览页面、搜索及其他行为，以发布-订阅的模式实时记录到对应的topic里。那么这些结果被订阅者拿到后，就可以做进一步的实时处理，或实时监控，或放到hadoop/离线数据仓库里处理。3、元信息监控作为操作记录的监控模块来使用，即汇集记录一些操作信息，可以理解为运维性质的数据监控吧。4、日志收集日志收集方面，其实开源产品有很多，包括Scribe、Apache Flume。很多人使用Kafka代替日志聚合（log aggregation）。日志聚合一般来说是从服务器上收集日志文件，然后放到一个集中的位置（文件服务器或HDFS）进行处理。然而Kafka忽略掉文件的细节，将其更清晰地抽象成一个个日志或事件的消息流。这就让Kafka处理过程延迟更低，更容易支持多数据源和分布式数据处理。比起以日志为中心的系统比如Scribe或者Flume来说，Kafka提供同样高效的性能和因为复制导致的更高的耐用性保证，以及更低的端到端延迟。kafka性能为了使得Kafka的吞吐率可以线性提高，物理上把Topic分成一个或多个Partition，每个Partition在物理上对应一个文件夹，该文件夹下存储这个Partition的所有消息和索引文件.每个分区都是有序的，不可变的记录序列，不断追加到结构化的提交日志中。分区中的记录每个分配一个连续的id号，称为offset(偏移量)，用于唯一标识分区内的每条记录。实际上，保留在每个消费者基础上的唯一元数据是该消费者在日志中的抵消或位置。这个偏移量是由消费者控制的：消费者通常会在读取记录时线性地推进其偏移量，但实际上，由于位置由消费者控制，因此它可以按任何喜欢的顺序消费记录。例如，消费者可以重置为较旧的offset(偏移量)以重新处理来自过去的数据，或者跳至最近的记录并从“now”开始消费。随你喜欢爱怎么读怎么读,而且这些操作对集群或其他消费者没有太大影响。这样的操作也就说kafka不用考虑加锁的问题,不存在消费完就要删除信息的问题,有效的保证了高吞吐率,这样没有锁竞争，充分发挥了横向的扩展性，吞吐量极高。这也就形成了分布式消费的概念。这里要注意，因为Kafka读取特定消息的时间复杂度为O(1)，即与文件大小无关，所以这里删除过期文件与提高Kafka性能无关,当然kafka也提供了删除旧数据的策略:时间,可以自己设置一个储存的最大时间.partition大小,可以给分区设置最大储存值.consumerp0=&gt;p3三个partition,而partition中的每个message只能被组（Consumer group）中的一个consumer（consumer 线程)消费.也就说一个partition只能被一个消费者消费（一个消费者可以同时消费多个partition）如果consumer从多个partition读到数据，不保证数据间的顺序性，kafka只保证在一个partition上数据是有序的，但多个partition，根据你读的顺序会有不同producerKakfa Broker Leader选举kafka集群是受zookeeper来管理的,这里需要将所有的kafka broker节点一起注册到zookeeper上,而这个过程中只有一个kafka broker能注册成功,在zookeeper上注册一个临时节点,这个kafka broker叫kafka broker Controller,其他的叫kafka broker follower,一旦这个kafka broker Controller发生宕机,临时节点会消失,其他的kafka broker follower会在竞争去zookeeper上注册,产生一个新Leader.(注:Kafka集群中broker之间的关系,不是主从关系，各个broker在集群中地位一样，我们可以随意的增加或删除任何一个broker节点。),还有一种情况是有Controller下的一个follower宕机了,这时Controller会去读取这个follower在zookeeper上所有的partition leader信息(host:port),并且找到这些partition的备份们,让他们选一个成为这个partition的leader.如果该partition的所有的备份都宕机了，则将新的leader设置为-1，等待恢复，等待任一个备份“活”过来，并且选它作为Leader.在Producer向kafka broker推送messagekafka在所有broker中产生一个controller，所有Partition的Leader选举都由controller决定。controller会将Leader的改变直接通过RPC的方式（比Zookeeper Queue的方式更高效）通知需为此作出响应的Broker。每个partition(分区)都有一台服务器充当“leader”，零个或多个服务器充当“follower”。leader处理分区的所有读取和写入请求，而follower被动地复制leader。如果leader失败，其中一个follower将自动成为新leader。每个服务器都充当其中一些分区的leader和其他人的follower，因此负载在集群内平衡良好。举个栗子消息生产者,就是向 kafka broker发消息的客户端。 Producer 采用异步 push 方式, 极大提高 Kafka 系统的吞吐率(可以通过参数控制是采用同步还是异步方式)。 producer 端 , 可以将消息 buffer 起来 , 当消息的条数达到一定阀值时 , 批量发送给 broker 。小数据 IO 太多,会拖慢整体的网络延迟,批量延迟发送事实上提升了网络效率。不过 这也有一定的隐患,比如说当 producer 失效时,那些尚未发送的消息将会丢失。producer将会和Topic下所有partition leader保持 socket 连接 ; 消息由 producer 直接 通过 socket 发送到 broker, 中间不会经过任何 ” 路由层 “. 事实上 , 消息被路由到哪个 partition 上 , 由 producer 客户端决定。 partition leader的位置 (host:port)注册在 zookeeper 中 ,producer 作为 zookeeper client,已经注册了 watch 用来监听 partition leader的变更事件。如上图kafka集群有四个broker,一个topic有四个partition,并且每一个partition都有一个follower(其实就是备份);一个消息流输入之后会先储存一个topic在不同的partition leader中(并行写入),然后在由partition leader同步到各自的备份中.我们加两个broker5,6,这个时候partition的变化partition(分区)机制的优势:当Producer发送消息到broker时，会根据Paritition机制选择将其存储到哪一个Partition。也就是我们上面说的机制，所有消息可以均匀分布到不同的Partition里，这样就实现了负载均衡。如果一个Topic对应一个文件，那这个文件所在的机器I/O将会成为这个Topic的性能瓶颈，而有了Partition后，不同的消息可以并行写入不同broker的不同Partition里，极大的提高了吞吐率。所以说kafka可以水平扩展，也就是扩展partition。segment一个partition可以实现跨服务器,可以一个分区占有一个服务器.参考博文日志收集为什么用kafka","categories":[{"name":"kafka","slug":"kafka","permalink":"http://www.fufan.me/categories/kafka/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://www.fufan.me/tags/kafka/"}]},{"title":"Kafka学习笔记（二）——Kafka shell命令","slug":"Kafka学习笔记（二）——Kafka-shell命令","date":"2018-03-26T10:17:00.000Z","updated":"2018-11-08T10:18:12.049Z","comments":true,"path":"2018/03/26/Kafka学习笔记（二）——Kafka-shell命令/","link":"","permalink":"http://www.fufan.me/2018/03/26/Kafka学习笔记（二）——Kafka-shell命令/","excerpt":"","text":"排查问题的时候可能会用到终端的一些命令，下面列举一下常用的一些命令启动zookeeper1bin/zookeeper-server-start.sh config/zookeeper.properties &amp;启动kafka1bin/kafka-server-start.sh config/server.properties &amp;停止kafka1bin/kafka-server-stop.sh停止zookeeper1bin/zookeeper-server-stop.sh创建topic1bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test展示topic1bin/kafka-topics.sh --list --zookeeper localhost:2181描述topic1bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic生产者1bin/kafka-console-producer.sh --broker-list 130.51.23.95:9092 --topic my-replicated-topic消费者1bin/kafka-console-consumer.sh --zookeeper 130.51.23.95:2181 --topic test --from-beginning","categories":[],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://www.fufan.me/tags/kafka/"}]},{"title":"Kafka学习笔记（一）——Kafka入门","slug":"Kafka学习笔记（一）——Kafka入门","date":"2018-03-18T10:32:00.000Z","updated":"2018-11-08T10:17:19.016Z","comments":true,"path":"2018/03/18/Kafka学习笔记（一）——Kafka入门/","link":"","permalink":"http://www.fufan.me/2018/03/18/Kafka学习笔记（一）——Kafka入门/","excerpt":"","text":"Apache Kafka：一个分布式流处理平台| 对比指标 | kafka | activemq | rabbitmq | rocketmq || — | — | — | — | — || 背景 | Kafka 是LinkedIn 开发的一个高性能、分布式的消息系统，广泛用于日志收集、流式数据处理、在线和离线消息分发等场景 | ActiveMQ是一种开源的，实现了JMS1.1规范的，面向消息(MOM)的中间件， 为应用程序提供高效的、可扩展的、稳定的和安全的企业级消息通信。 | RabbitMQ是一个由erlang开发的AMQP协议（Advanced Message Queue ）的开源实现。 | RocketMQ是阿里巴巴在2012年开源的分布式消息中间件，目前已经捐赠给Apache基金会，已经于2016年11月成为 Apache 孵化项目 ||开发语言 | Java、Scala | Java | Erlang | Java ||协议支持 | 自己实现的一套 | JMS协议 | AMQP | JMS、MQTT ||持久化 | 支持 | 支持 | 支持 | 支持 || producer容错 | 在kafka中提供了acks配置选项, acks=0 生产者在成功写入悄息之前不会等待任何来自服务器的响应 acks=1 只要集群的首领节点收到消息，生产者就会收到一个来自服务器的成功响应 acks=all 只有当所有参与复制的节点全部收到消息时，生产者才会收到一个来自服务器的成功响应,这种模式最安全 | 发送失败后即可重试 | 有ack模型。 ack模型可能重复消息 ，事务模型保证完全一致 | 和kafka类似 || 吞吐量 | kafka具有高的吞吐量，内部采用消息的批量处理，zero-copy机制，数据的存储和获取是本地磁盘顺序批量操作，具有O(1)的复杂度，消息处理的效率很高 | | rabbitMQ在吞吐量方面稍逊于kafka，他们的出发点不一样，rabbitMQ支持对消息的可靠的传递，支持事务，不支持批量的操作；基于存储的可靠性的要求存储可以采用内存或者硬盘。 | kafka在topic数量不多的情况下吞吐量比rocketMq高，在topic数量多的情况下rocketMq比kafka高 || 负载均衡 | kafka采用zookeeper对集群中的broker、consumer进行管理，可以注册topic到zookeeper上；通过zookeeper的协调机制，producer保存对应topic的broker信息，可以随机或者轮询发送到broker上；并且producer可以基于语义指定分片，消息发送到broker的某分片上 | | rabbitMQ的负载均衡需要单独的loadbalancer进行支持 | NamerServer进行负载均衡|相关名词Producer :消息生产者，向Broker发送消息的客户端Consumer :消息消费者，从Broker读取消息的客户端,消费者&lt;=消息的分区数量broker :消息中间件处理节点，一个Kafka节点就是一个broker，一个或者多个Broker可以组成一个Kafka集群topic : 主题，Kafka根据topic对消息进行归类，发布到Kafka集群的每条消息都需要指定一个topicPartition : 分区，物理上的概念，一个topic可以分为多个partition，每个partition内部是有序的，kafka默认根据key%partithon确定消息发送到具体的partitionConsumerGroup : 每个Consumer属于一个特定的Consumer Group，一条消息可以发送到多个不同的Consumer Group，但是一个Consumer Group中只能有一个Consumer能够消费该消息Topic 和 Partition一个Topic中的消息会按照指定的规则(默认是key的hash值%分区的数量，当然你也可以自定义)，发送到某一个分区上面；每一个分区都是一个顺序的、不可变的消息队列，并且可以持续的添加。分区中的消息都被分了一个序列号，称之为偏移量(offset)，在每个分区中此偏移量都是唯一的消费者所持有的元数据就是这个偏移量，也就是消费者在这个log（分区）中的位置。这个偏移量由消费者控制：正常情况当消费者消费消息的时候，偏移量也线性的的增加Consumer 和 Partition通常来讲，消息模型可以分为两种， 队列和发布-订阅式。队列的处理方式 是一个消费者组从队列的一端拉取数据，这个数据消费完就没了。在发布-订阅模型中，消息被广播给所有的消费者，接受到消息的消费者都能处理此消息。在Kafka模型中抽象出来了：消费者组（consumer group）消费者组（consumer group）：每个组中有若干个消费者，如果所有的消费者都在一个组中，那么这个就变成了队列模型；如果笑消费者在不同的组中，这就成了发布-订阅模型一个分区里面的数据只会由一个分组中的消费者处理，同分组的其他消费者不会重复处理消费者组中的消费者数量&lt;=分区数量，如果大于分区数量，多出来的消费者会处于收不到消息的状态，造成不必要的浪费。","categories":[{"name":"kafka","slug":"kafka","permalink":"http://www.fufan.me/categories/kafka/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://www.fufan.me/tags/kafka/"}]},{"title":"Consul学习笔记（一）——安装和命令使用","slug":"Consul学习笔记（一）——安装和命令使用","date":"2018-02-28T10:14:00.000Z","updated":"2018-11-08T10:14:33.306Z","comments":true,"path":"2018/02/28/Consul学习笔记（一）——安装和命令使用/","link":"","permalink":"http://www.fufan.me/2018/02/28/Consul学习笔记（一）——安装和命令使用/","excerpt":"","text":"由于公司目前工作当中微服务用到了consul集群来作为分布式系统中间件，用到了他内置了服务注册与发现框 架、分布一致性协议实现、健康检查、Key/Value存储、多数据中心方案。不再需要依赖其他工具（比如ZooKeeper等）。使用起来也较 为简单。简介Consul 是 HashiCorp 公司推出的开源工具，用于实现分布式系统的服务发现与配置。与其他分布式服务注册与发现的方案，Consul的方案更“一站式”，内置了服务注册与发现框 架、分布一致性协议实现、健康检查、Key/Value存储、多数据中心方案，不再需要依赖其他工具（比如ZooKeeper等）。使用起来也较 为简单。Consul使用Go语言编写，因此具有天然可移植性(支持Linux、windows和Mac OS X)；安装包仅包含一个可执行文件，方便部署，与Docker等轻量级容器可无缝配合 。Service Discovery (服务发现)Health Check (健康检查)Multi Datacenter (多数据中心)Key/Value Storage安装mac：64bit（查看mac位数：打开终端–&gt;”uname -a”）consul_0.6.4_darwin_amd64.zip和consul_0.6.4_web_ui.zip，从consul官网https://www.consul.io/downloads.html进行下载就好（选择好OS和位数）1、解压consul_0.6.4_darwin_amd64.zip2、将解压后的二进制文件consul（上边画红框的部分拷贝到/usr/local/bin下）1sudo scp consul /usr/local/bin/说明：使用sudo是因为权限问题。3、查看是否安装成功调用其命令12345678910111213141516171819202122232425262728 fufan@fufans-MacBook-Pro-2  ~  consul helpUsage: consul [--version] [--help] &lt;command&gt; [&lt;args&gt;]Available commands are: agent Runs a Consul agent catalog Interact with the catalog connect Interact with Consul Connect event Fire a new event exec Executes a command on Consul nodes force-leave Forces a member of the cluster to enter the &quot;left&quot; state info Provides debugging information for operators. intention Interact with Connect service intentions join Tell Consul agent to join cluster keygen Generates a new encryption key keyring Manages gossip layer encryption keys kv Interact with the key-value store leave Gracefully leaves the Consul cluster and shuts down lock Execute a command holding a lock maint Controls node or service maintenance mode members Lists the members of a Consul cluster monitor Stream logs from a Consul agent operator Provides cluster-level tools for Consul operators reload Triggers the agent to reload configuration files rtt Estimates network round trip time between nodes snapshot Saves, restores and inspects snapshots of Consul server state validate Validate config files/directories version Prints the Consul version watch Watch for changes in Consulconsul相关知识点AgentAgent 是一个守护进程运行在Consul集群的每个成员上有Client 和 Server 两种模式所有Agent都可以被调用DNS或者HTTP API,并负责检查和维护同步ClientClient 将所有RPC请求转发至ServerClient 是相对无状态的Client 唯一做的就是参与LAN Gossip PoolClient 只消耗少量的资源和少量的网络带宽Server参与 Raft quorum(一致性判断)响应RPC查询请求维护集群的状态转发查询到Leader 或 远程数据中心Datacenter数据中心私有的低延迟高带宽Consensus (一致性)Consul 使用consensus protocol 来提供CAP(一致性,高可用,分区容错性)Gossip一种协议: 用来保证 最终一致性 , 即: 无法保证在某个时刻, 所有节点状态一致, 但可以保证”最终”一致注册服务服务可以通过提供服务定义或通过对HTTP API进行适当的调用来注册。服务定义是注册服务最常用的方式，所以我们将在这一步中使用这种方法。 我们将建立在上一步中介绍的代理配置。首先，为Consul配置创建一个目录。 Consul将所有配置文件加载到配置目录中，因此Unix系统上的一个通用约定是将目录命名为/etc/consul.d（.d后缀意味着“该目录包含一组配置文件”）。建立服务配置目录:mkdir /etc/consul.d添加文件:echo ‘{“service”: {“name”: “web”, “tags”: [“rails”], “port”: 80}}’ | sudo tee /etc/consul.d/web.json以开发模式启动:consul agent -dev -config-dir=/etc/consul.d以服务方式启动:consul agent -server -bootstrap-expect 2 -data-dir ./tmp/consul -node=n1 -bind=192.168.109.241 -ui-dir ./dist -dc=dc1以客户端方式启动:consul agent -data-dir ./tmp/consul -ui-dir ./dist -bind=192.168.109.204 -dc=dc1加入集群将新节点添加到集群:consul join 192.168.100.101(其中101这个节点是master)显示成员:consul members查看UI管理页面http://192.168.0.70:8500/ui常用命令参数consul agent 命令的常用选项，如下：-data-dir作用：指定agent储存状态的数据目录这是所有agent都必须的对于server尤其重要，因为他们必须持久化集群的状态-config-dir作用：指定service的配置文件和检查定义所在的位置通常会指定为”某一个路径/consul.d”（通常情况下，.d表示一系列配置文件存放的目录）-config-file作用：指定一个要装载的配置文件该选项可以配置多次，进而配置多个配置文件（后边的会合并前边的，相同的值覆盖）-dev作用：创建一个开发环境下的server节点该参数配置下，不会有任何持久化操作，即不会有任何数据写入到磁盘这种模式不能用于生产环境（因为第二条）-bootstrap-expect作用：该命令通知consul server我们现在准备加入的server节点个数，该参数是为了延迟日志复制的启动直到我们指定数量的server节点成功的加入后启动。-node作用：指定节点在集群中的名称该名称在集群中必须是唯一的（默认采用机器的host）推荐：直接采用机器的IP-bind作用：指明节点的IP地址有时候不指定绑定IP，会报Failed to get advertise address: Multiple private IPs found. Please configure one. 的异常-server作用：指定节点为server每个数据中心（DC）的server数推荐至少为1，至多为5所有的server都采用raft一致性算法来确保事务的一致性和线性化，事务修改了集群的状态，且集群的状态保存在每一台server上保证可用性server也是与其他DC交互的门面（gateway）-client作用：指定节点为client，指定客户端接口的绑定地址，包括：HTTP、DNS、RPC默认是127.0.0.1，只允许回环接口访问若不指定为-server，其实就是-client-join作用：将节点加入到集群-datacenter（老版本叫-dc，-dc已经失效）作用：指定机器加入到哪一个数据中心中搭建集群此部分过程同redis集群、zookeeper集群类似问题踩坑googlegithub issues参考博文：Consul官方文档：https://www.consul.io/intro/getting-started/install.htmlConsul 系列博文：http://www.cnblogs.com/java-zhao/archive/2016/04/13/5387105.html使用consul实现分布式服务注册和发现：http://www.tuicool.com/articles/M3QFven","categories":[{"name":"consul","slug":"consul","permalink":"http://www.fufan.me/categories/consul/"}],"tags":[{"name":"consul","slug":"consul","permalink":"http://www.fufan.me/tags/consul/"}]},{"title":"Redis学习笔记（二）——redis安装","slug":"Redis学习笔记（二）——redis安装","date":"2018-02-15T03:05:00.000Z","updated":"2018-11-08T10:13:22.293Z","comments":true,"path":"2018/02/15/Redis学习笔记（二）——redis安装/","link":"","permalink":"http://www.fufan.me/2018/02/15/Redis学习笔记（二）——redis安装/","excerpt":"","text":"由于公司开发笔记本用的是mac，所以我这里介绍一下在mac和linux环境下的redis安装Linux1、下载源码，解压缩后编译源码。1234 $ wget http://download.redis.io/releases/redis-2.8.3.tar.gz$ tar xzf redis-2.8.3.tar.gz$ cd redis-2.8.3$ make2、编译完成后，在Src目录下，有四个可执行文件redis-server、redis-benchmark、redis-cli和redis.conf。然后拷贝到一个目录下。123456mkdir /usr/rediscp redis-server /usr/rediscp redis-benchmark /usr/rediscp redis-cli /usr/rediscp redis.conf /usr/rediscd /usr/redis3、启动Redis服务。1$ redis-server redis.conf4、然后用客户端测试一下是否启动成功。12345$ redis-cliredis&gt; set foo barOKredis&gt; get foo&quot;bar&quot;Macbrew 安装1、使用brew命令安装redis1brew install redis2、启动redis后台方式启动，brew services start redis。这样启动的好处是把控制台关掉后，redis仍然是启动的。当然，如果没有这样的需求，也可以这样启动1redis-server /usr/local/etc/redis.conf3、关闭redis1brew services stop redis4、使用控制台连接redis123redis-cliredis-cli -h 127.0.0.1 -predis.conf配置说明123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426# Redis配置文件样例# Note on units: when memory size is needed, it is possible to specifiy# it in the usual form of 1k 5GB 4M and so forth:## 1k =&gt; 1000 bytes# 1kb =&gt; 1024 bytes# 1m =&gt; 1000000 bytes# 1mb =&gt; 1024*1024 bytes# 1g =&gt; 1000000000 bytes# 1gb =&gt; 1024*1024*1024 bytes## units are case insensitive so 1GB 1Gb 1gB are all the same.# Redis默认不是以守护进程的方式运行，可以通过该配置项修改，使用yes启用守护进程# 启用守护进程后，Redis会把pid写到一个pidfile中，在/var/run/redis.piddaemonize no# 当Redis以守护进程方式运行时，Redis默认会把pid写入/var/run/redis.pid文件，可以通过pidfile指定pidfile /var/run/redis.pid# 指定Redis监听端口，默认端口为6379# 如果指定0端口，表示Redis不监听TCP连接port 6379# 绑定的主机地址# 你可以绑定单一接口，如果没有绑定，所有接口都会监听到来的连接# bind 127.0.0.1# Specify the path for the unix socket that will be used to listen for# incoming connections. There is no default, so Redis will not listen# on a unix socket when not specified.## unixsocket /tmp/redis.sock# unixsocketperm 755# 当客户端闲置多长时间后关闭连接，如果指定为0，表示关闭该功能timeout 0# 指定日志记录级别，Redis总共支持四个级别：debug、verbose、notice、warning，默认为verbose# debug (很多信息, 对开发／测试比较有用)# verbose (many rarely useful info, but not a mess like the debug level)# notice (moderately verbose, what you want in production probably)# warning (only very important / critical messages are logged)loglevel verbose# 日志记录方式，默认为标准输出，如果配置为redis为守护进程方式运行，而这里又配置为标准输出，则日志将会发送给/dev/nulllogfile stdout# To enable logging to the system logger, just set &apos;syslog-enabled&apos; to yes,# and optionally update the other syslog parameters to suit your needs.# syslog-enabled no# Specify the syslog identity.# syslog-ident redis# Specify the syslog facility. Must be USER or between LOCAL0-LOCAL7.# syslog-facility local0# 设置数据库的数量，默认数据库为0，可以使用select &lt;dbid&gt;命令在连接上指定数据库id# dbid是从0到‘databases’-1的数目databases 16################################ SNAPSHOTTING ################################## 指定在多长时间内，有多少次更新操作，就将数据同步到数据文件，可以多个条件配合# Save the DB on disk:## save &lt;seconds&gt; &lt;changes&gt;## Will save the DB if both the given number of seconds and the given# number of write operations against the DB occurred.## 满足以下条件将会同步数据:# 900秒（15分钟）内有1个更改# 300秒（5分钟）内有10个更改# 60秒内有10000个更改# Note: 可以把所有“save”行注释掉，这样就取消同步操作了save 900 1save 300 10save 60 10000# 指定存储至本地数据库时是否压缩数据，默认为yes，Redis采用LZF压缩，如果为了节省CPU时间，可以关闭该选项，但会导致数据库文件变的巨大rdbcompression yes# 指定本地数据库文件名，默认值为dump.rdbdbfilename dump.rdb# 工作目录.# 指定本地数据库存放目录，文件名由上一个dbfilename配置项指定# # Also the Append Only File will be created inside this directory.# # 注意，这里只能指定一个目录，不能指定文件名dir ./################################# REPLICATION ################################## 主从复制。使用slaveof从 Redis服务器复制一个Redis实例。注意，该配置仅限于当前slave有效# so for example it is possible to configure the slave to save the DB with a# different interval, or to listen to another port, and so on.# 设置当本机为slav服务时，设置master服务的ip地址及端口，在Redis启动时，它会自动从master进行数据同步# slaveof &lt;masterip&gt; &lt;masterport&gt;# 当master服务设置了密码保护时，slav服务连接master的密码# 下文的“requirepass”配置项可以指定密码# masterauth &lt;master-password&gt;# When a slave lost the connection with the master, or when the replication# is still in progress, the slave can act in two different ways:## 1) if slave-serve-stale-data is set to &apos;yes&apos; (the default) the slave will# still reply to client requests, possibly with out of data data, or the# data set may just be empty if this is the first synchronization.## 2) if slave-serve-stale data is set to &apos;no&apos; the slave will reply with# an error &quot;SYNC with master in progress&quot; to all the kind of commands# but to INFO and SLAVEOF.#slave-serve-stale-data yes# Slaves send PINGs to server in a predefined interval. It&apos;s possible to change# this interval with the repl_ping_slave_period option. The default value is 10# seconds.## repl-ping-slave-period 10# The following option sets a timeout for both Bulk transfer I/O timeout and# master data or ping response timeout. The default value is 60 seconds.## It is important to make sure that this value is greater than the value# specified for repl-ping-slave-period otherwise a timeout will be detected# every time there is low traffic between the master and the slave.## repl-timeout 60################################## SECURITY #################################### Warning: since Redis is pretty fast an outside user can try up to# 150k passwords per second against a good box. This means that you should# use a very strong password otherwise it will be very easy to break.# 设置Redis连接密码，如果配置了连接密码，客户端在连接Redis时需要通过auth &lt;password&gt;命令提供密码，默认关闭# requirepass foobared# Command renaming.## It is possilbe to change the name of dangerous commands in a shared# environment. For instance the CONFIG command may be renamed into something# of hard to guess so that it will be still available for internal-use# tools but not available for general clients.## Example:## rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52## It is also possilbe to completely kill a command renaming it into# an empty string:## rename-command CONFIG &quot;&quot;################################### LIMITS ##################################### 设置同一时间最大客户端连接数，默认无限制，Redis可以同时打开的客户端连接数为Redis进程可以打开的最大文件描述符数，# 如果设置maxclients 0，表示不作限制。当客户端连接数到达限制时，Redis会关闭新的连接并向客户端返回max Number of clients reached错误信息# maxclients 128# Don&apos;t use more memory than the specified amount of bytes.# When the memory limit is reached Redis will try to remove keys with an# EXPIRE set. It will try to start freeing keys that are going to expire# in little time and preserve keys with a longer time to live.# Redis will also try to remove objects from free lists if possible.## If all this fails, Redis will start to reply with errors to commands# that will use more memory, like SET, LPUSH, and so on, and will continue# to reply to most read-only commands like GET.## WARNING: maxmemory can be a good idea mainly if you want to use Redis as a# &apos;state&apos; server or cache, not as a real DB. When Redis is used as a real# database the memory usage will grow over the weeks, it will be obvious if# it is going to use too much memory in the long run, and you&apos;ll have the time# to upgrade. With maxmemory after the limit is reached you&apos;ll start to get# errors for write operations, and this may even lead to DB inconsistency.# 指定Redis最大内存限制，Redis在启动时会把数据加载到内存中，达到最大内存后，Redis会先尝试清除已到期或即将到期的Key，# 当此方法处理后，仍然到达最大内存设置，将无法再进行写入操作，但仍然可以进行读取操作。# Redis新的vm机制，会把Key存放内存，Value会存放在swap区# maxmemory &lt;bytes&gt;# MAXMEMORY POLICY: how Redis will select what to remove when maxmemory# is reached? You can select among five behavior:# # volatile-lru -&gt; remove the key with an expire set using an LRU algorithm# allkeys-lru -&gt; remove any key accordingly to the LRU algorithm# volatile-random -&gt; remove a random key with an expire set# allkeys-&gt;random -&gt; remove a random key, any key# volatile-ttl -&gt; remove the key with the nearest expire time (minor TTL)# noeviction -&gt; don&apos;t expire at all, just return an error on write operations# # Note: with all the kind of policies, Redis will return an error on write# operations, when there are not suitable keys for eviction.## At the date of writing this commands are: set setnx setex append# incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd# sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby# zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby# getset mset msetnx exec sort## The default is:## maxmemory-policy volatile-lru# LRU and minimal TTL algorithms are not precise algorithms but approximated# algorithms (in order to save memory), so you can select as well the sample# size to check. For instance for default Redis will check three keys and# pick the one that was used less recently, you can change the sample size# using the following configuration directive.## maxmemory-samples 3############################## APPEND ONLY MODE ################################ # Note that you can have both the async dumps and the append only file if you# like (you have to comment the &quot;save&quot; statements above to disable the dumps).# Still if append only mode is enabled Redis will load the data from the# log file at startup ignoring the dump.rdb file.# 指定是否在每次更新操作后进行日志记录，Redis在默认情况下是异步的把数据写入磁盘，如果不开启，可能会在断电时导致一段时间内的数据丢失。# 因为redis本身同步数据文件是按上面save条件来同步的，所以有的数据会在一段时间内只存在于内存中。默认为no# IMPORTANT: Check the BGREWRITEAOF to check how to rewrite the append# log file in background when it gets too big.appendonly no# 指定更新日志文件名，默认为appendonly.aof# appendfilename appendonly.aof# The fsync() call tells the Operating System to actually write data on disk# instead to wait for more data in the output buffer. Some OS will really flush # data on disk, some other OS will just try to do it ASAP.# 指定更新日志条件，共有3个可选值：# no:表示等操作系统进行数据缓存同步到磁盘（快）# always:表示每次更新操作后手动调用fsync()将数据写到磁盘（慢，安全）# everysec:表示每秒同步一次（折衷，默认值）appendfsync everysec# appendfsync no# When the AOF fsync policy is set to always or everysec, and a background# saving process (a background save or AOF log background rewriting) is# performing a lot of I/O against the disk, in some Linux configurations# Redis may block too long on the fsync() call. Note that there is no fix for# this currently, as even performing fsync in a different thread will block# our synchronous write(2) call.## In order to mitigate this problem it&apos;s possible to use the following option# that will prevent fsync() from being called in the main process while a# BGSAVE or BGREWRITEAOF is in progress.## This means that while another child is saving the durability of Redis is# the same as &quot;appendfsync none&quot;, that in pratical terms means that it is# possible to lost up to 30 seconds of log in the worst scenario (with the# default Linux settings).# # If you have latency problems turn this to &quot;yes&quot;. Otherwise leave it as# &quot;no&quot; that is the safest pick from the point of view of durability.no-appendfsync-on-rewrite no# Automatic rewrite of the append only file.# Redis is able to automatically rewrite the log file implicitly calling# BGREWRITEAOF when the AOF log size will growth by the specified percentage.# # This is how it works: Redis remembers the size of the AOF file after the# latest rewrite (or if no rewrite happened since the restart, the size of# the AOF at startup is used).## This base size is compared to the current size. If the current size is# bigger than the specified percentage, the rewrite is triggered. Also# you need to specify a minimal size for the AOF file to be rewritten, this# is useful to avoid rewriting the AOF file even if the percentage increase# is reached but it is still pretty small.## Specify a precentage of zero in order to disable the automatic AOF# rewrite feature.auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb################################## SLOW LOG #################################### The Redis Slow Log is a system to log queries that exceeded a specified# execution time. The execution time does not include the I/O operations# like talking with the client, sending the reply and so forth,# but just the time needed to actually execute the command (this is the only# stage of command execution where the thread is blocked and can not serve# other requests in the meantime).# # You can configure the slow log with two parameters: one tells Redis# what is the execution time, in microseconds, to exceed in order for the# command to get logged, and the other parameter is the length of the# slow log. When a new command is logged the oldest one is removed from the# queue of logged commands.# The following time is expressed in microseconds, so 1000000 is equivalent# to one second. Note that a negative number disables the slow log, while# a value of zero forces the logging of every command.slowlog-log-slower-than 10000# There is no limit to this length. Just be aware that it will consume memory.# You can reclaim memory used by the slow log with SLOWLOG RESET.slowlog-max-len 1024################################ VIRTUAL MEMORY ################################## WARNING! Virtual Memory is deprecated in Redis 2.4### The use of Virtual Memory is strongly discouraged.### WARNING! Virtual Memory is deprecated in Redis 2.4### The use of Virtual Memory is strongly discouraged.# Virtual Memory allows Redis to work with datasets bigger than the actual# amount of RAM needed to hold the whole dataset in memory.# In order to do so very used keys are taken in memory while the other keys# are swapped into a swap file, similarly to what operating systems do# with memory pages.# 指定是否启用虚拟内存机制，默认值为no，# VM机制将数据分页存放，由Redis将访问量较少的页即冷数据swap到磁盘上，访问多的页面由磁盘自动换出到内存中# 把vm-enabled设置为yes，根据需要设置好接下来的三个VM参数，就可以启动VM了vm-enabled no# vm-enabled yes# This is the path of the Redis swap file. As you can guess, swap files# can&apos;t be shared by different Redis instances, so make sure to use a swap# file for every redis process you are running. Redis will complain if the# swap file is already in use.## Redis交换文件最好的存储是SSD（固态硬盘）# 虚拟内存文件路径，默认值为/tmp/redis.swap，不可多个Redis实例共享# *** WARNING *** if you are using a shared hosting the default of putting# the swap file under /tmp is not secure. Create a dir with access granted# only to Redis user and configure Redis to create the swap file there.vm-swap-file /tmp/redis.swap# With vm-max-memory 0 the system will swap everything it can. Not a good# default, just specify the max amount of RAM you can in bytes, but it&apos;s# better to leave some margin. For instance specify an amount of RAM# that&apos;s more or less between 60 and 80% of your free RAM.# 将所有大于vm-max-memory的数据存入虚拟内存，无论vm-max-memory设置多少，所有索引数据都是内存存储的（Redis的索引数据就是keys）# 也就是说当vm-max-memory设置为0的时候，其实是所有value都存在于磁盘。默认值为0vm-max-memory 0# Redis swap文件分成了很多的page，一个对象可以保存在多个page上面，但一个page上不能被多个对象共享，vm-page-size是要根据存储的数据大小来设定的。# 建议如果存储很多小对象，page大小最后设置为32或64bytes；如果存储很大的对象，则可以使用更大的page，如果不确定，就使用默认值vm-page-size 32# 设置swap文件中的page数量由于页表（一种表示页面空闲或使用的bitmap）是存放在内存中的，在磁盘上每8个pages将消耗1byte的内存# swap空间总容量为 vm-page-size * vm-pages## With the default of 32-bytes memory pages and 134217728 pages Redis will# use a 4 GB swap file, that will use 16 MB of RAM for the page table.## It&apos;s better to use the smallest acceptable value for your application,# but the default is large in order to work in most conditions.vm-pages 134217728# Max number of VM I/O threads running at the same time.# This threads are used to read/write data from/to swap file, since they# also encode and decode objects from disk to memory or the reverse, a bigger# number of threads can help with big objects even if they can&apos;t help with# I/O itself as the physical device may not be able to couple with many# reads/writes operations at the same time.# 设置访问swap文件的I/O线程数，最后不要超过机器的核数，如果设置为0，那么所有对swap文件的操作都是串行的，可能会造成比较长时间的延迟，默认值为4vm-max-threads 4############################### ADVANCED CONFIG ################################ Hashes are encoded in a special way (much more memory efficient) when they# have at max a given numer of elements, and the biggest element does not# exceed a given threshold. You can configure this limits with the following# configuration directives.# 指定在超过一定的数量或者最大的元素超过某一临界值时，采用一种特殊的哈希算法hash-max-zipmap-entries 512hash-max-zipmap-value 64# Similarly to hashes, small lists are also encoded in a special way in order# to save a lot of space. The special representation is only used when# you are under the following limits:list-max-ziplist-entries 512list-max-ziplist-value 64# Sets have a special encoding in just one case: when a set is composed# of just strings that happens to be integers in radix 10 in the range# of 64 bit signed integers.# The following configuration setting sets the limit in the size of the# set in order to use this special memory saving encoding.set-max-intset-entries 512# Similarly to hashes and lists, sorted sets are also specially encoded in# order to save a lot of space. This encoding is only used when the length and# elements of a sorted set are below the following limits:zset-max-ziplist-entries 128zset-max-ziplist-value 64# Active rehashing uses 1 millisecond every 100 milliseconds of CPU time in# order to help rehashing the main Redis hash table (the one mapping top-level# keys to values). The hash table implementation redis uses (see dict.c)# performs a lazy rehashing: the more operation you run into an hash table# that is rhashing, the more rehashing &quot;steps&quot; are performed, so if the# server is idle the rehashing is never complete and some more memory is used# by the hash table.# # The default is to use this millisecond 10 times every second in order to# active rehashing the main dictionaries, freeing memory when possible.## If unsure:# use &quot;activerehashing no&quot; if you have hard latency requirements and it is# not a good thing in your environment that Redis can reply form time to time# to queries with 2 milliseconds delay.# 指定是否激活重置哈希，默认为开启activerehashing yes################################## INCLUDES #################################### 指定包含其他的配置文件，可以在同一主机上多个Redis实例之间使用同一份配置文件，而同时各实例又拥有自己的特定配置文件# include /path/to/local.conf# include /path/to/other.conf","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://www.fufan.me/tags/redis/"}]},{"title":"Mac开发提升效率工具（一）","slug":"Mac开发提升效率工具（一）","date":"2018-02-09T06:07:00.000Z","updated":"2018-11-09T06:17:17.315Z","comments":true,"path":"2018/02/09/Mac开发提升效率工具（一）/","link":"","permalink":"http://www.fufan.me/2018/02/09/Mac开发提升效率工具（一）/","excerpt":"","text":"Markdown编辑器目前可供选择的markdown编辑器至少以下有六种:Typora, 简洁轻便免费, 独有的所见即所得, 可在预览状态下编辑, 快捷键丰富, 脚本高亮功能出彩, 导出为pdf后, 排版同样正常, 这点非常难得, 笔者使用的就是该款.Ulysses, 功能强大, 快捷键丰富, 支持目录导入, 支持多终端同步.MWeb Lite, MWeb的微型版, 不收费, 支持目录导入.macdown, 基于mou开发, 轻量, 不支持目录导入.mou 历史悠久, 据说有少量的bug, 具体请参考 Mac 下两款 Markdown 编辑器 Mou/MacDown 大 PK - 简书 .markeditor, 新出的markdown编辑器, 注重视觉感受, 界面不错, 但运行较慢.以上, 推荐开发使用 Typora, PM等使用 Ulysses.Mac重度依赖者开发工具Charles 网络封包分析应用, mac必备.aText 输入增强应用, 比 TextExpander 要人性化许多，并且对中文和第三方输入法的支持都要更好.Dash mac上api集合应用, 几乎包含各种语言的api文档.SnippetsLab 优秀的代码片段管理工具, 轻量, 可基于菜单栏操作.提高效率Alfred 3 神奇的魔法帽, 支持 ① 快速打开application; ② 支持Finder, Calculator, Contacts, Clipboard, iTunes, System, Terminal 等原生应用的各种便捷功能; ③ 支持workflow(工作流).iterm2 增强版的终端应用, 功能强大, 支持分屏, 历史记录, 选中即复制等.Sip 全屏取色应用, 支持快捷键调出(前端福音, 寻找多年, 终于发掘出来了).Keka 压缩或解压缩应用, 开源免费, 压缩比高, 操作便捷, 支持rar等解压, 压缩中文目录后, 在windows下打开不会存在乱码等现象.SwitchHosts 域名host解析必备神器, 支持 windows和Mac的开源工具, mac下只有几百K大小.Scroll Reverser mac滚动方向自定义应用, 可分别设置鼠标和触摸板的上下左右的滚动效果.Size up 分屏应用, 类似Moon的一款应用, 支持上下左右居中、4个角落快速分屏及多屏幕切换.Divvy 另一款分屏应用, 可将屏幕分成多宫格的形式, 然后为每个格子定义快捷键, 遗憾的是不支持多屏幕切换.Graphviz 贝尔实验室开发的有向图/无向图自动布局应用, 支持dot脚本绘制结构图, 流程图等. 可参考教程 利用Graphviz 画结构图 及 使用graphviz绘制流程图 .XMind 思维导图应用, 适合业务及思路梳理.iThoughtsX 另一款思维导图应用, 更加简洁和轻量.Pomodoro One 番茄工作法的一款应用.博主必备ScreenFlow 这或许是mac上最好用的屏幕录制应用.Annotate 屏幕截图批注应用, 令人惊喜的是, 支持划区域gif制作, 教程以及动图制作者必备.Licecap mac上超强大的且极简的gif录制应用, 使用免费, 支持FPS帧率调整且无录制时间限制(笔者用它录制了很多gif动图).KeyCastr 将mac按键显示在屏幕上，分享演示、录制视频或动图时超赞.Mac定制化Bartender 2 菜单栏管理应用, 支持隐藏所有菜单栏图标, 还您一个干净的菜单栏.CDock 任务栏定制应用, 可设置Dock全透明, 还您一个清爽的任务栏.TextBar 自定义菜单栏输出, 支持script运行, 支持H5渲染.Growl 自定义通知样式, 支持多种主题以及颜色, 大小, 渐隐时间等各项参数的自定义.Karabiner 键盘映射修改神器.Keyboard Maestro 键盘大师, mac下功能最为丰富的键盘增强应用.BetterTouchTool mac触摸板增强神器.Übersicht 华丽的桌面自定义应用, 类似于windows的 rainmeter. 支持H5.Today Scripts 个性化通知栏插件, 支持bash脚本(最新的OSX系统不支持).Mountain Tweaks mac隐藏功能开启应用.折腾党玩转MacTripMode 移动热点流量管家, 出差达人的福音.Caffeine 点亮mac, 避免长时间演示ppt而进入到休眠状态.Tickeys 键盘打字风格模拟应用, 支持 Cherry轴等多种风格.keycue 快捷键辅助应用, 帮助记忆快捷键.AirServer IOS连接mac必备.Beyond Compare 文件比较应用, 支持文件, 目录, FTP远程地址比较等.Debookee 网络抓包及数据分析应用.EasyFind 小而强大的文件搜索应用, 媲美windows下的Everything.FileZilla 免费开源的FTP应用.OmniDiskSweeper 硬盘空间扫描应用, 帮助mac减肥.Kaleidoscope 文件和图像比较应用, 支持图片比较, 能与 git, svn 等版本控制工具完美结合.AppCleaner mac应用卸载工具, 结合 AppCleaner 的workflow, 使用效果更佳.TeamViewer 远程开发或协助必备应用.Script Debugger 强大的AppleScript编辑器.Reeder 界面优美的RSS订阅应用.HyperSwitch 带有预览图的快速切换, 作用同Command+Tab.Cool retro term 终端变身复古显示器.Fruit Juice 电池管理应用, 帮助延迟电池的使用时间.终端命令ohmyzsh shell有很多种, 常用的bash就是之一. 而zsh是shell中目前最强大的, 没有之一. ohmyzsh屏蔽了zsh复杂的配置, 真正达到了一键上手zsh的目的.12345# Mac下自动安装&amp;设置wget https://github.com/robbyrussell/oh-my-zsh/raw/master/tools/install.sh -O - | sh# 设置 shell 默认使用 zshchsh -s /bin/zsh#在 dock 栏右键退出终端, 然后重启终端~安装完成autojump 支持快速跳转到曾经打开过的目录下,安装方法: brew install autojump .tmux 终端复用工具, 支持在终端中创建不依赖于终端的窗口, 安装方法: brew install tmux. 使用请参考：Tmux使用手册.Chrome Extension篇自制Iheader 监听和修改http/https请求/响应头，可用于渗透测试（笔者修改请求头用于跨域调试，特别好用）。Qrcode URL生成二维码，如果网页中包含选中文本，则生成选中文本的二维码。前端有关React Developer Tools React开发者工具.Redux DevTools Redux开发者工具.FE助手 百度推出的前端助手, 具有很多便捷的小功能.YSlow 雅虎性能分析工具.Postman 接口调试工具, 几乎支持所有类型的http(s)请求.EditThisCookie cookie编辑工具, 可用于获取或设置http only等cookie的值.JSONView json预览工具, 接口调试必备.Page Ruler 页面尺子, 页面重构或者严格按照设计图开发页面时, 将会非常有用.Alexa Traffic Rank 网站Alexa排名查看工具.工作效率有关OneTab 快速关闭并存储浏览器当前窗口所有Tab页, 可用于下次一键全部恢复.Merge Windows 合并所有浏览器窗口为同一个窗口.Vimium 键盘党必备, 使用vim命令管理页面.Vysor mac上直接操作 Android 手机, 且可远程共享手机操作界面.网站有关Octotree Github重度依赖者必备, 提供左侧边栏, 快速浏览仓库内容.AdBlock 超强去广告工具, 最受欢迎的Chrome扩展, 拥有超过4000万用户.阅读模式 快速开启阅读模式, 进入沉浸式阅读, 并非支持所有网页.Blipshot 全网页截图工具, 支持自动垂直滚动, 截取网页的所有内容为一张图片.Chrome Extension开发相关文章Sample Extensions - Google Chrome图灵社区: 合集 : Chrome扩展及应用开发Google Chrome扩展开发系列Alfred workflow我曾经耗费巨大的精力, 试图在计算机的使用效率上找到一条优化的策略, 一直以来都收效甚微. 直到遇上Alfred, 它强大的工作流机制, 才让我明白原来计算机可以这么玩. 因为它彻底解决了输入输出的痛点, 极大的减少了程序之间的切换成本以及按键成本.传统意义上, 使用mac时, 为了查询一个单词, 或者翻译一个单词, 我们要么经历五步: ① 手动打开浏览器 ② 进入谷歌首页 ③ 选中输入框 ④ 输入或粘贴查询单词, 然后空格并加上”翻译” 两个字, 然后再回车 ⑤ 等待浏览器展示查询结果; 要么经历四步: ① 打开翻译应用(比如自带词典) ② 输入或粘贴查询单词 ③ 翻译应用输出查询结果 ④ 查询过后, 一般都需要Command+Q退出应用(否则Dock栏将会全是未关闭的应用).查询单词这个场景中, 我们至少需要兴师动众, 切换或打开一个应用两次, 定位输入框一次, 输入或复制粘贴一次. 且查询结果页也会挡住当前的工作区, 使得我们分心, 甚至忘记自己刚刚在做啥. 五个字 — 体验不流畅.而 Alfred 的工作流正是为了解决这个问题而设计的. 如果我们使用网友开发的 有道词典 的 workflow, 最快只需通过两次按键便可获取单词的查询结果. 假如: 为了查询单词”workflow”, 我会选中单词所在区域, 然后按住 Option+Y 键(我已将有道翻译的快捷键设置为 Option+Y), 单词查询结果就出来了, 而且不需要切换应用, 同时查询结果也较少的挡住工作区了.以上 Alfred 界面使用了少数派的主题.有关其他的workflow 内容, 请移步 Alfred Workflows , 那里会有更多非常不错的 workflow 供您选用.","categories":[{"name":"mac","slug":"mac","permalink":"http://www.fufan.me/categories/mac/"}],"tags":[{"name":"mac","slug":"mac","permalink":"http://www.fufan.me/tags/mac/"},{"name":"工具","slug":"工具","permalink":"http://www.fufan.me/tags/工具/"}]},{"title":"Redis学习笔记（一）——初识redis","slug":"Redis学习笔记（一）——初识redis","date":"2018-02-07T13:02:00.000Z","updated":"2018-11-08T10:11:15.679Z","comments":true,"path":"2018/02/07/Redis学习笔记（一）——初识redis/","link":"","permalink":"http://www.fufan.me/2018/02/07/Redis学习笔记（一）——初识redis/","excerpt":"","text":"由于加入公司一段时间了，公司的爬虫项目太过于依赖redis这个中间件，包括线上的几次重大异常都是由redis来引起的（redis分布式锁问题，阿里云redis主从和分布式服务异常问题等）。所以找时间专门学习一下关于redis的一些使用。Redis学习链接：菜鸟redis教程官网教程Redis简介Redis是一个速度极快的非关系数据库，也就是我们所说的NoSQL数据库(non-relational database)，它可以存储键(key)与5种不同类型的值(value)之间的映射(mapping)，可以将存储在内存的键值对数据持久化到硬盘，可以使用复制特性来扩展读性能，还可以使用客户端分片来扩展性能，并且它还提供了多种语言的API。Redis 是完全开源免费的，遵守BSD协议，是一个高性能的key-value数据库。Redis支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用。Redis支持数据的备份，即master-slave模式的数据备份。优势性能极高： Redis能读的速度是110000次/s,写的速度是81000次/s丰富的数据类型：Redis支持二进制案例的 Strings, Lists, Hashes, Sets 及 Ordered Sets 数据类型操作。原子性：Redis的所有操作都是原子性的，同时Redis还支持对几个操作全并后的原子性执行。丰富的特性：支持 publish/subscribe, 通知, key 过期等等特性。分布式锁：很多分布式系统中可以用redis的setnx和getset来做分布式锁数据结构字符串 - Stringstring类型是二进制安全的。意思是redis的string可以包含任何数据。比如jpg图片或者序列化的对象 。string类型是Redis最基本的数据类型，一个键最大能存储512MB。api示例：12345127.0.0.1:6379&gt; set username fufanOK127.0.0.1:6379&gt; get username&quot;fufan&quot;127.0.0.1:6379&gt;哈希 - hashRedis hash 是一个键值对集合。Redis hash是一个string类型的field和value的映射表，hash特别适合用于存储对象。每个 hash 可以存储 232 - 1 键值对（40多亿）。api示例：1234567127.0.0.1:6379&gt; HSET user name fufan password 121314(integer) 2127.0.0.1:6379&gt; HGETALL user1) &quot;name&quot;2) &quot;fufan&quot;3) &quot;password&quot;4) &quot;121314&quot;列表 - listRedis 列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素导列表的头部（左边）或者尾部（右边）。列表最多可存储 2^32 - 1 元素 (4294967295, 每个列表可存储40多亿)。api示例：1234567891011127.0.0.1:6379&gt; LPUSH userlist fufan(integer) 1127.0.0.1:6379&gt; LPUSH userlist yajun(integer) 2127.0.0.1:6379&gt; LPUSH userlist luwei(integer) 3127.0.0.1:6379&gt; LPOP userlist&quot;luwei&quot;127.0.0.1:6379&gt; RPOP userlist&quot;fufan&quot;127.0.0.1:6379&gt;集合 - setRedis的Set是string类型的无序集合。集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是O(1)根据集合内元素的唯一性，第二次插入的元素将被忽略。集合中最大的成员数为 2^32 -1(4294967295,每个集合可存储40多亿个成员)12345678910127.0.0.1:6379&gt; sadd product phone(integer) 1127.0.0.1:6379&gt; sadd product pad(integer) 1127.0.0.1:6379&gt; sadd product tv(integer) 1127.0.0.1:6379&gt; SMEMBERS product1) &quot;tv&quot;2) &quot;phone&quot;3) &quot;pad&quot;有序集合 - sorted setRedis zset 和 set一样也是string类型元素的集合,且不允许重复的成员。不同的是每个元素都会关联一个double类型的分数。redis正是通过分数来为集合中的成员进行从小到大的排序。zset的成员是唯一的,但分数(score)却可以重复。1234567891011121314151617127.0.0.1:6379&gt; ZADD fufan 100 chinese(integer) 1127.0.0.1:6379&gt; ZADD fufan 100 math(integer) 1127.0.0.1:6379&gt; ZADD fufan 135 english(integer) 1127.0.0.1:6379&gt; ZRANGE fufan 0 10 withscores1) &quot;chinese&quot;2) &quot;100&quot;3) &quot;math&quot;4) &quot;100&quot;5) &quot;english&quot;6) &quot;135&quot;127.0.0.1:6379&gt; ZRANGEBYSCORE fufan 100 1001) &quot;chinese&quot;2) &quot;math&quot;127.0.0.1:6379&gt;","categories":[{"name":"redis","slug":"redis","permalink":"http://www.fufan.me/categories/redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://www.fufan.me/tags/redis/"}]},{"title":"centos安装python3.6","slug":"centos安装python3-6","date":"2017-11-23T09:08:00.000Z","updated":"2018-11-23T09:09:10.937Z","comments":true,"path":"2017/11/23/centos安装python3-6/","link":"","permalink":"http://www.fufan.me/2017/11/23/centos安装python3-6/","excerpt":"","text":"安装可能用到的依赖1yum install -y openssl-devel bzip2-devel expat-devel gdbm-devel readline-devel sqlite-devel下载Python3.6.5源码1wget https://www.python.org/ftp/python/3.6.5/Python-3.6.5.tgz解压到当前目录1tar -xzvf Python-3.6.5.tgz进入解压后的目录1cd Python-3.6.5安装到/usr/local/python目录，不用事先创建python目录1./configure --prefix=/usr/local/python编译1make安装1make altinstall进入/usr/bin目录1cd /usr/bin重命名python2的快捷方式12mv python python.bakmv pip pip.bak创建python3与pip3软连接12ln -s /usr/local/python/bin/python3.6 /usr/bin/pythonln -s /usr/local/python/bin/pip3.6 /usr/bin/pip","categories":[{"name":"python","slug":"python","permalink":"http://www.fufan.me/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://www.fufan.me/tags/python/"}]},{"title":"JVM专题（三）——GC算法 垃圾收集器","slug":"JVM专题（三）——GC算法-垃圾收集器","date":"2017-11-20T16:30:00.000Z","updated":"2018-11-08T10:38:47.127Z","comments":true,"path":"2017/11/21/JVM专题（三）——GC算法-垃圾收集器/","link":"","permalink":"http://www.fufan.me/2017/11/21/JVM专题（三）——GC算法-垃圾收集器/","excerpt":"","text":"这篇文件将给大家介绍GC都有哪几种算法，以及JVM都有那些垃圾回收器，它们的工作原理。概述垃圾收集 Garbage Collection 通常被称为“GC”，它诞生于1960年 MIT 的 Lisp 语言，经过半个多世纪，目前已经十分成熟了。 jvm 中，程序计数器、虚拟机栈、本地方法栈都是随线程而生随线程而灭，栈帧随着方法的进入和退出做入栈和出栈操作，实现了自动的内存清理，因此，我们的内存垃圾回收主要集中于 java 堆和方法区中，在程序运行期间，这部分内存的分配和使用都是动态的.对象存活判断判断对象是否存活一般有两种方式：引用计数：每个对象有一个引用计数属性，新增一个引用时计数加1，引用释放时计数减1，计数为0时可以回收。此方法简单，无法解决对象相互循环引用的问题。可达性分析（Reachability Analysis）：从GC Roots开始向下搜索，搜索所走过的路径称为引用链。当一个对象到GC Roots没有任何引用链相连时，则证明此对象是不可用的。不可达对象。在Java语言中，GC Roots包括：虚拟机栈中引用的对象。方法区中类静态属性实体引用的对象。方法区中常量引用的对象。本地方法栈中JNI引用的对象。垃圾收集算法标记 -清除算法“标记-清除”（Mark-Sweep）算法，如它的名字一样，算法分为“标记”和“清除”两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收掉所有被标记的对象。之所以说它是最基础的收集算法，是因为后续的收集算法都是基于这种思路并对其缺点进行改进而得到的。它的主要缺点有两个：一个是效率问题，标记和清除过程的效率都不高；另外一个是空间问题，标记清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致，当程序在以后的运行过程中需要分配较大对象时无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。复制算法“复制”（Copying）的收集算法，它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。这样使得每次都是对其中的一块进行内存回收，内存分配时也就不用考虑内存碎片等复杂情况，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效。只是这种算法的代价是将内存缩小为原来的一半，持续复制长生存期的对象则导致效率降低。标记-压缩算法复制收集算法在对象存活率较高时就要执行较多的复制操作，效率将会变低。更关键的是，如果不想浪费50%的空间，就需要有额外的空间进行分配担保，以应对被使用的内存中所有对象都100%存活的极端情况，所以在老年代一般不能直接选用这种算法。根据老年代的特点，有人提出了另外一种“标记-整理”（Mark-Compact）算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存分代收集算法GC分代的基本假设：绝大部分对象的生命周期都非常短暂，存活时间短。“分代收集”（Generational Collection）算法，把Java堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。在新生代中，每次垃圾收集时都发现有大批对象死去，只有少量存活，那就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集。而老年代中因为对象存活率高、没有额外空间对它进行分配担保，就必须使用“标记-清理”或“标记-整理”算法来进行回收。垃圾收集器1如果说收集算法是内存回收的方法论，垃圾收集器就是内存回收的具体实现Serial收集器串行收集器是最古老，最稳定以及效率高的收集器，可能会产生较长的停顿，只使用一个线程去回收。新生代、老年代使用串行回收；新生代复制算法、老年代标记-压缩；垃圾收集的过程中会Stop The World（服务暂停）参数控制：-XX:+UseSerialGC 串行收集器ParNew收集器ParNew收集器其实就是Serial收集器的多线程版本。新生代并行，老年代串行；新生代复制算法、老年代标记-压缩参数控制：-XX:+UseParNewGC ParNew收集器-XX:ParallelGCThreads 限制线程数量Parallel收集器Parallel Scavenge收集器类似ParNew收集器，Parallel收集器更关注系统的吞吐量。可以通过参数来打开自适应调节策略，虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或最大的吞吐量；也可以通过参数控制GC的时间不大于多少毫秒或者比例；新生代复制算法、老年代标记-压缩参数控制：-XX:+UseParallelGC 使用Parallel收集器+ 老年代串行Parallel Old 收集器Parallel Old是Parallel Scavenge收集器的老年代版本，使用多线程和“标记－整理”算法。这个收集器是在JDK 1.6中才开始提供参数控制： -XX:+UseParallelOldGC 使用Parallel收集器+ 老年代并行CMS收集器CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。目前很大一部分的Java应用都集中在互联网站或B/S系统的服务端上，这类应用尤其重视服务的响应速度，希望系统停顿时间最短，以给用户带来较好的体验。从名字（包含“Mark Sweep”）上就可以看出CMS收集器是基于“标记-清除”算法实现的，它的运作过程相对于前面几种收集器来说要更复杂一些，整个过程分为4个步骤，包括：初始标记（CMS initial mark）并发标记（CMS concurrent mark）重新标记（CMS remark）并发清除（CMS concurrent sweep）其中初始标记、重新标记这两个步骤仍然需要“Stop The World”。初始标记仅仅只是标记一下GC Roots能直接关联到的对象，速度很快，并发标记阶段就是进行GC Roots Tracing的过程，而重新标记阶段则是为了修正并发标记期间，因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段稍长一些，但远比并发标记的时间短。由于整个过程中耗时最长的并发标记和并发清除过程中，收集器线程都可以与用户线程一起工作，所以总体上来说，CMS收集器的内存回收过程是与用户线程一起并发地执行。老年代收集器（新生代使用ParNew）优点: 并发收集、低停顿缺点: 产生大量空间碎片、并发阶段会降低吞吐量参数控制：-XX:+UseConcMarkSweepGC 使用CMS收集器-XX:+ UseCMSCompactAtFullCollection Full GC后，进行一次碎片整理；整理过程是独占的，会引起停顿时间变长-XX:+CMSFullGCsBeforeCompaction 设置进行几次Full GC后，进行一次碎片整理-XX:ParallelCMSThreads 设定CMS的线程数量（一般情况约等于可用CPU数量）G1收集器G1是目前技术发展的最前沿成果之一，HotSpot开发团队赋予它的使命是未来可以替换掉JDK1.5中发布的CMS收集器。与CMS收集器相比G1收集器有以下特点：空间整合，G1收集器采用标记整理算法，不会产生内存空间碎片。分配大对象时不会因为无法找到连续空间而提前触发下一次GC。可预测停顿，这是G1的另一大优势，降低停顿时间是G1和CMS的共同关注点，但G1除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为N毫秒的时间片段内，消耗在垃圾收集上的时间不得超过N毫秒，这几乎已经是实时Java（RTSJ）的垃圾收集器的特征了。上面提到的垃圾收集器，收集的范围都是整个新生代或者老年代，而G1不再是这样。使用G1收集器时，Java堆的内存布局与其他收集器有很大差别，它将整个Java堆划分为多个大小相等的独立区域（Region），虽然还保留有新生代和老年代的概念，但新生代和老年代不再是物理隔阂了，它们都是一部分（可以不连续）Region的集合。G1的新生代收集跟ParNew类似，当新生代占用达到一定比例的时候，开始出发收集。和CMS类似，G1收集器收集老年代对象会有短暂停顿。收集步骤：1、标记阶段，首先初始标记(Initial-Mark),这个阶段是停顿的(Stop the World Event)，并且会触发一次普通Mintor GC。对应GC log:GC pause (young) (inital-mark)2、Root Region Scanning，程序运行过程中会回收survivor区(存活到老年代)，这一过程必须在young GC之前完成。3、Concurrent Marking，在整个堆中进行并发标记(和应用程序并发执行)，此过程可能被young GC中断。在并发标记阶段，若发现区域对象中的所有对象都是垃圾，那个这个区域会被立即回收(图中打X)。同时，并发标记过程中，会计算每个区域的对象活性(区域中存活对象的比例)。4、Remark, 再标记，会有短暂停顿(STW)。再标记阶段是用来收集 并发标记阶段 产生新的垃圾(并发阶段和应用程序一同运行)；G1中采用了比CMS更快的初始快照算法:snapshot-at-the-beginning (SATB)。5、Copy/Clean up，多线程清除失活对象，会有STW。G1将回收区域的存活对象拷贝到新区域，清除Remember Sets，并发清空回收区域并把它返回到空闲区域链表中。6、复制/清除过程后。回收区域的活性对象已经被集中回收到深蓝色和深绿色区域。常用的收集器组合服务器新生代GC策略老年老代GC策略说明组合1SerialSerial OldSerial和Serial Old都是单线程进行GC，特点就是GC时暂停所有应用线程。组合2SerialCMS+Serial OldCMS（Concurrent Mark Sweep）是并发GC，实现GC线程和应用线程并发工作，不需要暂停所有应用线程。另外，当CMS进行GC失败时，会自动使用Serial Old策略进行GC。组合3ParNewCMS使用-XX:+UseParNewGC选项来开启。ParNew是Serial的并行版本，可以指定GC线程数，默认GC线程数为CPU的数量。可以使用-XX:ParallelGCThreads选项指定GC的线程数。如果指定了选项-XX:+UseConcMarkSweepGC选项，则新生代默认使用ParNew GC策略。组合4ParNewSerial Old使用-XX:+UseParNewGC选项来开启。新生代使用ParNew GC策略，年老代默认使用Serial Old GC策略。组合5Parallel ScavengeSerial OldParallel Scavenge策略主要是关注一个可控的吞吐量：应用程序运行时间 / (应用程序运行时间 + GC时间)，可见这会使得CPU的利用率尽可能的高，适用于后台持久运行的应用程序，而不适用于交互较多的应用程序。组合6Parallel ScavengeParallel OldParallel Old是Serial Old的并行版本组合7G1GCG1GC-XX:+UnlockExperimentalVMOptions -XX:+UseG1GC #开启；-XX:MaxGCPauseMillis =50 #暂停时间目标；-XX:GCPauseIntervalMillis =200 #暂停间隔目标；-XX:+G1YoungGenSize=512m #年轻代大小；-XX:SurvivorRatio=6 #幸存区比例系统吞吐量和系统并发数以及响时间的关系理解","categories":[{"name":"java虚拟机","slug":"java虚拟机","permalink":"http://www.fufan.me/categories/java虚拟机/"}],"tags":[{"name":"java虚拟机","slug":"java虚拟机","permalink":"http://www.fufan.me/tags/java虚拟机/"}]},{"title":"JVM专题（二）——JVM内存模型","slug":"JVM专题（二）——JVM内存模型","date":"2017-11-18T19:30:00.000Z","updated":"2018-11-08T10:38:30.873Z","comments":true,"path":"2017/11/19/JVM专题（二）——JVM内存模型/","link":"","permalink":"http://www.fufan.me/2017/11/19/JVM专题（二）——JVM内存模型/","excerpt":"","text":"所有的Java开发人员可能会遇到这样的困惑？我该为堆内存设置多大空间呢？OutOfMemoryError的异常到底涉及到运行时数据的哪块区域？该怎么解决呢？其实如果你经常解决服务器性能问题，那么这些问题就会变的非常常见，了解JVM内存也是为了服务器出现性能问题的时候可以快速的了解那块的内存区域出现问题，以便于快速的解决生产故障。先看一张图，这张图能很清晰的说明JVM内存结构布局。JVM内存结构主要有三大块：堆内存、方法区和栈。堆内存是JVM中最大的一块由年轻代和老年代组成，而年轻代内存又被分成三部分，Eden空间、From Survivor空间、To Survivor空间,默认情况下年轻代按照8:1:1的比例来分配；方法区存储类信息、常量、静态变量等数据，是线程共享的区域，为与Java堆区分，方法区还有一个别名Non-Heap(非堆)；栈又分为java虚拟机栈和本地方法栈主要用于方法的执行。在通过一张图来了解如何通过参数来控制各区域的内存大小控制参数-Xms设置堆的最小空间大小。-Xmx设置堆的最大空间大小。-XX:NewSize设置新生代最小空间大小。-XX:MaxNewSize设置新生代最大空间大小。-XX:PermSize设置永久代最小空间大小。-XX:MaxPermSize设置永久代最大空间大小。-Xss设置每个线程的堆栈大小。没有直接设置老年代的参数，但是可以设置堆空间大小和新生代空间大小两个参数来间接控制。老年代空间大小=堆空间大小-年轻代大空间大小从更高的一个维度再次来看JVM和系统调用之间的关系方法区和对是所有线程共享的内存区域；而java栈、本地方法栈和程序员计数器是运行是线程私有的内存区域。下面我们详细介绍每个区域的作用Java堆（Heap）对于大多数应用来说，Java堆（Java Heap）是Java虚拟机所管理的内存中最大的一块。Java堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配内存。Java堆是垃圾收集器管理的主要区域，因此很多时候也被称做“GC堆”。如果从内存回收的角度看，由于现在收集器基本都是采用的分代收集算法，所以Java堆中还可以细分为：新生代和老年代；再细致一点的有Eden空间、From Survivor空间、To Survivor空间等。根据Java虚拟机规范的规定，Java堆可以处于物理上不连续的内存空间中，只要逻辑上是连续的即可，就像我们的磁盘空间一样。在实现时，既可以实现成固定大小的，也可以是可扩展的，不过当前主流的虚拟机都是按照可扩展来实现的（通过-Xmx和-Xms控制）。如果在堆中没有内存完成实例分配，并且堆也无法再扩展时，将会抛出OutOfMemoryError异常。方法区（Method Area）方法区（Method Area）与Java堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然Java虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做Non-Heap（非堆），目的应该是与Java堆区分开来。对于习惯在HotSpot虚拟机上开发和部署程序的开发者来说，很多人愿意把方法区称为“永久代”（Permanent Generation），本质上两者并不等价，仅仅是因为HotSpot虚拟机的设计团队选择把GC分代收集扩展至方法区，或者说使用永久代来实现方法区而已。Java虚拟机规范对这个区域的限制非常宽松，除了和Java堆一样不需要连续的内存和可以选择固定大小或者可扩展外，还可以选择不实现垃圾收集。相对而言，垃圾收集行为在这个区域是比较少出现的，但并非数据进入了方法区就如永久代的名字一样“永久”存在了。这个区域的内存回收目标主要是针对常量池的回收和对类型的卸载，一般来说这个区域的回收“成绩”比较难以令人满意，尤其是类型的卸载，条件相当苛刻，但是这部分区域的回收确实是有必要的。根据Java虚拟机规范的规定，当方法区无法满足内存分配需求时，将抛出OutOfMemoryError异常。方法区有时被称为持久代（PermGen）。所有的对象在实例化后的整个运行周期内，都被存放在堆内存中。堆内存又被划分成不同的部分：伊甸区(Eden)，幸存者区域(Survivor Sapce)，老年代（Old Generation Space）。方法的执行都是伴随着线程的。原始类型的本地变量以及引用都存放在线程栈中。而引用关联的对象比如String，都存在在堆中。程序计数器（Program Counter Register）程序计数器（Program Counter Register）是一块较小的内存空间，它的作用可以看做是当前线程所执行的字节码的行号指示器。在虚拟机的概念模型里（仅是概念模型，各种虚拟机可能会通过一些更高效的方式去实现），字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。由于Java虚拟机的多线程是通过线程轮流切换并分配处理器执行时间的方式来实现的，在任何一个确定的时刻，一个处理器（对于多核处理器来说是一个内核）只会执行一条线程中的指令。因此，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各条线程之间的计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。如果线程正在执行的是一个Java方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址；如果正在执行的是Natvie方法，这个计数器值则为空（Undefined）。JVM栈（JVM Stacks）与程序计数器一样，Java虚拟机栈（Java Virtual Machine Stacks）也是线程私有的，它的生命周期与线程相同。虚拟机栈描述的是Java方法执行的内存模型：每个方法被执行的时候都会同时创建一个栈帧（Stack Frame）用于存储局部变量表、操作栈、动态链接、方法出口等信息。每一个方法被调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。局部变量表存放了编译期可知的各种基本数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference类型，它不等同于对象本身，根据不同的虚拟机实现，它可能是一个指向对象起始地址的引用指针，也可能指向一个代表对象的句柄或者其他与此对象相关的位置）和returnAddress类型（指向了一条字节码指令的地址）。其中64位长度的long和double类型的数据会占用2个局部变量空间（Slot），其余的数据类型只占用1个。局部变量表所需的内存空间在编译期间完成分配，当进入一个方法时，这个方法需要在帧中分配多大的局部变量空间是完全确定的，在方法运行期间不会改变局部变量表的大小。在Java虚拟机规范中，对这个区域规定了两种异常状况：如果线程请求的栈深度大于虚拟机所允许的深度，将抛出StackOverflowError异常；如果虚拟机栈可以动态扩展（当前大部分的Java虚拟机都可动态扩展，只不过Java虚拟机规范中也允许固定长度的虚拟机栈），当扩展时无法申请到足够的内存时会抛出OutOfMemoryError异常。本地方法栈（Native Method Stacks）本地方法栈（Native Method Stacks）与虚拟机栈所发挥的作用是非常相似的，其区别不过是虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则是为虚拟机使用到的Native方法服务。虚拟机规范中对本地方法栈中的方法使用的语言、使用方式与数据结构并没有强制规定，因此具体的虚拟机可以自由实现它。甚至有的虚拟机（譬如Sun HotSpot虚拟机）直接就把本地方法栈和虚拟机栈合二为一。与虚拟机栈一样，本地方法栈区域也会抛出StackOverflowError和OutOfMemoryError异常。哪儿的OutOfMemoryError对内存结构清晰的认识同样可以帮助理解不同OutOfMemoryErrors：1Exception in thread “main”: java.lang.OutOfMemoryError: Java heap space原因：对象不能被分配到堆内存中1Exception in thread “main”: java.lang.OutOfMemoryError: PermGen space原因：类或者方法不能被加载到持久代。它可能出现在一个程序加载很多类的时候，比如引用了很多第三方的库；1Exception in thread “main”: java.lang.OutOfMemoryError: Requested array size exceeds VM limit原因：创建的数组大于堆内存的空间1Exception in thread “main”: java.lang.OutOfMemoryError: request &lt;size&gt; bytes for &lt;reason&gt;. Out of swap space?原因：分配本地分配失败。JNI、本地库或者Java虚拟机都会从本地堆中分配内存空间。1Exception in thread “main”: java.lang.OutOfMemoryError: &lt;reason&gt; &lt;stack trace&gt;（Native method）原因：同样是本地方法内存分配失败，只不过是JNI或者本地方法或者Java虚拟机发现","categories":[{"name":"java虚拟机","slug":"java虚拟机","permalink":"http://www.fufan.me/categories/java虚拟机/"}],"tags":[{"name":"java虚拟机","slug":"java虚拟机","permalink":"http://www.fufan.me/tags/java虚拟机/"}]},{"title":"关于hashCode和equals方法很好的解释","slug":"hashCode和equals方法","date":"2017-11-17T07:22:00.000Z","updated":"2018-11-17T07:23:13.288Z","comments":true,"path":"2017/11/17/hashCode和equals方法/","link":"","permalink":"http://www.fufan.me/2017/11/17/hashCode和equals方法/","excerpt":"","text":"12345678910111213141516171819201.hashcode是用来查找的，如果你学过数据结构就应该知道，在查找和排序这一章有 例如内存中有这样的位置 0 1 2 3 4 5 6 7 而我有个类，这个类有个字段叫ID,我要把这个类存放在以上8个位置之一，如果不用hashcode而任意存放，那么当查找时就需要到这八个位置里挨个去找，或者用二分法一类的算法。 但如果用hashcode那就会使效率提高很多。 我们这个类中有个字段叫ID,那么我们就定义我们的hashcode为ID％8，然后把我们的类存放在取得得余数那个位置。比如我们的ID为9，9除8的余数为1，那么我们就把该类存在1这个位置，如果ID是13，求得的余数是5，那么我们就把该类放在5这个位置。这样，以后在查找该类时就可以通过ID除 8求余数直接找到存放的位置了。 2.但是如果两个类有相同的hashcode怎么办那（我们假设上面的类的ID不是唯一的），例如9除以8和17除以8的余数都是1，那么这是不是合法的，回答是：可以这样。那么如何判断呢？在这个时候就需要定义 equals了。 也就是说，我们先通过 hashcode来判断两个类是否存放某个桶里，但这个桶里可能有很多类，那么我们就需要再通过 equals 来在这个桶里找到我们要的类。 那么。重写了equals()，为什么还要重写hashCode()呢？ 想想，你要在一个桶里找东西，你必须先要找到这个桶啊，你不通过重写hashcode()来找到桶，光重写equals()有什么用啊总结当你真要的需要重写equals方法，这两点一定要记住：A.如果两个对象相等（equals() 返回 true），那么它们的 hashCode()一定要相同；B.如果两个对象hashCode()相等，它们并不一定相等（equals() 不一定返回 true）。如果重写的equals方法但不重写hashCode，都是耍流氓，会有意想不到的结果。重写hashCode方法时，尽可能将所有用于相等比较的参数都参与hashCode的计算。建立hash散列表的意义就是在于，提高查询效率，当数据量大时，尤为显著。","categories":[{"name":"java","slug":"java","permalink":"http://www.fufan.me/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://www.fufan.me/tags/java/"}]},{"title":"Https通信过程（一）（转载）","slug":"Https通信过程（一）（转载）","date":"2017-11-14T03:34:00.000Z","updated":"2018-11-14T03:35:57.795Z","comments":true,"path":"2017/11/14/Https通信过程（一）（转载）/","link":"","permalink":"http://www.fufan.me/2017/11/14/Https通信过程（一）（转载）/","excerpt":"","text":"什么是 HTTPS ?不管是使用手机还是电脑上网，都离不开数据的通讯现在互联网上传输数据，普遍使用的是超文本传输协议，即 HTTP (HyperText Transfer Protocol)所以，我们以前在上网的时候，会发现所有的网址都有一个 http:// 前缀：简单而言，HTTP 协议定义了一套规范，让客户端或浏览器可以和服务器正常通信，完成数据传输但是，HTTP 使用明文传输，比如你输入账号/密码提交登录：很有可能被中间人窃听，从而造成数据泄露，所以说 HTTP 是不安全的，现代浏览器会在地址栏提示连接不安全：为什么 HTTPS 是安全的？只要把传输的数据加密，那么通信就是安全的，前提是除通信双方外，任何第三方无法解密：在上图示例中，通信的数据经过加密，即使被中间人窃听到了，它也无法知道数据内容HTTPS 是怎么实现安全通信的？加密传输确实安全，但是客户端把数据加密后，服务器怎么解密呢？又怎样保证中间人窃听到密文后无法解密呢？答案是：使用对称加密技术什么是对称加密？简单而言，通信双方各有一把相同的钥匙（所谓对称），客户端把数据加密锁起来后，传送给服务器，服务器再用钥匙解密。同理，服务器加密后传输给客户端的数据，客户端也可以用钥匙解密那么，新的问题又出现了：怎样在通信之前，给双方分配两把一样的钥匙呢？如果真的只有两个人要通信的话，可以简单的私下见个面分配好，以后要通信的时候用就行。但是，实际通信往往是一个服务器和成千上万的客户端之间，总不能让每个人都和服务器先私下见个面另外，即使使用了对称加密技术，如果一方保管不善的话，也有可能钥匙被人偷了去复制一个，这样就存在很大的安全隐患，最好是每个客户端每次和服务器通信都用不同的密钥一个简单的解决方案是：客户端在每次请求通信之前，先和服务器协商，通过某种办法，产生只有双方知道的对称密钥这个过程就是所谓：密钥交换(Key Exchange)密钥交换算法有很多种实现，常见的有：Deffie-Hellman 密钥交换算法RSA 密钥交换算法本文以较简单的 RSA 密钥交换为例简单而言，RSA 密钥交换算法需要客户端向服务器提供一个 Pre-Master-Key，然后通信双方再生成 Master-Key，最后根据 Master-Key 产生后续一系列所需要的密钥，包括传输数据的时候使用的对称密钥那么，客户端怎么把 Pre-Master-Key 告诉服务器呢？直接明文传输么？我们之前说过，没加密的通信都会被窃听，是不安全的似乎进入死循环了：为了加密通信，需要先把 Pre-Master-Key 传送给服务器，但是这个传送又必须要加密我们引入一种新的加密技术：非对称加密什么是非对称加密？简单而言，服务器可以生成一对不同的密钥（所谓非对称），一把私自保存，称为私钥；一把向所有人公开，称为公钥 这对密钥有这样的性质：公钥加密后的数据只有私钥能解密，私钥加密后的数据只有公钥能解密非对称加密的一种经典实现叫 RSA 算法，这种加密算法最早 1977 年由罗纳德·李维斯特（Ron Rivest）、阿迪·萨莫尔（Adi Shamir）和伦纳德·阿德曼（Leonard Adleman）一起提出的，RSA 就是他们三人姓氏开头字母拼在一起组成的有了非对称加密的技术后，事情就好办了：客户端把 Pre-Master-Key 用服务器的公钥加密后，传送给服务器因为只有服务器才有私钥，所以只有服务器才能解密数据，获取客户端发送来的 Pre-Master-Key具体的交互过程：客户端向服务器索取公钥 PublicKey；服务器将公钥发给客户端（这里没有保密需求，因为公钥是向所有人公开的）；客户端使用服务器的公钥 PublicKey 把 Pre-Master-Key 加密成密文，传送给服务器；服务器用私钥 PrivateKey 解密密文，获取到客户端发送的 Pre-Master-Key;看起来很完美，但是第 2 步骤又引发了一个新问题：由于互联网是公开的，服务器发送给客户端的公钥可能在传送过程中被中间人截获并篡改（所谓中间人攻击 Man-in-the-middle attack，缩写：MITM）因为中间人也可以生成一对非对称密钥，它会截获服务器发送的公钥，然后把它自己的公钥 MiddleMan-PublicKey 发送给客户端，进行欺骗可怜我们的客户端，竟然信以为真！然后傻乎乎的把自己的 Pre-Master-Key 用 MiddleMan-PublicKey 加密后，发给中间人怎么解决这个问题？问题等价于：客户端怎么确定收到的公钥，真的就是服务器的公钥？想一想你乘高铁、坐飞机的时候，怎么向工作人员证明你是你答案很简单，到公安局（权威机构 英文名：Authority）出个身份证明（Certificate）身份证上记载了你的号码、姓名、年龄、照片、住址，还有签发机关、有效期等所以，服务器也想办法到权威机构 (Authority) 办一张证书 Certificate，上面记载了服务器的域名、公钥、所属单位，还有签发机关、有效期等当客户端收到服务器发过来的证书后，只要证书不是伪造的，那么上面记载的公钥肯定也就是真的！不过，这里又有个新问题：怎么证明证书不是伪造的？我们介绍一种防伪手段：签名（Signature）什么是签名？我们在生活、工作过程中，经常遇到需要签名的情况：刷信用卡、签合同等，用来证明这是本人的行为。签名之所以可信，是因为理论上每个人的签名都有生理学基础，别人是无法伪造的，就像你的指纹一样所以，只要服务器发送的证书上有权威机构 Authority 的签名，就可以确信证书是颁发给服务器的，而不是谁伪造的这就相当于，只要你的请假条上有领导的签名，那么 HR 就会确信领导已经审批同意你请假了如果说人类签名使用纸笔，那么计算机的数字化签名怎么实现呢？答案是使用非对称加密技术：数字证书认证机构（Certificate Authority，简称 CA）生成一对公/私钥；服务器将自己的域名、公钥等信息提交给 CA 审查；CA 审查无误，使用私钥把服务器信息的摘要加密，生成的密文就是所谓签名（Signature）；CA 把服务器的信息、签名、有效期等信息集合到一张证书上，颁发给服务器；客户端收到服务器发送的证书后，使用 CA 的公钥解密签名，获得服务器信息的摘要，如果和证书上记录的服务器信息的摘要一致，说明服务器信息是经过 CA 认可的什么是信息摘要？简单来说，就是一段任意长的数据，经过信息摘要处理后，可以得到一段固定长度的数据，比如 32 字节，只要原始数据有任意变动，生成的信息摘要都不一样但是，在第5步骤又有一个新问题：客户端怎么知道 CA 的公钥？答案：与生俱来世界上的根 CA 就那么几家，浏览器或者操作系统在出厂的时候，已经内置了这些机构的自签名证书，上面记录他们的公钥信息，你也可以在需要的时候手动安装 CA 证书以 Windows 系统为例：至此，HTTPS 通信过程已经很明朗了：操作系统/浏览器 自带了 CA 根证书；客户端因此可以验证服务器发送的证书真实性，从而获取到服务器的公钥；有了服务器的公钥，客户端就可以把 Pre-Master-Key 传送给服务器；服务器获取到 Pre-Master-Key 后，通过后续产生的对称密钥，就可以和客户端加密通信了。总结本文简述了 HTTPS 通讯过程的基本原理，涉及到了对称加密、非对称加密、信息摘要、签名、密钥交换等技术基础，以及发行机构、数字证书等概念具体的 HTTPS 实现细节还要复杂得多，这里并没有展开讲，但是并不影响对 HTTPS 不熟悉的读者对原理有基本的认知博文链接看完你就知道什么是 HTTPS 了","categories":[{"name":"http","slug":"http","permalink":"http://www.fufan.me/categories/http/"}],"tags":[{"name":"http","slug":"http","permalink":"http://www.fufan.me/tags/http/"}]},{"title":"Java面试总结积累（基础篇）之JVM问题(三)","slug":"Java面试总结积累（基础篇）之JVM问题-三","date":"2017-11-13T11:44:00.000Z","updated":"2018-11-07T02:01:43.507Z","comments":true,"path":"2017/11/13/Java面试总结积累（基础篇）之JVM问题-三/","link":"","permalink":"http://www.fufan.me/2017/11/13/Java面试总结积累（基础篇）之JVM问题-三/","excerpt":"","text":"一. 垃圾回收算法的实现原理垃圾回收算法有一下几种：1. 引用计数法对于一个A对象，只要有任何一个对象引用了A，则A的引用计算器就加1，当引用失效时，引用计数器减1.只要A的引用计数器值为0，则对象A就不可能再被使用。存在两个问题：1.无法处理循环引用的问题，因此在Java的垃圾回收器中，没有使用该算法；2.伴随一个加法操作和减法操作，对系统性能会有一定的影响。2. 标记清除法标记清除法是现代垃圾回收算法的思想基础。分为两个阶段：标记阶段和清除阶段。缺点是可能产生的最大的问题就是空间碎片。标记清除算法先通过根节点标记所有可达对象，然后清除所有不可达对象，完成垃圾回收。3. 复制算法将原有的内存空间分为两块相同的存储空间，每次只使用一块，在垃圾回收时，将正在使用的内存块中存活对象复制到未使用的那一块内存空间中，之后清除正在使用的内存块中的所有对象，完成垃圾回收。在java中的新生代串行垃圾回收器中，使用了复制算法的思想，新生代分为eden空间、from空间和to空间3个部分，其中from和to空间可以看做用于复制的两块大小相同、可互换角色的内存空间块（同一时间只能有一个被当做当前内存空间使用，另一个在垃圾回收时才发挥作用），from和to空间也称为survivor空间，用于存放未被回收的对象。在java中的新生代串行垃圾回收器中，使用了复制算法的思想，新生代分为eden空间、from空间和to空间3个部分，其中from和to空间可以看做用于复制的两块大小相同、可互换角色的内存空间块（同一时间只能有一个被当做当前内存空间使用，另一个在垃圾回收时才发挥作用），from和to空间也称为survivor空间，用于存放未被回收的对象。4. 标记压缩算法类似标记清除算法，也是先将可达对象标记，然后扫描的时候，在清除不可达对象之前，先做了一步压缩到内存空间的一端的操作，减少了内存空间中的碎片。这样做避免的碎片的产生，又不需要两块相同的内存空间，因此性价比高。5. 分代算法将内存空间根据对象的特点不同进行划分，选择合适的垃圾回收算法，以提高垃圾回收的效率。通常，java虚拟机会将所有的新建对象都放入称为新生代的内存空间。新生代的特点是：对象朝生夕灭，大约90%的对象会很快回收，因此，新生代比较适合使用复制算法。当一个对象经过几次垃圾回收后依然存活，对象就会放入老年代的内存空间，在老年代中，几乎所有的对象都是经过几次垃圾回收后依然得以存活的，因此，认为这些对象在一段时间内，甚至在程序的整个生命周期将是常驻内存的。老年代的存活率是很高的，如果依然使用复制算法回收老年代，将需要复制大量的对象。这种做法是不可取的，根据分代的思想，对老年代的回收使用标记清除或者标记压缩算法可以提高垃圾回收效率。分代的思想被现有的虚拟机广泛使用，几乎所有的垃圾回收器都区分新生代和老年代。6. 分区算法分区算法将整个堆空间划分为连续的不同小区间。每一个小区间都独立使用，独立回收。算法优点是：可以控制一次回收多少个小区间通常，相同的条件下，堆空间越大，一次GC所需的时间就越长，从而产生的停顿时间就越长。为了更好的控制GC产生的停顿时间，将一块大的内存区域分割成多个小块，根据目标的停顿时间，每次合理的回收若干个小区间，而不是整个堆空间，从而减少一个GC的停顿时间。如图所示：二. 当出现了内存溢出，你怎么排错。内存溢出可能存在的情况OOMStack Overflow运行时常量池溢出方法区溢出1. OOM原因:即堆区溢出，对象过多导致内存疯长，大于预设置的最大堆容量后报出java heap space解决方法:1)首先确认是内存泄露(Memory Leak)还是内存溢出(Memory Overflow);2)如果是内存泄漏引起的,查看GC Roots引用链,找出为什么无法被垃圾回收的原因;3)如果是内存溢出,检查虚拟机的堆参数(-Xmx最大值和-Xms最小值),对比物理内存看是否可以调大;4)由于本人在工作中负责过分布式爬虫的项目，发现内存疯长也可能是堆外内存的问题，和并发线程相关。严重的需要使用jheap dump下内存结构进行分析，利用工具jprofile、eclipse等，后续会有专门的专题理一下这一块的处理办法。2. Stack Overflow虚拟机栈和本地方法栈溢出原因:在单线程下,虚拟机栈容量太小或者定义了大量的本地变量,会抛出SO;解决方法:增大虚拟机栈容量，可以通过-Xss参数来设定栈容量;3. PermGen space原因:代码在运行时创建了大量的常量,超出了常量池上限;解决方法:通过修改-XX:PermSize和-XX:MaxPermSize参数来修改方法区大小,从而修改常量池大小;4. 方法区溢出原因:在运行时,ClassLoader动态加载了大量的Class信息,超出方法区上限;解决方法:通过修改-XX:PermSize和-XX:MaxPermSize参数来修改方法区大小;下面罗列一些经常用到的jvm参数:用到的JVM启动参数:-Xss2M 设置JVM栈内存大小-Xms20M 设置堆内存初始值-Xmx20M 设置堆内存最大值-Xmn10M 设置堆内存中新生代大小-XX:SurvivorRatio=8设置堆内存中新生代Eden 和 Survivor 比例-XX:PermSize=10M设置方法区内存初始值-XX:MaxPermSize=10M设置方法区内存最大值三. JVM内存模型的相关知识了解多少，比如重排序，内存屏障，happen-before，主内存，工作内存等。重排序通常是编译器或运行时环境为了优化程序性能而采取的对指令进行重新排序执行的一种手段。重排序分为两类：编译期重排序和运行期重排序，分别对应编译时和运行时环境。内存屏障（Memory Barrier，或有时叫做内存栅栏，Memory Fence）是一种CPU指令，用于控制特定条件下的重排序和内存可见性问题。Java编译器也会根据内存屏障的规则禁止重排序。主内存存在于主内存中的变量和对象，可以被所有线程共享工作内存只存在于各个线程中，被线程私有，线程要读取主内存变量时，必须拷贝一份主内存中的到工作内存，不能直接从主内存中读取。不同线程之间无法直接访问其他线程工作内存中的变量，线程间变量值的传递需要通过主内存来完成。四. 如何实现内存可见性要实现共享变量的可见性，必须保证两点线程修改后的共享变量值能够及时从工作内存中刷新到主内存中其他线程能够及时把共享变量的最新值从主内存更新到自己的工作内存中synchronized实现可见性线程解锁前，必须把共享变量的最新值刷新到主内存中线程加锁时，将清空工作内存中共享变量的值，从而使用共享变量时需要从主存中重新读取最新的值线程执行互斥代码的过程获得互斥锁清空工作内存从主内存拷贝变量的最新副本到工作内存执行代码将更改后的共享变量的值刷新到主内存中释放互斥锁volatile实现可见性能够保证volatile变量的可见性不能保证volatile变量复合操作的原子原理：通过加入内存屏障和禁止重排序优化来实现的（1.在每个volatile写操作前插入StoreStore屏障，在写操作后插入StoreLoad屏障；2.在每个volatile读操作前插入LoadLoad屏障，在读操作后插入LoadStore屏障）通俗的讲：volatile变量在每次被线程访问时，都强迫从主内存中重读该变量的值，而当该变量发生变化时，又会强迫将最新的值刷新到主内存。这样任何时刻，不同的线程总能看到该变量的最新值。线程写volatile变量的过程：改变线程工作内存中volatile变量副本的值将改变后的副本的值从工作内存刷新到主内存线程读volatile变量的过程：从主内存中读取volatile变量的最新值到线程的工作内存中从工作内存中读取volatile变量的副本synchronized vs volatilevolatile不需要加锁，比synchronized更轻量级，不会阻塞线程synchronized既能保证可见性，又能保证原子性，而volatile只能保证可见性，无法保证原子性五. 简单说说你了解的类加载器，可以打破双亲委派么，怎么打破。JDK 默认提供了如下几种ClassLoaderBootstrp loaderExtClassLoaderAppClassLoader为什么要有三个类加载器，一方面是分工，各自负责各自的区块，另一方面为了实现委托模型。java采用了委托模型机制，这个机制简单来讲，就是“类装载器有载入类的需求时，会先请示其Parent使用其搜索路径帮忙载入，如果Parent 找不到,那么才由自己依照自己的搜索路径搜索类”为什么要使用这种双亲委托模式呢？因为这样可以避免重复加载，当父亲已经加载了该类的时候，就没有必要子ClassLoader再加载一次。考虑到安全因素，我们试想一下，如果不使用这种委托模式，那我们就可以随时使用自定义的String来动态替代java核心api中定义类型，这样会存在非常大的安全隐患，而双亲委托的方式，就可以避免这种情况，因为String已经在启动时被加载，所以用户自定义类是无法加载一个自定义的ClassLoader。/思考：假如我们自己写了一个java.lang.String的类，我们是否可以替换调JDK本身的类？/答案是否定的。我们不能实现。为什么呢？我看很多网上解释是说双亲委托机制解决这个问题，其实不是非常的准确。因为双亲委托机制是可以打破的，你完全可以自己写一个classLoader来加载自己写的java.lang.String类，但是你会发现也不会加载成功，具体就是因为针对java.*开头的类，jvm的实现中已经保证了必须由bootstrp来加载。定义自已的ClassLoader既然JVM已经提供了默认的类加载器，为什么还要定义自已的类加载器呢？因为Java中提供的默认ClassLoader，只加载指定目录下的jar和class，如果我们想加载其它位置的类或jar时，比如：我要加载网络上的一个class文件，通过动态加载到内存之后，要调用这个类中的方法实现我的业务逻辑。在这样的情况下，默认的ClassLoader就不能满足我们的需求了，所以需要定义自己的ClassLoader。定义自已的类加载器分为两步：继承java.lang.ClassLoader重写父类的findClass方法读者可能在这里有疑问，父类有那么多方法，为什么偏偏只重写findClass方法？因为JDK已经在loadClass方法中帮我们实现了ClassLoader搜索类的算法，当在loadClass方法中搜索不到类时，loadClass方法就会调用findClass方法来搜索类，所以我们只需重写该方法即可。如没有特殊的要求，一般不建议重写loadClass搜索类的算法。六. 讲讲JAVA的反射机制。JAVA反射机制是在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法;对于任意一个对象，都能够调用它的任意方法和属性;这种动态获取信息以及动态调用对象方法的功能称为java语言的反射机制。在java中反射是很重要的，在现在的很多框架中也都运用了反射的概念，比如spring中的aop机制就是利用反射原理，动态代理，其实说起动态代理就必须要说反射。在写java我们使用对象的时候一般都是使用new的方式来创建对象，这些将要在程序中使用的对象在编译期间都已经知道了，但是编译期间和运行期间还不一样。假如有类Person，Student类extends了Person（都有空构造函数），Person person=new Stuednt();在编译的时候person是Person类型的，但是在运行的时候确实Student的，有时候我们在程序运行期间根据类去生成相应的对象然后进行一系列的操作，这就是反射，所谓的反射个人理解就是在JVM运行期间通过查找到相应的类，通过类获取其属性以及方法来创造对象。参考博文jvm参数设置大全浅谈CMS垃圾收集器与G1收集器JVM的GC策略java jvm内存管理/gc策略/参数设置","categories":[{"name":"面试","slug":"面试","permalink":"http://www.fufan.me/categories/面试/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://www.fufan.me/tags/jvm/"},{"name":" 面试","slug":"面试","permalink":"http://www.fufan.me/tags/面试/"}]},{"title":"Java面试总结积累（基础篇）之JVM问题(二)","slug":"Java面试总结积累（基础篇）之JVM问题-二","date":"2017-11-10T02:41:00.000Z","updated":"2018-11-06T11:44:21.407Z","comments":true,"path":"2017/11/10/Java面试总结积累（基础篇）之JVM问题-二/","link":"","permalink":"http://www.fufan.me/2017/11/10/Java面试总结积累（基础篇）之JVM问题-二/","excerpt":"","text":"一. JVM中一次完整的GC流程是怎样的，对象如何晋升到老年代，说说你知道的几种主要的JVM参数。GC流程当现在有一个新的对象产生，那么对象一定需要内存空间，于是现在就需要为该对象进行内存空间的申请；首先会判断eden是否有内存空间，如果此时有内存空间，则直接将新对象保存在eden；但是如果此时eden的内存空间不足，那么会自动执行一个MinorGC操作，将eden的无用内存空间进行清理，清理之后会继续判断eden的内存空间是否充足？如果内存空间充足，则将新的对象直接在eden进行空间分配；如果执行了MinorGC之后发现eden的内存依然不足，那么这个时候会进行survivor判断，如果survivor有剩余空间，则将eden的部分活跃对象保存在survivor，那么随后继续判断eden的内存空间是否充足，如果充足，则在eden进行新对象的空间分配；如果此时survivor也已经没有内存空间了，则继续判断老年区，如果此时老年区空间充足，则将survivor中的活跃对象保存到老年代，而后survivor就会存现有空余空间，随后eden将活跃对象保存在survivor之中，而后在eden里为新对象开辟空间；如果这个时候老年代也满了，那么这个时候将产生M ajor GC（FullGC），进行老年代的内存清理。如果老年代执行了Full GC之后发现依然无法进行对象的保存，就会产生OOM异常“OutOfMemoryError”jvm参数-Xmx3550m：设置JVM最大堆内存为3550M。-Xms3550m：设置JVM初始堆内存为3550M。此值可以设置与-Xmx相同，以避免每次垃圾回收完成后JVM重新分配内存。-Xss128k：设置每个线程的栈大小。JDK5.0以后每个线程栈大小为1M，之前每个线程栈大小为256K。应当根据应用的线程所需内存大小进行调整。在相同物理内存下，减小这个值能生成更多的线程。但是操作系统对一个进程内的线程数还是有限制的，不能无限生成，经验值在3000~5000左右。需要注意的是：当这个值被设置的较大（例如&gt;2MB）时将会在很大程度上降低系统的性能。-Xmn2g：设置年轻代大小为2G。在整个堆内存大小确定的情况下，增大年轻代将会减小年老代，反之亦然。此值关系到JVM垃圾回收，对系统性能影响较大，官方推荐配置为整个堆大小的3/8。-XX:NewSize=1024m：设置年轻代初始值为1024M。-XX:MaxNewSize=1024m：设置年轻代最大值为1024M。-XX:PermSize=256m：设置持久代初始值为256M。-XX:MaxPermSize=256m：设置持久代最大值为256M。-XX:NewRatio=4：设置年轻代（包括1个Eden和2个Survivor区）与年老代的比值。表示年轻代比年老代为1:4。-XX:SurvivorRatio=4：设置年轻代中Eden区与Survivor区的比值。表示2个Survivor区（JVM堆内存年轻代中默认有2个大小相等的Survivor区）与1个Eden区的比值为2:4，即1个Survivor区占整个年轻代大小的1/6。-XX:MaxTenuringThreshold=7：表示一个对象如果在Survivor区（救助空间）移动了7次还没有被垃圾回收就进入年老代。如果设置为0的话，则年轻代对象不经过Survivor区，直接进入年老代，对于需要大量常驻内存的应用，这样做可以提高效率。如果将此值设置为一个较大值，则年轻代对象会在Survivor区进行多次复制，这样可以增加对象在年轻代存活时间，增加对象在年轻代被垃圾回收的概率，减少Full GC的频率，这样做可以在某种程度上提高服务稳定性。疑问解答-Xmn，-XX:NewSize/-XX:MaxNewSize，-XX:NewRatio 3组参数都可以影响年轻代的大小，混合使用的情况下，优先级是什么？如下：高优先级：-XX:NewSize/-XX:MaxNewSize中优先级：-Xmn（默认等效 -Xmn=-XX:NewSize=-XX:MaxNewSize=?）低优先级：-XX:NewRatio推荐使用-Xmn参数，原因是这个参数简洁，相当于一次设定 NewSize/MaxNewSIze，而且两者相等，适用于生产环境。-Xmn 配合 -Xms/-Xmx，即可将堆内存布局完成。-Xmn参数是在JDK 1.4 开始支持。二. 你知道哪几种垃圾收集器，各自的优缺点，重点讲下cms和G1，包括原理，流程，优缺点。CMS收集器CMS收集器是一种以获取最短回收停顿时间为目标的收集器。基于“标记-清除”算法实现，它的运作过程如下：1）初始标记2）并发标记3）重新标记4）并发清除初始标记、从新标记这两个步骤仍然需要“stop the world”，初始标记仅仅只是标记一下GC Roots能直接关联到的对象，熟读很快，并发标记阶段就是进行GC Roots Tracing，而重新标记阶段则是为了修正并发标记期间因用户程序继续运作而导致标记产生表动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段稍长点，但远比并发标记的时间短。CMS是一款优秀的收集器，主要优点：并发收集、低停顿。缺点：1）CMS收集器对CPU资源非常敏感。在并发阶段，它虽然不会导致用户线程停顿，但是会因为占用了一部分线程而导致应用程序变慢，总吞吐量会降低。2）CMS收集器无法处理浮动垃圾，可能会出现“Concurrent Mode Failure（并发模式故障）”失败而导致Full GC产生。浮动垃圾：由于CMS并发清理阶段用户线程还在运行着，伴随着程序运行自然就会有新的垃圾不断产生，这部分垃圾出现的标记过程之后，CMS无法在当次收集中处理掉它们，只好留待下一次GC中再清理。这些垃圾就是“浮动垃圾”。3）CMS是一款“标记–清除”算法实现的收集器，容易出现大量空间碎片。当空间碎片过多，将会给大对象分配带来很大的麻烦，往往会出现老年代还有很大空间剩余，但是无法找到足够大的连续空间来分配当前对象，不得不提前触发一次Full GC。G1收集器G1是一款面向服务端应用的垃圾收集器。1、并行于并发：G1能充分利用CPU、多核环境下的硬件优势，使用多个CPU（CPU或者CPU核心）来缩短stop-The-World停顿时间。部分其他收集器原本需要停顿Java线程执行的GC动作，G1收集器仍然可以通过并发的方式让java程序继续执行。2、分代收集：虽然G1可以不需要其他收集器配合就能独立管理整个GC堆，但是还是保留了分代的概念。它能够采用不同的方式去处理新创建的对象和已经存活了一段时间，熬过多次GC的旧对象以获取更好的收集效果。3、空间整合：与CMS的“标记–清理”算法不同，G1从整体来看是基于“标记整理”算法实现的收集器；从局部上来看是基于“复制”算法实现的。4、可预测的停顿：这是G1相对于CMS的另一个大优势，降低停顿时间是G1和ＣＭＳ共同的关注点，但Ｇ１除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为M毫秒的时间片段内，5、G1运作步骤：1、初始标记； 2、并发标记； 3、最终标记； 4、筛选回收； 上面几个步骤的运作过程和CMS有很多相似之处。初始标记阶段仅仅只是标记一下GC Roots能直接关联到的对象，并且修改TAMS的值，让下一个阶段用户程序并发运行时，能在正确可用的Region中创建新对象，这一阶段需要停顿线程，但是耗时很短，并发标记阶段是从GC Root开始对堆中对象进行可达性分析，找出存活的对象，这阶段时耗时较长，但可与用户程序并发执行。而最终标记阶段则是为了修正在并发标记期间因用户程序继续运作而导致标记产生变动的那一部分标记记录，虚拟机将这段时间对象变化记录在线程Remenbered Set Logs里面，最终标记阶段需要把Remembered Set Logs的数据合并到Remembered Set Logs里面，最终标记阶段需要把Remembered Set Logs的数据合并到Remembered Set中，这一阶段需要停顿线程，但是可并行执行。最后在筛选回收阶段首先对各个Region的回收价值和成本进行排序，根据用户所期望的GC停顿时间来制定回收计划。三. GC策略是如何的，有哪些New Generation的GC策略Serial GC。采用单线程方式，用Copying算法。到这里我们再来说说为什么New Generation会再次被划分成Eden Space和S0、S1，相信聪明的你一定已经想到Copying算法所需要的额外内存空间了吧，S0和S1又称为From Space和To Space。具体细节自己好好想想。Parallel Scavenge。将内存空间分段来使用多线程，也是用Copying算法。ParNew。比Parallel Scavenge多做了与Old Generation使用CMS GC一起发生时的特殊处理。Old Generation的GC策略Serial GC。当然也是单线程方式，但是实现是将Mark-Sweep和Mark-Compact结合了下，做了点改进。Parallel Mark-Sweep、Parallel Mark-Compact。同样也是把Old Generation空间进行划分成regions，只是粒度更细了。为什么用这两个算法，不用我赘述了吧。CMS（Concurrent Mark-Sweep） GC。我承认这个GC我真的没怎么看懂，目的是为了实现并发，结果就造成具体实现太麻烦了。有兴趣的朋友去看书吧，文末我说了是哪本书。这里有个地方可以说一下，就是算法使用的还是Mark-Sweep，对于内存碎片的问题，CMS提供了一个内存碎片的整理功能，会在执行几次Full GC以后执行一次。4. JVM的垃圾收集器JVM给出了3种选择：串行收集器、并行收集器、并发收集器。串行收集器只适用于小数据量的情况，所以生产环境的选择主要是并行收集器和并发收集器。默认情况下JDK5.0以前都是使用串行收集器，如果想使用其他收集器需要在启动时加入相应参数。JDK5.0以后，JVM会根据当前系统配置进行智能判断。串行收集器-XX:+UseSerialGC：设置串行收集器。并行收集器（吞吐量优先）-XX:+UseParallelGC：设置为并行收集器。此配置仅对年轻代有效。即年轻代使用并行收集，而年老代仍使用串行收集。-XX:ParallelGCThreads=20：配置并行收集器的线程数，即：同时有多少个线程一起进行垃圾回收。此值建议配置与CPU数目相等。-XX:+UseParallelOldGC：配置年老代垃圾收集方式为并行收集。JDK6.0开始支持对年老代并行收集。-XX:MaxGCPauseMillis=100：设置每次年轻代垃圾回收的最长时间（单位毫秒）。如果无法满足此时间，JVM会自动调整年轻代大小，以满足此时间。-XX:+UseAdaptiveSizePolicy：设置此选项后，并行收集器会自动调整年轻代Eden区大小和Survivor区大小的比例，以达成目标系统规定的最低响应时间或者收集频率等指标。此参数建议在使用并行收集器时，一直打开。并发收集器（响应时间优先）XX:+UseConcMarkSweepGC：即CMS收集，设置年老代为并发收集。CMS收集是JDK1.4后期版本开始引入的新GC算法。它的主要适合场景是对响应时间的重要性需求大于对吞吐量的需求，能够承受垃圾回收线程和应用线程共享CPU资源，并且应用中存在比较多的长生命周期对象。CMS收集的目标是尽量减少应用的暂停时间，减少Full GC发生的几率，利用和应用程序线程并发的垃圾回收线程来标记清除年老代内存。-XX:+UseParNewGC：设置年轻代为并发收集。可与CMS收集同时使用。JDK5.0以上，JVM会根据系统配置自行设置，所以无需再设置此参数。-XX:CMSFullGCsBeforeCompaction=0：由于并发收集器不对内存空间进行压缩和整理，所以运行一段时间并行收集以后会产生内存碎片，内存使用效率降低。此参数设置运行0次Full GC后对内存空间进行压缩和整理，即每次Full GC后立刻开始压缩和整理内存。-XX:+UseCMSCompactAtFullCollection：打开内存空间的压缩和整理，在Full GC后执行。可能会影响性能，但可以消除内存碎片。-XX:+CMSIncrementalMode：设置为增量收集模式。一般适用于单CPU情况。-XX:CMSInitiatingOccupancyFraction=70：表示年老代内存空间使用到70%时就开始执行CMS收集，以确保年老代有足够的空间接纳来自年轻代的对象，避免Full GC的发生。其它垃圾回收参数-XX:+ScavengeBeforeFullGC：年轻代GC优于Full GC执行。-XX:-DisableExplicitGC：不响应 System.gc() 代码。-XX:+UseThreadPriorities：启用本地线程优先级API。即使 java.lang.Thread.setPriority() 生效，不启用则无效。-XX:SoftRefLRUPolicyMSPerMB=0：软引用对象在最后一次被访问后能存活0毫秒（JVM默认为1000毫秒）。-XX:TargetSurvivorRatio=90：允许90%的Survivor区被占用（JVM默认为50%）。提高对于Survivor区的使用率。辅助信息参数设置-XX:-CITime：打印消耗在JIT编译的时间。-XX:ErrorFile=./hs_err_pid.log：保存错误日志或数据到指定文件中。-XX:HeapDumpPath=./java_pid.hprof：指定Dump堆内存时的路径。-XX:-HeapDumpOnOutOfMemoryError：当首次遭遇内存溢出时Dump出此时的堆内存。-XX:OnError=”;”：出现致命ERROR后运行自定义命令。-XX:OnOutOfMemoryError=”;”：当首次遭遇内存溢出时执行自定义命令。-XX:-PrintClassHistogram：按下 Ctrl+Break 后打印堆内存中类实例的柱状信息，同JDK的 jmap -histo 命令。-XX:-PrintConcurrentLocks：按下 Ctrl+Break 后打印线程栈中并发锁的相关信息，同JDK的 jstack -l 命令。-XX:-PrintCompilation：当一个方法被编译时打印相关信息。-XX:-PrintGC：每次GC时打印相关信息。-XX:-PrintGCDetails：每次GC时打印详细信息。-XX:-PrintGCTimeStamps：打印每次GC的时间戳。-XX:-TraceClassLoading：跟踪类的加载信息。-XX:-TraceClassLoadingPreorder：跟踪被引用到的所有类的加载信息。-XX:-TraceClassResolution：跟踪常量池。-XX:-TraceClassUnloading：跟踪类的卸载信息。调优实战1. 大型网站服务器案例承受海量访问的动态Web应用服务器配置：8 CPU, 8G MEM, JDK 1.6.X参数方案：-server -Xmx3550m -Xms3550m -Xmn1256m -Xss128k -XX:SurvivorRatio=6 -XX:MaxPermSize=256m -XX:ParallelGCThreads=8 -XX:MaxTenuringThreshold=0 -XX:+UseConcMarkSweepGC调优说明：-Xmx 与 -Xms 相同以避免JVM反复重新申请内存。-Xmx 的大小约等于系统内存大小的一半，即充分利用系统资源，又给予系统安全运行的空间。-Xmn1256m 设置年轻代大小为1256MB。此值对系统性能影响较大，Sun官方推荐配置年轻代大小为整个堆的3/8。-Xss128k 设置较小的线程栈以支持创建更多的线程，支持海量访问，并提升系统性能。-XX:SurvivorRatio=6 设置年轻代中Eden区与Survivor区的比值。系统默认是8，根据经验设置为6，则2个Survivor区与1个Eden区的比值为2:6，一个Survivor区占整个年轻代的1/8。-XX:ParallelGCThreads=8 配置并行收集器的线程数，即同时8个线程一起进行垃圾回收。此值一般配置为与CPU数目相等。-XX:MaxTenuringThreshold=0 设置垃圾最大年龄（在年轻代的存活次数）。如果设置为0的话，则年轻代对象不经过Survivor区直接进入年老代。对于年老代比较多的应用，可以提高效率；如果将此值设置为一个较大值，则年轻代对象会在Survivor区进行多次复制，这样可以增加对象再年轻代的存活时间，增加在年轻代即被回收的概率。根据被海量访问的动态Web应用之特点，其内存要么被缓存起来以减少直接访问DB，要么被快速回收以支持高并发海量请求，因此其内存对象在年轻代存活多次意义不大，可以直接进入年老代，根据实际应用效果，在这里设置此值为0。-XX:+UseConcMarkSweepGC 设置年老代为并发收集。CMS（ConcMarkSweepGC）收集的目标是尽量减少应用的暂停时间，减少Full GC发生的几率，利用和应用程序线程并发的垃圾回收线程来标记清除年老代内存，适用于应用中存在比较多的长生命周期对象的情况。内部集成构建服务器案例高性能数据处理的工具应用服务器配置：1 CPU, 4G MEM, JDK 1.6.X参数方案：-server -XX:PermSize=196m -XX:MaxPermSize=196m -Xmn320m -Xms768m -Xmx1024m调优说明：-XX:PermSize=196m -XX:MaxPermSize=196m 根据集成构建的特点，大规模的系统编译可能需要加载大量的Java类到内存中，所以预先分配好大量的持久代内存是高效和必要的。-Xmn320m 遵循年轻代大小为整个堆的3/8原则。-Xms768m -Xmx1024m 根据系统大致能够承受的堆内存大小设置即可。参考博文jvm参数设置大全浅谈CMS垃圾收集器与G1收集器JVM的GC策略java jvm内存管理/gc策略/参数设置","categories":[{"name":"面试","slug":"面试","permalink":"http://www.fufan.me/categories/面试/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://www.fufan.me/tags/jvm/"},{"name":"面试","slug":"面试","permalink":"http://www.fufan.me/tags/面试/"}]},{"title":"Java面试总结积累（基础篇）之JVM问题(一)","slug":"Java面试总结积累（基础篇）之JVM问题-一","date":"2017-11-06T11:36:00.000Z","updated":"2018-11-06T11:41:23.861Z","comments":true,"path":"2017/11/06/Java面试总结积累（基础篇）之JVM问题-一/","link":"","permalink":"http://www.fufan.me/2017/11/06/Java面试总结积累（基础篇）之JVM问题-一/","excerpt":"","text":"一. 什么情况下会发生栈内存溢出。栈溢出(StackOverflowError)原因：12栈是每个线程私有的，他的生命周期与线程相同，每个方法在执行的时候都会创建一个栈帧，用来存储局部变量表，操作数栈，动态链接，方法出口灯信息。局部变量表又包含基本数据类型，对象引用类型（局部变量表编译器完成，运行期间不会变化）所以我们可以理解为栈溢出就是方法执行是创建的栈帧超过了栈的深度。解决方法1我们需要使用参数 -Xss 去调整JVM栈的大小二. 什么情况下会发生堆内存溢出。堆溢出(OutOfMemoryError:java heap space)原因1heap space表示堆空间，堆中主要存储的是对象。如果不断的new对象则会导致堆中的空间溢出解决1可以通过 -Xmx4096M 调整堆的总大小三. JVM的内存结构，Eden和Survivor比例。JVM的内存结构jvm将管理的内存中分几块，方法区、堆、虚拟机栈、本地方法栈、程序计数器、运行时常量池线程私有的块有：虚拟机栈、本地方法栈、程序计数器线程共享的块有：方法区、堆、运行时常量池方法区：它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据非堆数据堆：heap是java虚拟机所管理内存中最大的一块，我们创建的对象实例都是存储在这里，它也是垃圾回收的主要区域。现在垃圾回收的算法基本用的是分代收集，虚拟机将其分为新生代和老年代，新生代分为eden和survivor，survivor还可以分为from和to，我们可以通过设置jvm参数的方式来对其进行比例分配。我们也可以通过-Xmx和-Xms控制堆的最大和初始化值，一般将其设置为相同的值，避免其重新进行分配，提高性能。如果在堆中没有内存完成实例分配，并且堆也无法再扩展时，将会抛出OutOfMemoryError 异常。分配和回收内容请看jvm系列中的blog。运行时常量池：运行时常量池（Runtime Constant Pool）是方法区的一部分。Class 文件中除了有类的版本、字段、方法、接口等描述等信息外，还有一项信息是常量池（Constant PoolTable），用于存放编译期生成的各种字面量和符号引用，这部分内容将在类加载后存放到方法区的运行时常量池中。虚拟机栈：每个方法被执行的时候都会同时创建一个栈帧用于存储局部变量表、操作栈、动态链接、方法出口等信息。每一个方法被调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。线程可以通过访问对象的引用找到访问堆中的对象。有两种情况会抛出异常，一是单个线程的栈深度大于虚拟机所允许的深度，将抛出StackOverflowError 异常；如果虚拟机栈可以动态扩展，当扩展时无法申请到足够的内存时会抛出OutOfMemoryError 异常。本地方法栈：本地方法栈（Native Method Stacks）与虚拟机栈所发挥的作用是非常相似的，其区别不过是虚拟机栈为虚拟机执行Java 方法（也就是字节码）服务，而本地方法栈则是为虚拟机使用到的Native 方法服务。其他同虚拟机栈原理类似。程序计数器：程序计数器（Program Counter Register）是一块较小的内存空间，它的作用可以看做是当前线程所执行的字节码的行号指示器。在虚拟机的概念模型里（仅是概念模型，各种虚拟机可能会通过一些更高效的方式去实现），字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。由于Java 虚拟机的多线程是通过线程轮流切换并分配处理器执行时间的方式来实现的，在任何一个确定的时刻，一个处理器（对于多核处理器来说是一个内核）只会执行一条线程中的指令。因此，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各条线程之间的计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。Eden和Survivor比例-XX:NewSize和-XX:MaxNewSize：用于设置年轻代的大小，建议设为整个堆大小的1/3或者1/4,两个值设为一样大。-XX:SurvivorRatio：用于设置Eden和其中一个Survivor的比值，这个值也比较重要。XX:+PrintTenuringDistribution：这个参数用于显示每次Minor GC时Survivor区中各个年龄段的对象的大小。.-XX:InitialTenuringThreshol和-XX:MaxTenuringThresholdJVM内存为什么要分成新生代，老年代，持久代。新生代中为什么要分为Eden和Survivor。为什么需要把堆分代？不分代不能完成他所做的事情么？其实不分代完全可以，分代的唯一理由就是优化GC性能。你先想想，如果没有分代，那我们所有的对象都在一块，GC的时候我们要找到哪些对象没用，这样就会对堆的所有区域进行扫描。而我们的很多对象都是朝生夕死的，如果分代的话，我们把新创建的对象放到某一地方，当GC的时候先把这块存“朝生夕死”对象的区域进行回收，这样就会腾出很大的空间出来。新生代：大多数新生的对象在Eden区分配，当Eden区没有足够空间进行分配时，虚拟机就会进行一次MinorGC。在方法中new一个对象，方法调用完毕，对象就无用，这就是典型的新生代对象。（新生对象在Eden区经历过一次MinorGC并且被Survivor容纳的话，对象年龄为1，并且每熬过一次MinorGC，年龄就会加1，直到15，就会晋升到老年代）注意动态对象的判定：Survivor空间中相同年龄的对象大小总和大于Survivor空间的一半，大于或者等于该年龄的对象就可以直接进入老年代。老年代：在新生代中经历了N次垃圾回收后仍然存活的对象，就会被放到老年代中，而且大对象（占用大量连续内存空间的java对象如很长的字符串及数组）直接进入老年代。当survivor空间不够用时，需要依赖老年代进行分配担保。永久代方法区主要存放Class和Meta的信息，Class在被加载的时候被放入永久代。 它和存放对象的堆区域不同，GC(Garbage Collection)不会在主程序运行期对永久代进行清理，所以如果你的应用程序会加载很多Class的话,就很可能出现PermGen space错误。GC分类MinorGC：是指清理新生代MajorGC：是指清理老年代（很多MajorGC是由MinorGC触发的）FullGC：是指清理整个堆空间包括年轻代和永久代参考博文jvm参数设置大全浅谈CMS垃圾收集器与G1收集器JVM的GC策略","categories":[{"name":"面试","slug":"面试","permalink":"http://www.fufan.me/categories/面试/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://www.fufan.me/tags/jvm/"},{"name":"面试","slug":"面试","permalink":"http://www.fufan.me/tags/面试/"}]},{"title":"JVM专题之jvm知识点总览","slug":"JVM专题之jvm知识点总览","date":"2017-10-31T02:20:00.000Z","updated":"2018-11-07T02:20:37.232Z","comments":true,"path":"2017/10/31/JVM专题之jvm知识点总览/","link":"","permalink":"http://www.fufan.me/2017/10/31/JVM专题之jvm知识点总览/","excerpt":"","text":"jvm体系总体分四大块：类的加载机制jvm内存结构GC算法 垃圾回收GC分析 命令调优类加载机制主要关注点：什么是类的加载类的生命周期类加载器双亲委派模型什么是类的加载类的加载指的是将类的.class文件中的二进制数据读入到内存中，将其放在运行时数据区的方法区内，然后在堆区创建一个java.lang.Class对象，用来封装类在方法区内的数据结构。类的加载的最终产品是位于堆区中的Class对象，Class对象封装了类在方法区内的数据结构，并且向Java程序员提供了访问方法区内的数据结构的接口。类的生命周期类的生命周期包括这几个部分，加载、连接、初始化、使用和卸载，其中前三部是类的加载的过程,如下图；加载，查找并加载类的二进制数据，在Java堆中也创建一个java.lang.Class类的对象连接，连接又包含三块内容：验证、准备、初始化。1）验证，文件格式、元数据、字节码、符号引用验证；2）准备，为类的静态变量分配内存，并将其初始化为默认值；3）解析，把类中的符号引用转换为直接引用初始化，为类的静态变量赋予正确的初始值使用，new出对象程序中使用卸载，执行垃圾回收类加载器启动类加载器：Bootstrap ClassLoader，负责加载存放在JDK\\jre\\lib(JDK代表JDK的安装目录，下同)下，或被-Xbootclasspath参数指定的路径中的，并且能被虚拟机识别的类库扩展类加载器：Extension ClassLoader，该加载器由sun.misc.Launcher$ExtClassLoader实现，它负责加载DK\\jre\\lib\\ext目录中，或者由java.ext.dirs系统变量指定的路径中的所有类库（如javax.*开头的类），开发者可以直接使用扩展类加载器。应用程序类加载器：Application ClassLoader，该类加载器由sun.misc.Launcher$AppClassLoader来实现，它负责加载用户类路径（ClassPath）所指定的类，开发者可以直接使用该类加载器类加载机制全盘负责，当一个类加载器负责加载某个Class时，该Class所依赖的和引用的其他Class也将由该类加载器负责载入，除非显示使用另外一个类加载器来载入父类委托，先让父类加载器试图加载该类，只有在父类加载器无法加载该类时才尝试从自己的类路径中加载该类缓存机制，缓存机制将会保证所有加载过的Class都会被缓存，当程序中需要使用某个Class时，类加载器先从缓存区寻找该Class，只有缓存区不存在，系统才会读取该类对应的二进制数据，并将其转换成Class对象，存入缓存区。这就是为什么修改了Class后，必须重启JVM，程序的修改才会生效jvm内存结构主要关注点：jvm内存结构都是什么对象分配规则jvm内存结构方法区和堆是所有线程共享的内存区域；而java栈、本地方法栈和程序计数器是运行是线程私有的内存区域。Java堆（Heap）,是Java虚拟机所管理的内存中最大的一块。Java堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配内存。方法区（Method Area）,方法区（Method Area）与Java堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。程序计数器（Program Counter Register）,程序计数器（Program Counter Register）是一块较小的内存空间，它的作用可以看做是当前线程所执行的字节码的行号指示器。JVM栈（JVM Stacks）,与程序计数器一样，Java虚拟机栈（Java Virtual Machine Stacks）也是线程私有的，它的生命周期与线程相同。虚拟机栈描述的是Java方法执行的内存模型：每个方法被执行的时候都会同时创建一个栈帧（Stack Frame）用于存储局部变量表、操作栈、动态链接、方法出口等信息。每一个方法被调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。本地方法栈（Native Method Stacks）,本地方法栈（Native Method Stacks）与虚拟机栈所发挥的作用是非常相似的，其区别不过是虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则是为虚拟机使用到的Native方法服务。对象分配规则对象优先分配在Eden区，如果Eden区没有足够的空间时，虚拟机执行一次Minor GC。大对象直接进入老年代（大对象是指需要大量连续内存空间的对象）。这样做的目的是避免在Eden区和两个Survivor区之间发生大量的内存拷贝（新生代采用复制算法收集内存）。长期存活的对象进入老年代。虚拟机为每个对象定义了一个年龄计数器，如果对象经过了1次Minor GC那么对象会进入Survivor区，之后每经过一次Minor GC那么对象的年龄加1，知道达到阀值对象进入老年区。动态判断对象的年龄。如果Survivor区中相同年龄的所有对象大小的总和大于Survivor空间的一半，年龄大于或等于该年龄的对象可以直接进入老年代。空间分配担保。每次进行Minor GC时，JVM会计算Survivor区移至老年区的对象的平均大小，如果这个值大于老年区的剩余值大小则进行一次Full GC，如果小于检查HandlePromotionFailure设置，如果true则只进行Monitor GC,如果false则进行Full GC。GC算法 垃圾回收主要关注点：对象存活判断GC算法垃圾回收器对象存活判断判断对象是否存活一般有两种方式：引用计数：每个对象有一个引用计数属性，新增一个引用时计数加1，引用释放时计数减1，计数为0时可以回收。此方法简单，无法解决对象相互循环引用的问题。可达性分析（Reachability Analysis）：从GC Roots开始向下搜索，搜索所走过的路径称为引用链。当一个对象到GC Roots没有任何引用链相连时，则证明此对象是不可用的，不可达对象。GC算法GC最基础的算法有三种：标记 -清除算法、复制算法、标记-压缩算法，我们常用的垃圾回收器一般都采用分代收集算法。标记 -清除算法，“标记-清除”（Mark-Sweep）算法，如它的名字一样，算法分为“标记”和“清除”两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收掉所有被标记的对象。复制算法，“复制”（Copying）的收集算法，它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。标记-压缩算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存分代收集算法，“分代收集”（Generational Collection）算法，把Java堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。垃圾回收器Serial收集器，串行收集器是最古老，最稳定以及效率高的收集器，可能会产生较长的停顿，只使用一个线程去回收。ParNew收集器，ParNew收集器其实就是Serial收集器的多线程版本。Parallel收集器，Parallel Scavenge收集器类似ParNew收集器，Parallel收集器更关注系统的吞吐量。Parallel Old 收集器，Parallel Old是Parallel Scavenge收集器的老年代版本，使用多线程和“标记－整理”算法CMS收集器，CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。G1收集器，G1 (Garbage-First)是一款面向服务器的垃圾收集器,主要针对配备多颗处理器及大容量内存的机器. 以极高概率满足GC停顿时间要求的同时,还具备高吞吐量性能特征GC分析 命令调优主要关注点：GC日志分析调优命令调优工具GC日志分析Young GC日志:Full GC日志:调优命令Sun JDK监控和故障处理命令有jps jstat jmap jhat jstack jinfojps，JVM Process Status Tool,显示指定系统内所有的HotSpot虚拟机进程。jstat，JVM statistics Monitoring是用于监视虚拟机运行时状态信息的命令，它可以显示出虚拟机进程中的类装载、内存、垃圾收集、JIT编译等运行数据。jmap，JVM Memory Map命令用于生成heap dump文件jhat，JVM Heap Analysis Tool命令是与jmap搭配使用，用来分析jmap生成的dump，jhat内置了一个微型的HTTP/HTML服务器，生成dump的分析结果后，可以在浏览器中查看jstack，用于生成java虚拟机当前时刻的线程快照。jinfo，JVM Configuration info 这个命令作用是实时查看和调整虚拟机运行参数。调优工具常用调优工具分为两类,jdk自带监控工具：jconsole和jvisualvm，第三方有：MAT(Memory Analyzer Tool)、GChisto。jconsole，Java Monitoring and Management Console是从java5开始，在JDK中自带的java监控和管理控制台，用于对JVM中内存，线程和类等的监控jvisualvm，jdk自带全能工具，可以分析内存快照、线程快照；监控内存变化、GC变化等。MAT，Memory Analyzer Tool，一个基于Eclipse的内存分析工具，是一个快速、功能丰富的Java heap分析工具，它可以帮助我们查找内存泄漏和减少内存消耗GChisto，一款专业分析gc日志的工具jprofile","categories":[{"name":"java虚拟机","slug":"java虚拟机","permalink":"http://www.fufan.me/categories/java虚拟机/"}],"tags":[{"name":"java虚拟机","slug":"java虚拟机","permalink":"http://www.fufan.me/tags/java虚拟机/"}]},{"title":"JVM专题（绪论）","slug":"JVM系列（绪论）","date":"2017-10-26T02:17:00.000Z","updated":"2018-11-07T02:30:53.357Z","comments":true,"path":"2017/10/26/JVM系列（绪论）/","link":"","permalink":"http://www.fufan.me/2017/10/26/JVM系列（绪论）/","excerpt":"","text":"作为一个合格的java程序员，对于jvm的熟悉使用是非常有必要的，这个系列将会介绍我们常用jvm的一些原理和常用命令，以及常见的问题的排查及处理。由于最近工作需要，顺便整理和学习一下jvm的相关知识，自己工作中遇到的问题，同时借鉴网上一些资料和博客，理一下这部分知识。博文目录JVM专题之jvm知识点总览JVM专题（一）——类加载机制JVM专题（二）——JVM内存模型JVM专题（三）——GC算法 垃圾收集器JVM专题（四）——jvm调优-命令篇JVM专题（五）——Java GC 分析JVM专题（六）——Java服务GC参数调优案例JVM专题（七）——jvm调优-工具篇博文参考jvm知识点总览感谢博客作者的分享，受益匪浅，已吸收。","categories":[{"name":"java虚拟机","slug":"java虚拟机","permalink":"http://www.fufan.me/categories/java虚拟机/"}],"tags":[{"name":"java虚拟机","slug":"java虚拟机","permalink":"http://www.fufan.me/tags/java虚拟机/"}]},{"title":"常用的vim命令整理（持续更新）","slug":"常用的vim命令整理（持续更新）","date":"2017-10-02T12:58:00.000Z","updated":"2018-11-04T14:46:09.403Z","comments":true,"path":"2017/10/02/常用的vim命令整理（持续更新）/","link":"","permalink":"http://www.fufan.me/2017/10/02/常用的vim命令整理（持续更新）/","excerpt":"","text":"1. 插入模式 (command mode)命令模式切换至插入状态i光标前插入I行首插入a光标后插入A行尾插入o行上新行O行下新行2. 命令模式 (insert mode)ESC从插入状态切换至命令模式光标移动h左移j下移k上移l右移H到屏幕顶部M到屏幕中央L到屏幕底部0到行首$到行尾Ctrl+f向前翻屏Ctrl+b向后翻屏Ctrl+d向前翻半屏Ctrl+u向后翻半屏定位gg回到文件首行,G回到文件尾行:n和nG光标定位到文件第n行(:20或20G表示光标定位到第20行):set nu 或:set number显示行号,:set nonu 取消显示行号ctrl+g删除x删除光标所在字符(与Delete键相同的方向),X删除光标所在字符(与Backspace键相同的方向)nx删除光标后n个字符dd删除光标所在行ndd删除光标所在行以后的n行D删除光标到行尾的内容dG删除光标所在行到文件末尾的内容n1,n2d删除行n1到行n2的内容，包括第n1和n2行都被删除s删除一个字符来插入模式S删除当前行以插入模式复制、剪切、粘贴、替换yy或Y复制当前行nyy或nY从当前行开始赋值n行ggVG全选剪切使用dd和ndd，相当于删除p在光标所在行之后粘贴P在光标所在行之前粘贴r替换当前字符后回到命令模式R一直替换知道通过ESC回到命令模式查找、替换\\KeyWord回车，n查找下一处?KeyWord回车，n查找上一处n重复相同方向N重复反向方向·:s/old/new/g替换整个文件，不确认:s/old/new/gc替换整个文件，确认:n1,n2s/old/new/g替换n1-n2行中匹配内容，不确认撤销u保存及离开:w保存文件:w!强制保存:w file将修改另外保存到file:wq保存文件并退出:wq!强制保存文件并退出:q不保存退出:q!不保存强制退出:e!放弃所有修改，从上次保存文件开始再编辑","categories":[],"tags":[]},{"title":"ssh免密码登录步骤及别名设置","slug":"ssh免密码登录步骤及别名设置","date":"2017-10-02T05:23:00.000Z","updated":"2018-11-04T14:45:55.388Z","comments":true,"path":"2017/10/02/ssh免密码登录步骤及别名设置/","link":"","permalink":"http://www.fufan.me/2017/10/02/ssh免密码登录步骤及别名设置/","excerpt":"","text":"ssh免密码登录步骤及别名设置1. 生成本机的公私钥ssh-keygen -t rsa2. 将公钥复制到目标机器上ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.0.1003. 设置别名登录vim ~/.ssh/config添加如下内容123456Host 100HostName 192.168.0.100Port 22User rootIdentityFile ~/.ssh/id_rsa.pubIdentitiesOnly yes4. 登录ssh 100","categories":[{"name":"linux","slug":"linux","permalink":"http://www.fufan.me/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://www.fufan.me/tags/linux/"}]},{"title":"Java8语法特性讲解（二）—— Stream API","slug":"Untitled-1","date":"2017-07-15T13:21:00.000Z","updated":"2018-11-11T13:22:11.639Z","comments":true,"path":"2017/07/15/Untitled-1/","link":"","permalink":"http://www.fufan.me/2017/07/15/Untitled-1/","excerpt":"","text":"函数式编程是Java8最值得学习的新特性。为什么需要 StreamStream作为Java 8的一大亮点，它与java.io包里的InputStream和OutputStream完全不同的概念。Java 8中Stream是对集合对象功能的增强，它专注于对集合对象进行各种非常便利、高效的聚合操作，或者大批量数据操作。Stream API借助于同样新出现的Lambda表达式，极大的提高编程效率和程序可读性。同时它提供串行和并行两种模式进行汇聚操作，并发模式能够充分利用多核处理器的优势，使用fork/join并行方式来拆分任务和加速处理过程。Stream的流程创建Stream使用Collection子类获取Stream使用Stream静态方法of来创建Stream可以用静态的Stream.of方法转换成一个Stream。of方法：有两个重载方法，一个接受变长参数，一个接受单一值。12Stream&lt;Integer&gt; integerStream = Stream.of(1, 2, 3, 5);Stream&lt;String&gt; stringStream = Stream.of(&quot;LiangGzone&quot;);使用Stream静态方法generate来创建Stream123456Stream.generate(new Supplier&lt;Double&gt;() &#123; @Override public Double get() &#123; return Math.random(); &#125;&#125;);lambda表达式改写，如下1Stream.generate(Math::random);使用Stream静态方法iterate来创建Stream这段代码就是先获取一个无限长度的正整数集合的Stream，然后取出前10个打印。1Stream.iterate(1, i -&gt; i + 1).limit(10).forEach(System.out::println);流转换流转换是指从一个流中读取数据，并将转换后的数据写入到另外一个流中。流转换操作都是懒加载，多个转换操作只会在汇聚操作的时候融合起来，一次循环完成。distinct对于Stream中包含的元素进行去重操作（去重逻辑依赖元素的equals方法），新生成的Stream中没有重复的元素。1list.stream().distinct();filter对于Stream中包含的元素使用给定的过滤函数进行过滤操作，新生成的Stream只包含符合条件的元素。1list.stream().filter(w -&gt; w.length()&gt;=10);map对于Stream中包含的元素使用给定的转换函数进行转换操作，新生成的Stream只包含转换生成的元素。limit &amp; skipskip: 返回一个丢弃原Stream的前N个元素后剩下元素组成的新Stream，如果原Stream中包含的元素个数小于N，那么返回空Stream。limit: 对一个Stream进行截断操作，获取其前N个元素，如果原Stream中包含的元素个数小于N，那就获取其所有的元素。1Stream.iterate(1, i-&gt;i+1).skip(5).limit(5);peek生成一个包含原Stream的所有元素的新Stream，同时会提供一个消费函数（Consumer实例），新Stream每个元素被消费的时候都会执行给定的消费函数。123list.stream().peek(s -&gt; &#123; System.out.println(&quot;it is &quot; + s); &#125;).collect(Collectors.toList());notice: peek接收一个没有返回值的λ表达式，可以做一些输出，外部处理等。map接收一个有返回值的λ表达式，之后Stream的泛型类型将转换为map参数λ表达式返回的类型参数类型中的Function 比 Consumer 多了一个 return。sorted对一个Stream进行排序操作。1list.stream().sorted(Comparator.comparing(String::length).reversed());聚合操作reduce我们，通过reduce方法实现求和操作。1234public static int reduceBySum()&#123; List&lt;Integer&gt; list = Arrays.asList(1, 2, 3, 4, 5); return list.stream().reduce((sum, i) -&gt; sum + i).get();&#125;我们，还可以通过reduce实现累加器函数。123456public static int reduceBySum3()&#123; List&lt;String&gt; list = Arrays.asList(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;); return list.stream().reduce(0, (sum, word) -&gt; sum + word.length(), (sum1, sum2) -&gt; sum1 + sum2);&#125;collectcollect可以把Stream中的要有元素收集到一个结果容器中（比如Collection）123456789public static List&lt;Integer&gt; collect1()&#123; List&lt;Integer&gt; numList = Arrays.asList(1, 1, null, 2, 3, 4, null, 5, 6, 7, 8, 9, 10); List&lt;Integer&gt; numsWithoutNullList = numList.stream() .filter(num -&gt; num != null) .collect(() -&gt; new ArrayList&lt;Integer&gt;(), (list, item) -&gt; list.add(item), (list1, list2) -&gt; list1.addAll(list2)); return numsWithoutNullList;&#125;改造成方法引用，比较简洁，如下1234567public static List&lt;Integer&gt; collect2()&#123; List&lt;Integer&gt; numList = Arrays.asList(1, 1, null, 2, 3, 4, null, 5, 6, 7, 8, 9, 10); List&lt;Integer&gt; numsWithoutNullList = numList.stream() .filter(num -&gt; num != null) .collect(ArrayList::new, ArrayList::add, ArrayList::addAll); return numsWithoutNullList;&#125;我们可以通过Collectors提供的工具类。1234567public static List&lt;Integer&gt; collect3()&#123; List&lt;Integer&gt; numList = Arrays.asList(1, 1, null, 2, 3, 4, null, 5, 6, 7, 8, 9, 10); List&lt;Integer&gt; numsWithoutNullList = numList.stream() .filter(num -&gt; num != null) .collect(Collectors.toList()); return numsWithoutNullList;&#125;参考博文Java8 特性详解（二） Stream API","categories":[{"name":"java","slug":"java","permalink":"http://www.fufan.me/categories/java/"}],"tags":[{"name":"java8","slug":"java8","permalink":"http://www.fufan.me/tags/java8/"}]},{"title":"Java8语法特性讲解（一）—— Lambda","slug":"Java8语法特性讲解（一）——-Lambda","date":"2017-07-12T13:20:00.000Z","updated":"2018-11-11T13:22:33.050Z","comments":true,"path":"2017/07/12/Java8语法特性讲解（一）——-Lambda/","link":"","permalink":"http://www.fufan.me/2017/07/12/Java8语法特性讲解（一）——-Lambda/","excerpt":"","text":"函数式编程是Java8最值得学习的新特性。为什么要使用lambda表达式从函数式接口说起理解Functional Interface（函数式接口）是学习Java8 lambda表达式的关键所在。函数式接口的定义其实很简单：任何接口，如果只包含唯一一个抽象方法，那么它就是一个函数式接口。对于函数式接口，我们可以通过lambda表达式来创建该接口的对象。为了让编译器帮助我们确保一个接口满足函数式接口的要求，也就是说有且仅有一个抽象方法。Java8提供了@FunctionalInterface注解。举个简单的例子，Runnable接口就是一个FI，下面是它的源代码：1234@FunctionalInterfacepublic interface Runnable &#123; public abstract void run();&#125;使用@FunctionalInterface注解并不强制要求。但是使用注解会让代码看上去更清楚。lambda表达式的语法糖语法糖Java中lambda表达式的格式：参数、箭头-&gt;、一个表达式。为了演示lambda表达式的语法糖，我们通过Comparator作为例子。在Java8之前，可以借助匿名内部类来实现。12345678910public static List&lt;String&gt; compareTest1()&#123; List&lt;String&gt; wordList = Arrays.asList(&quot;lianggzone&quot;, &quot;spring&quot;, &quot;summer&quot;, &quot;autumn&quot;, &quot;winter&quot;); wordList.sort(new Comparator&lt;String&gt;() &#123; @Override public int compare(String w1, String w2) &#123; return Integer.compare(w1.length(), w2.length()); &#125; &#125;); return wordList; &#125;Comparator是个函数式接口，我们可以用Lambda表达式来实现。Lambda表达式，很像一个匿名的方法，只是小括号内的参数列表和花括号内的代码被-&gt;分隔开了。1234567public static List&lt;String&gt; compareTest2()&#123; List&lt;String&gt; wordList = Arrays.asList(&quot;lianggzone&quot;, &quot;spring&quot;, &quot;summer&quot;, &quot;autumn&quot;, &quot;winter&quot;); wordList.sort((String w1, String w2) -&gt; &#123; return Integer.compare(w1.length(), w2.length()); &#125;); return wordList; &#125;如果一个lambda表达式的参数类型是可以被推导的，那么就可以省略它们的类型。1234567public static List&lt;String&gt; compareTest3()&#123; List&lt;String&gt; wordList = Arrays.asList(&quot;lianggzone&quot;, &quot;spring&quot;, &quot;summer&quot;, &quot;autumn&quot;, &quot;winter&quot;); wordList.sort((w1, w2) -&gt; &#123; return Integer.compare(w1.length(), w2.length()); &#125;); return wordList; &#125;如果一个lambda表达式的代码块只是return后面跟一个表达式，那么还可以进一步简化。12345public static List&lt;String&gt; compareTest4()&#123; List&lt;String&gt; wordList = Arrays.asList(&quot;lianggzone&quot;, &quot;spring&quot;, &quot;summer&quot;, &quot;autumn&quot;, &quot;winter&quot;); wordList.sort((w1, w2) -&gt; Integer.compare(w1.length(), w2.length())); return wordList; &#125;如果某个方法只含有一个参数，并且该参数的类型可以被推导出来，你甚至可以省略小括号。12345public static List&lt;String&gt; compareTest5()&#123; List&lt;String&gt; wordList = Arrays.asList(&quot;lianggzone&quot;, &quot;spring&quot;, &quot;summer&quot;, &quot;autumn&quot;, &quot;winter&quot;); wordList.forEach(word -&gt; System.out.println(word)); return wordList; &#125;当我们要在另外一个独立线程中执行一些逻辑时，通常会将代码放在一个实现Runable接口的类的run方法中。12345678910public static void runnableTest1()&#123; Executors.newSingleThreadExecutor().execute(new Runnable() &#123; @Override public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; System.out.println(i); &#125; &#125; &#125;); &#125;如果lambda表达式没有参数，你仍可以提供一对空的小括号，如同不含参数的方法。1234567public static void runnableTest2()&#123; Executors.newSingleThreadExecutor().execute(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) &#123; System.out.println(i); &#125; &#125;); &#125;注意点lambda表达式执行返回类型，会根据上下文推到出来，我们不需要设置它的返回类型。lambda表达式中，只有某些分支中返回值，这样是错误的。方法引用有些时候，lambda表达式的代码就只是一个简单的方法调用而已，遇到这种情况，lambda表达式还可以进一步简化为方法引用。：：操作符将方法名和对象或类的名字分隔开来。对象：：实例对象在Java8之前，我们打印List内容，正常是这样做的。123456789public static void print1()&#123; List&lt;String&gt; wordList = Arrays.asList(&quot;lianggzone&quot;, &quot;spring&quot;, &quot;summer&quot;, &quot;autumn&quot;, &quot;winter&quot;); wordList.forEach(new Consumer&lt;String&gt;() &#123; @Override public void accept(String word) &#123; System.out.println(word); &#125; &#125;); &#125;因为Consumer是函数式接口，我们通过lambda表达式改造下。1234public static void print2()&#123; List&lt;String&gt; wordList = Arrays.asList(&quot;lianggzone&quot;, &quot;spring&quot;, &quot;summer&quot;, &quot;autumn&quot;, &quot;winter&quot;); wordList.forEach(word -&gt; System.out.println(word)); &#125;如果改造成方法引用，表达式 System.out::println, 等价于word -&gt; System.out.println(word)。1234public static void print3()&#123; List&lt;String&gt; wordList = Arrays.asList(&quot;lianggzone&quot;, &quot;spring&quot;, &quot;summer&quot;, &quot;autumn&quot;, &quot;winter&quot;); wordList.forEach(System.out::println); &#125;类：：实例对象例如，我们不区分大小写对字符串进行排序。12345public static String[] sort1()&#123; String[] words = new String[]&#123;&quot;lianggzone&quot;, &quot;spring&quot;, &quot;summer&quot;, &quot;autumn&quot;, &quot;winter&quot;&#125;; Arrays.sort(words, (x, y) -&gt; x.compareToIgnoreCase(y)); return words;&#125;如果改造成方法引用，表达式: String::compareToIgnoreCase,等价于(x, y) -&gt; x.compareToIgnoreCase(y),第一参数会成为执行方法的对象。12345public static String[] sort2()&#123; String[] words = new String[]&#123;&quot;lianggzone&quot;, &quot;spring&quot;, &quot;summer&quot;, &quot;autumn&quot;, &quot;winter&quot;&#125;; Arrays.sort(words, String::compareToIgnoreCase); return words;&#125;对象：：静态方法例如，我们对集合进行排序。12345public static List&lt;Integer&gt; sortList1()&#123; List&lt;Integer&gt; wordList = Arrays.asList(21, 53); wordList.sort((w1, w2) -&gt; Integer.compare(w1, w2)); return wordList;&#125;如果改造成方法引用，表达式: Integer::compare,等价于(w1, w2) -&gt; Integer.compare(w1, w2),第一参数会成为执行方法的对象。12345public static List&lt;Integer&gt; sortList2()&#123; List&lt;Integer&gt; wordList = Arrays.asList(21, 53); wordList.sort(Integer::compare); return wordList;&#125;构造器引用123456// lambdawords.stream().map(word -&gt; &#123; return new StringBuilder(word);&#125;);// constructor referencewords.stream().map(StringBuilder::new);变量作用域在Java8之前， 内部类只能访问final的局部变量 ，为了适应lambda表达式，Java8放宽了这种限制，只要变量实际上不可变（effectively final）就可以。123456public static void effectivelyFinal()&#123; int a = 100; Executors.newSingleThreadExecutor().execute(() -&gt; &#123; System.out.println(a); &#125;); &#125;在lambda表达式中，被引用的变量的值不能被修改。做出这个约束的是有原因的，因为lambda表达式中的变量不是线程安全的。接口的静态方法和默认方法静态方法从Java8开始，接口也可以有静态方法了。有了这个语法，我们就可以把和接口相关的帮助方法直接定义在接口里。比如Function接口就定义了一个工厂方法indentity()。表示， 一个功能接口，可以作为赋值的目标一个lambda表达式或方法参考。T -函数输入的类型R -函数的结果的类型12345public interface Function&lt;T, R&gt; &#123; static &lt;T&gt; Function&lt;T, T&gt; identity() &#123; return t -&gt; t; &#125;&#125;事实上，Java8中，很多接口已经添加了静态方法，例如，Java8中的一个使用案例12345public interface Path &#123; public static Path get(String first, String... more) &#123; return FileSystems.getDefault().getPath(first, more); &#125;&#125;默认方法JDK8 以后接口可以有方法体，通过在方法上添加default的修饰符，就可以添加方法体。接口默认方法的“类优先”原则：若一个接口中定义了一个默认方法，而另外一个父类或接口中又定义了一个同名的方法时选择父类中的方法。 如果一个父类提供了具体的实现，那么接口中具有相同名称和参数的默认方法会被忽略。接口冲突。 如果一个父接口提供一个默认方法，而另一个接口也提供了一个具有相同名称和参数列表的方法（不管方法是否是默认方法）， 那么必须覆盖该方法来解决冲突。对比抽象类这是接口向抽象类的靠近。抽象类可做，接口不可做：抽象类能够定义 非static final 的属性，而接口的属性都是static final的。抽象类能够定义 非public 方法，而接口的方法都是public的。接口可做，抽象类不可做：接口可以多继承(实现)，而抽象类只能单继承。参考博文Java8 特性详解（一） Lambda","categories":[{"name":"java","slug":"java","permalink":"http://www.fufan.me/categories/java/"}],"tags":[{"name":"java8","slug":"java8","permalink":"http://www.fufan.me/tags/java8/"}]},{"title":"java加密算法基础","slug":"java加密算法基础","date":"2017-06-23T13:10:00.000Z","updated":"2018-11-06T15:54:55.766Z","comments":true,"path":"2017/06/23/java加密算法基础/","link":"","permalink":"http://www.fufan.me/2017/06/23/java加密算法基础/","excerpt":"","text":"机密算法分类，基础加密算法基本有四种：Base64Md5ShaHMAC复杂加密有两种：对称加密非对称加密基础加密Base64按 照RFC2045的定义，Base64被定义为：Base64内容传送编码被设计用来把任意序列的8位字节描述为一种不易被人直接识别的形式。（The Base64 Content-Transfer-Encoding is designed to represent arbitrary sequences of octets in a form that need not be humanly readable.）常见于邮件、http加密，截取http信息，你就会发现登录操作的用户名、密码字段通过BASE64加密的。java代码如下：123456789101112131415161718192021/** * BASE64解密 * * @param key * @return * @throws Exception */ public static byte[] decryptBASE64(String key) throws Exception &#123; return (new BASE64Decoder()).decodeBuffer(key); &#125; /** * BASE64加密 * * @param key * @return * @throws Exception */ public static String encryptBASE64(byte[] key) throws Exception &#123; return (new BASE64Encoder()).encodeBuffer(key); &#125;主要就是BASE64Encoder、BASE64Decoder两个类，我们只需要知道使用对应的方法即可。另，BASE加密后产生的字节位数是8的倍数，如果不够位数以=符号填充。MD5MD5 – message-digest algorithm 5 （信息-摘要算法）缩写，广泛用于加密和解密技术，常用于文件校验。校验？不管文件多大，经过MD5后都能生成唯一的MD5值。好比现在的ISO校验，都 是MD5校验。怎么用？当然是把ISO经过MD5后产生MD5的值。一般下载linux-ISO的朋友都见过下载链接旁边放着MD5的串。就是用来验证文 件是否一致的。通过java代码实现如下：123456789101112131415/** * MD5加密 * * @param data * @return * @throws Exception */ public static byte[] encryptMD5(byte[] data) throws Exception &#123; MessageDigest md5 = MessageDigest.getInstance(KEY_MD5); md5.update(data); return md5.digest(); &#125;通常我们不直接使用上述MD5加密。通常将MD5产生的字节数组交给BASE64再加密一把，得到相应的字符串。SHASHA(Secure Hash Algorithm，安全散列算法），数字签名等密码学应用中重要的工具，被广泛地应用于电子商务等信息安全领域。虽然，SHA与MD5通过碰撞法都被破解了， 但是SHA仍然是公认的安全加密算法，较之MD5更为安全。通过java代码实现如下：123456789101112131415161718192021222324252627282930/** * 初始化HMAC密钥 * * @return * @throws Exception */ public static String initMacKey() throws Exception &#123; KeyGenerator keyGenerator = KeyGenerator.getInstance(KEY_MAC); SecretKey secretKey = keyGenerator.generateKey(); return encryptBASE64(secretKey.getEncoded()); &#125; /** * HMAC加密 * * @param data * @param key * @return * @throws Exception */ public static byte[] encryptHMAC(byte[] data, String key) throws Exception &#123; SecretKey secretKey = new SecretKeySpec(decryptBASE64(key), KEY_MAC); Mac mac = Mac.getInstance(secretKey.getAlgorithm()); mac.init(secretKey); return mac.doFinal(data); &#125;BASE64的加密解密是双向的，可以求反解。MD5、SHA以及HMAC是单向加密，任何数据加密后只会产生唯一的一个加密串，通常用来校验数据在传输过程中是否被修改。其中HMAC算法有一个密钥，增强了数据传输过程中的安全性，强化了算法外的不可控因素。单向加密的用途主要是为了校验数据在传输过程中是否被修改。复杂加密对称加密DESDES-Data Encryption Standard,即数据加密算法。是IBM公司于1975年研究成功并公开发表的。DES算法的入口参数有三个:Key、Data、Mode。其中 Key为8个字节共64位,是DES算法的工作密钥;Data也为8个字节64位,是要被加密或被解密的数据;Mode为DES的工作方式,有两种:加密 或解密。DES算法把64位的明文输入块变为64位的密文输出块,它所使用的密钥也是64位。其实DES有很多同胞兄弟，如DESede(TripleDES)、AES、Blowfish、RC2、RC4(ARCFOUR)。这里就不过多阐述了，大同小异，只要换掉ALGORITHM换成对应的值，同时做一个代码替换SecretKey secretKey = new SecretKeySpec(key, ALGORITHM);就可以了，此外就是密钥长度不同了。PBEPBE——Password-based encryption（基于密码加密）。其特点在于口令由用户自己掌管，不借助任何物理媒体；采用随机数（这里我们叫做盐）杂凑多重加密等方法保证数据的安全性。是一种简便的加密方式。非对称加密RSA这种算法1978年就出现了，它是第一个既能用于数据加密也能用于数字签名的算法。它易于理解和操作，也很流行。算法的名字以发明者的名字命名：Ron Rivest, AdiShamir 和Leonard Adleman。这种加密算法的特点主要是密钥的变化，上文我们看到DES只有一个密钥。相当于只有一把钥匙，如果这把钥匙丢了，数据也就不安全了。RSA同时有两把钥 匙，公钥与私钥。同时支持数字签名。数字签名的意义在于，对传输过来的数据进行校验。确保数据在传输工程中不被修改。流程分析：甲方构建密钥对儿，将公钥公布给乙方，将私钥保留。甲方使用私钥加密数据，然后用私钥对加密后的数据签名，发送给乙方签名以及加密后的数据；乙方使用公钥、签名来验证待解密数据是否有效，如果有效使用公钥对数据解密。乙方使用公钥加密数据，向甲方发送经过加密后的数据；甲方获得加密数据，通过私钥解密。简要总结一下，使用公钥加密、私钥解密，完成了乙方到甲方的一次数据传递，通过私钥加密、公钥解密，同时通过私钥签名、公钥验证签名，完成了一次甲方到乙方的数据传递与验证，两次数据传递完成一整套的数据交互！类似数字签名，数字信封是这样描述的：数字信封数字信封用加密技术来保证只有特定的收信人才能阅读信的内容。 流程：信息发送方采用对称密钥来加密信息，然后再用接收方的公钥来加密此对称密钥（这部分称为数字信封），再将它和信息一起发送给接收方；接收方先用相应的私钥打开数字信封，得到对称密钥，然后使用对称密钥再解开信息。接下来我们分析DH加密算法，一种适基于密钥一致协议的加密算法。DHDiffie- Hellman算法(D-H算法)，密钥一致协议。是由公开密钥密码体制的奠基人Diffie和Hellman所提出的一种思想。简单的说就是允许两名用 户在公开媒体上交换信息以生成”一致”的、可以共享的密钥。换句话说，就是由甲方产出一对密钥（公钥、私钥），乙方依照甲方公钥产生乙方密钥对（公钥、私 钥）。以此为基线，作为数据传输保密基础，同时双方使用同一种对称加密算法构建本地密钥（SecretKey）对数据加密。这样，在互通了本地密钥 （SecretKey）算法后，甲乙双方公开自己的公钥，使用对方的公钥和刚才产生的私钥加密数据，同时可以使用对方的公钥和自己的私钥对数据解密。不单 单是甲乙双方两方，可以扩展为多方共享数据通讯，这样就完成了网络交互数据的安全通讯！该算法源于中国的同余定理——中国馀数定理。流程分析：甲方构建密钥对儿，将公钥公布给乙方，将私钥保留；双方约定数据加密算法；乙方通过甲方公钥构建密钥对儿，将公钥公布给甲方，将私钥保留。甲方使用私钥、乙方公钥、约定数据加密算法构建本地密钥，然后通过本地密钥加密数据，发送给乙方加密后的数据；乙方使用私钥、甲方公钥、约定数据加密算法构建本地密钥，然后通过本地密钥对数据解密。乙方使用私钥、甲方公钥、约定数据加密算法构建本地密钥，然后通过本地密钥加密数据，发送给甲方加密后的数据；甲方使用私钥、乙方公钥、约定数据加密算法构建本地密钥，然后通过本地密钥对数据解密。常见加密算法DES（Data Encryption Standard）：数据加密标准，速度较快，适用于加密大量数据的场合；3DES（Triple DES）：是基于DES，对一块数据用三个不同的密钥进行三次加密，强度更高；RC2和 RC4：用变长密钥对大量数据进行加密，比 DES 快；IDEA（International Data Encryption Algorithm）国际数据加密算法：使用 128 位密钥提供非常强的安全性；RSA：由 RSA 公司发明，是一个支持变长密钥的公共密钥算法，需要加密的文件块的长度也是可变的；DSA（Digital Signature Algorithm）：数字签名算法，是一种标准的 DSS（数字签名标准）；AES（Advanced Encryption Standard）：高级加密标准，是下一代的加密算法标准，速度快，安全级别高，目前 AES 标准的一个实现是 Rijndael 算法；BLOWFISH，它使用变长的密钥，长度可达448位，运行速度很快；其它算法，如ElGamal、Deffie-Hellman、新型椭圆曲线算法ECC等。 比如说，MD5，你在一些比较正式而严格的网站下的东西一般都会有MD5值给出，如安全焦点的软件工具，每个都有MD5。严格来说MD5并不能算是一种加密算法，只能说是一种摘要算法（数据摘要算法是密码学算法中非常重要的一个分支，它通过对所有数据提取指纹信息以实现数据签名、数据完整性校验等功能，由于其不可逆性，有时候会被用做敏感信息的加密。数据摘要算法也被称为哈希(Hash)算法、散列算法。）参考博文各种Java加密算法","categories":[{"name":"加密算法","slug":"加密算法","permalink":"http://www.fufan.me/categories/加密算法/"}],"tags":[{"name":"加密","slug":"加密","permalink":"http://www.fufan.me/tags/加密/"}]},{"title":"Java性能优化N个建议（持续更新）","slug":"Java性能优化N个建议（持续更新）","date":"2017-06-11T14:34:00.000Z","updated":"2018-11-11T14:35:33.075Z","comments":true,"path":"2017/06/11/Java性能优化N个建议（持续更新）/","link":"","permalink":"http://www.fufan.me/2017/06/11/Java性能优化N个建议（持续更新）/","excerpt":"","text":"代码优化， 一个很重要的课题。可能有些人觉得没用，一些细小的地方有什么好修改的，改与不改对于代码的运行效率有什么影响呢？这个问题我是这么考虑的，就像大海里面的鲸鱼一样，它吃一条小虾米有用吗？没用，但是，吃的小虾米一多之后，鲸鱼就被喂饱了。代码优化也是一样，如果项目着眼于尽快无BUG上线，那么此时可以抓大放小，代码的细节可以不精打细磨；但是如果有足够的时间开发、维护代码，这时候就必须考虑每个可以优化的细节了，一个一个细小的优化点累积起来，对于代码的运行效率绝对是有提升的。这里除了下面我列举的这些点意外，我们还可以参考阿里巴巴java开发规范和google的java代码开发规范来写规范的代码目标减小代码的体积提高代码运行的效率使代码变优雅具体细节1、尽量指定类、方法的final修饰符2、尽量重用对象3、尽可能使用局部变量4、及时关闭流5、尽量减少对变量的重复计算6、尽量采用懒加载的策略，即在需要的时候才创建7、慎用异常8、不要在循环中使用try…catch…，应该把其放在最外层9、如果能估计到待添加的内容长度，为底层以数组方式实现的集合、工具类指定初始长度10、当复制大量数据时，使用System.arraycopy()命令11、乘法和除法使用移位操作12、循环内不要不断创建对象引用13、基于效率和类型检查的考虑，应该尽可能使用array，无法确定数组大小时才使用ArrayList14、尽量使用HashMap、ArrayList、StringBuilder，除非线程安全需要，否则不推荐使用Hashtable、Vector、StringBuffer，后三者由于使用同步机制而导致了性能开销15、不要将数组声明为public static final16、尽量在合适的场合使用单例17、尽量避免随意使用静态变量18、及时清除不再需要的会话19、实现RandomAccess接口的集合比如ArrayList，应当使用最普通的for循环而不是foreach循环来遍历foreach循环的底层实现原理就是迭代器Iterator，参见Java语法糖1：可变长度参数以及foreach循环原理。所以后半句”反过来，如果是顺序访问的，则使用Iterator会效率更高”的意思就是顺序访问的那些类实例，使用foreach循环去遍历。详见 [java常用集合源码分析之ArrayList遍历方式以及效率比较（五）]20、使用同步代码块替代同步方法21、将常量声明为static final，并以大写命名22、不要创建一些不使用的对象，不要导入一些不使用的类23、程序运行过程中避免使用反射24、使用数据库连接池和线程池25、使用带缓冲的输入输出流进行IO操作26、顺序插入和随机访问比较多的场景使用ArrayList，元素删除和中间插入比较多的场景使用LinkedList27、不要让public方法中有太多的形参28、字符串变量和字符串常量equals的时候将字符串常量写在前面29、请知道，在java中if (i == 1)和if (1 == i)是没有区别的，但从阅读习惯上讲，建议使用前者30、不要对数组使用toString()方法31、不要对超出范围的基本数据类型做向下强制转型32、公用的集合类中不使用的数据一定要及时remove掉33、把一个基本数据类型转为字符串，基本数据类型.toString()是最快的方式、String.valueOf(数据)次之、数据+””最慢34、使用最有效率的方式去遍历Map35、对资源的close()建议分开操作","categories":[{"name":"java","slug":"java","permalink":"http://www.fufan.me/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://www.fufan.me/tags/java/"}]},{"title":"java多线程系列（七）——JUC锁","slug":"java多线程系列（七）——JUC锁","date":"2017-06-05T17:47:00.000Z","updated":"2018-11-05T17:55:14.987Z","comments":true,"path":"2017/06/06/java多线程系列（七）——JUC锁/","link":"","permalink":"http://www.fufan.me/2017/06/06/java多线程系列（七）——JUC锁/","excerpt":"","text":"下面介绍一下JUC包中可以让我们在多线程并发中使用的锁。UC包中的锁，包括：Lock接口ReadWriteLock接口LockSupport阻塞原语Condition条件AbstractOwnableSynchronizer/AbstractQueuedSynchronizer/AbstractQueuedLongSynchronizer三个抽象类ReentrantLock独占锁ReentrantReadWriteLock读写锁由于CountDownLatch，CyclicBarrier和Semaphore也是通过AQS来实现的；因此，我也将它们归纳到锁的框架中进行介绍。先看看锁的框架图，如下所示。下面简述一下每个类或接口Lock接口JUC包中的 Lock 接口支持那些语义不同(重入、公平等)的锁规则。所谓语义不同，是指锁可是有”公平机制的锁”、”非公平机制的锁”、”可重入的锁”等等。”公平机制”是指”不同线程获取锁的机制是公平的”，而”非公平机制”则是指”不同线程获取锁的机制是非公平的”，”可重入的锁”是指同一个锁能够被一个线程多次获取。ReadWriteLockReadWriteLock 接口以和Lock类似的方式定义了一些读取者可以共享而写入者独占的锁。JUC包只有一个类实现了该接口，即 ReentrantReadWriteLock，因为它适用于大部分的标准用法上下文。但程序员可以创建自己的、适用于非标准要求的实现。AbstractOwnableSynchronizer/AbstractQueuedSynchronizer/AbstractQueuedLongSynchronizerAbstractQueuedSynchronizer就是被称之为AQS的类，它是一个非常有用的超类，可用来定义锁以及依赖于排队阻塞线程的其他同步器；ReentrantLock，ReentrantReadWriteLock，CountDownLatch，CyclicBarrier和Semaphore等这些类都是基于AQS类实现的。AbstractQueuedLongSynchronizer 类提供相同的功能但扩展了对同步状态的 64 位的支持。两者都扩展了类 AbstractOwnableSynchronizer（一个帮助记录当前保持独占同步的线程的简单类）。LockSupportLockSupport提供“创建锁”和“其他同步类的基本线程阻塞原语”。LockSupport的功能和”Thread中的Thread.suspend()和Thread.resume()有点类似”，LockSupport中的park() 和 unpark() 的作用分别是阻塞线程和解除阻塞线程。但是park()和unpark()不会遇到“Thread.suspend 和 Thread.resume所可能引发的死锁”问题。ConditionCondition需要和Lock联合使用，它的作用是代替Object监视器方法，可以通过await(),signal()来休眠/唤醒线程。Condition 接口描述了可能会与锁有关联的条件变量。这些变量在用法上与使用 Object.wait 访问的隐式监视器类似，但提供了更强大的功能。需要特别指出的是，单个 Lock 可能与多个 Condition 对象关联。为了避免兼容性问题，Condition 方法的名称与对应的 Object 版本中的不同。ReentrantLockReentrantLock是独占锁。所谓独占锁，是指只能被独自占领，即同一个时间点只能被一个线程锁获取到的锁。ReentrantLock锁包括”公平的ReentrantLock”和”非公平的ReentrantLock”。”公平的ReentrantLock”是指”不同线程获取锁的机制是公平的”，而”非公平的 ReentrantLock”则是指”不同线程获取锁的机制是非公平的”，ReentrantLock是”可重入的锁”。ReentrantLock的UML类图如下：ReentrantLock实现了Lock接口。ReentrantLock中有一个成员变量sync，sync是Sync类型；Sync是一个抽象类，而且它继承于AQS。ReentrantLock中有”公平锁类”FairSync和”非公平锁类”NonfairSync，它们都是Sync的子类。ReentrantReadWriteLock中sync对象，是FairSync与NonfairSync中的一种，这也意味着ReentrantLock是”公平锁”或”非公平锁”中的一种，ReentrantLock默认是非公平锁。ReentrantReadWriteLockReentrantReadWriteLock是读写锁接口ReadWriteLock的实现类，它包括子类ReadLock和WriteLock。ReentrantLock是共享锁，而WriteLock是独占锁。ReentrantReadWriteLock的UML类图如下：ReentrantReadWriteLock实现了ReadWriteLock接口。ReentrantReadWriteLock中包含sync对象，读锁readerLock和写锁writerLock。读锁ReadLock和写锁WriteLock都实现了Lock接口。和”ReentrantLock”一样，sync是Sync类型；而且，Sync也是一个继承于AQS的抽象类。Sync也包括”公平锁”FairSync和”非公平锁”NonfairSync。CountDownLatchCountDownLatch是一个同步辅助类，在完成一组正在其他线程中执行的操作之前，它允许一个或多个线程一直等待。CountDownLatch的UML类图如下：CountDownLatch包含了sync对象，sync是Sync类型。CountDownLatch的Sync是实例类，它继承于AQS。CyclicBarrierCyclicBarrier是一个同步辅助类，允许一组线程互相等待，直到到达某个公共屏障点 (common barrier point)。因为该 barrier 在释放等待线程后可以重用，所以称它为循环 的 barrier。CyclicBarrier的UML类图如下：CyclicBarrier是包含了”ReentrantLock对象lock”和”Condition对象trip”，它是通过独占锁实现的。CyclicBarrier和CountDownLatch的区别是：1. CountDownLatch的作用是允许1或N个线程等待其他线程完成执行；而CyclicBarrier则是允许N个线程相互等待。2. CountDownLatch的计数器无法被重置；CyclicBarrier的计数器可以被重置后使用，因此它被称为是循环的barrier。SemaphoreSemaphore是一个计数信号量，它的本质是一个”共享锁”。信号量维护了一个信号量许可集。线程可以通过调用acquire()来获取信号量的许可；当信号量中有可用的许可时，线程能获取该许可；否则线程必须等待，直到有可用的许可为止。 线程可以通过release()来释放它所持有的信号量许可。Semaphore的UML类图如下：和”ReentrantLock”一样，Semaphore包含了sync对象，sync是Sync类型；而且，Sync也是一个继承于AQS的抽象类。Sync也包括”公平信号量”FairSync和”非公平信号量”NonfairSync。参考博文Java多线程系列目录(共43篇)","categories":[{"name":"多线程","slug":"多线程","permalink":"http://www.fufan.me/categories/多线程/"}],"tags":[{"name":"多线程","slug":"多线程","permalink":"http://www.fufan.me/tags/多线程/"}]},{"title":"java多线程系列（六）——JUC并发集合","slug":"java多线程系列（六）——JUC并发集合","date":"2017-05-30T17:29:00.000Z","updated":"2018-11-05T17:30:24.014Z","comments":true,"path":"2017/05/31/java多线程系列（六）——JUC并发集合/","link":"","permalink":"http://www.fufan.me/2017/05/31/java多线程系列（六）——JUC并发集合/","excerpt":"","text":"说到并发集合，还是先回到java的集合包里来。Java集合包集合包主要包括两大类：CollectionListSetMapList的实现类主要有: LinkedList, ArrayList, Vector, Stack。LinkedList是双向链表实现的双端队列；它不是线程安全的，只适用于单线程。ArrayList是数组实现的队列，它是一个动态数组；它也不是线程安全的，只适用于单线程。Vector是数组实现的矢量队列，它也一个动态数组；不过和ArrayList不同的是，Vector是线程安全的，它支持并发。Stack是Vector实现的栈；和Vector一样，它也是线程安全的。Set的实现类主要有: HastSet和TreeSet。HashSet是一个没有重复元素的集合，它通过HashMap实现的；HashSet不是线程安全的，只适用于单线程。TreeSet也是一个没有重复元素的集合，不过和HashSet不同的是，TreeSet中的元素是有序的；它是通过TreeMap实现的；TreeSet也不是线程安全的，只适用于单线程。Map的实现类主要有: HashMap，WeakHashMap, Hashtable和TreeMap。HashMap是存储“键-值对”的哈希表；它不是线程安全的，只适用于单线程。WeakHashMap是也是哈希表；和HashMap不同的是，HashMap的“键”是强引用类型，而WeakHashMap的“键”是弱引用类型，也就是说当WeakHashMap 中的某个键不再正常使用时，会被从WeakHashMap中被自动移除。WeakHashMap也不是线程安全的，只适用于单线程。Hashtable也是哈希表；和HashMap不同的是，Hashtable是线程安全的，支持并发。TreeMap也是哈希表，不过TreeMap中的“键-值对”是有序的，它是通过R-B Tree(红黑树)实现的；TreeMap不是线程安全的，只适用于单线程。为了方便，我们将前面介绍集合类统称为”java集合包“。java集合包大多是“非线程安全的”，虽然可以通过Collections工具类中的方法获取java集合包对应的同步类，但是这些同步类的并发效率并不是很高。回顾完这些集合以后，我发现工作中其实多线程中使用到的集合印象深刻点的就是LinkedBlockingQueue，就是在线程池那里涉及到过，不过其实还有很多类似的变种，并发大师Doug Lea在JUC(java.util.concurrent)包中添加了java集合包中单线程类的对应的支持高并发的类。例如，ArrayList对应的高并发类是CopyOnWriteArrayList，HashMap对应的高并发类是ConcurrentHashMap，等等。JUC包在添加”java集合包“对应的高并发类时，为了保持API接口的一致性，使用了”Java集合包“中的框架。例如，CopyOnWriteArrayList实现了“Java集合包”中的List接口，HashMap继承了“java集合包”中的AbstractMap类，等等。得益于“JUC包使用了Java集合包中的类”，如果我们了解了Java集合包中的类的思想之后，理解JUC包中的类也相对容易；理解时，最大的难点是，对JUC包是如何添加对“高并发”的支持的！JUC中的集合类List和SetJUC集合包中的List和Set实现类包括: CopyOnWriteArrayList, CopyOnWriteArraySet和ConcurrentSkipListSet。ConcurrentSkipListSet稍后在说明Map时再说明，CopyOnWriteArrayList 和 CopyOnWriteArraySet的框架如下图所示：CopyOnWriteArrayList相当于线程安全的ArrayList，它实现了List接口。CopyOnWriteArrayList是支持高并发的。CopyOnWriteArraySet相当于线程安全的HashSet，它继承于AbstractSet类。CopyOnWriteArraySet内部包含一个CopyOnWriteArrayList对象，它是通过CopyOnWriteArrayList实现的。MapJUC集合包中Map的实现类包括: ConcurrentHashMap和ConcurrentSkipListMap。它们的框架如下图所示：ConcurrentHashMap是线程安全的哈希表(相当于线程安全的HashMap)；它继承于AbstractMap类，并且实现ConcurrentMap接口。ConcurrentHashMap是通过“锁分段”来实现的，它支持并发。ConcurrentSkipListMap是线程安全的有序的哈希表(相当于线程安全的TreeMap); 它继承于AbstractMap类，并且实现ConcurrentNavigableMap接口。ConcurrentSkipListMap是通过“跳表”来实现的，它支持并发。ConcurrentSkipListSet是线程安全的有序的集合(相当于线程安全的TreeSet)；它继承于AbstractSet，并实现了NavigableSet接口。ConcurrentSkipListSet是通过ConcurrentSkipListMap实现的，它也支持并发。QueueJUC集合包中Queue的实现类包括: ArrayBlockingQueue, LinkedBlockingQueue, LinkedBlockingDeque, ConcurrentLinkedQueue和ConcurrentLinkedDeque。它们的框架如下图所示：ArrayBlockingQueue是数组实现的线程安全的有界的阻塞队列。LinkedBlockingQueue是单向链表实现的(指定大小)阻塞队列，该队列按 FIFO（先进先出）排序元素。LinkedBlockingDeque是双向链表实现的(指定大小)双向并发阻塞队列，该阻塞队列同时支持FIFO和FILO两种操作方式。ConcurrentLinkedQueue是单向链表实现的无界队列，该队列按 FIFO（先进先出）排序元素。ConcurrentLinkedDeque是双向链表实现的无界队列，该队列同时支持FIFO和FILO两种操作方式参考博文Java多线程系列目录(共43篇)","categories":[{"name":"多线程","slug":"多线程","permalink":"http://www.fufan.me/categories/多线程/"}],"tags":[{"name":"多线程","slug":"多线程","permalink":"http://www.fufan.me/tags/多线程/"}]},{"title":"java多线程系列（五）——JUC原子类","slug":"java多线程系列（五）——JUC原子类","date":"2017-05-25T17:09:00.000Z","updated":"2018-11-05T17:10:03.303Z","comments":true,"path":"2017/05/26/java多线程系列（五）——JUC原子类/","link":"","permalink":"http://www.fufan.me/2017/05/26/java多线程系列（五）——JUC原子类/","excerpt":"","text":"根据修改的数据类型，可以将JUC包中的原子操作类可以分为4类。基本类型: AtomicInteger, AtomicLong, AtomicBoolean ;数组类型: AtomicIntegerArray, AtomicLongArray, AtomicReferenceArray ;引用类型: AtomicReference, AtomicStampedRerence, AtomicMarkableReference ;对象的属性修改类型: AtomicIntegerFieldUpdater, AtomicLongFieldUpdater, AtomicReferenceFieldUpdater 。这些类存在的目的是对相应的数据进行原子操作。所谓原子操作，是指操作过程不会被中断，保证数据操作是以原子方式进行的。值得一提的是，Java的AtomXXX类并不是使用了锁的方式进行同步，而是采用了一种新的理念，叫做CAS.CAS是一组原语指令，用来实现多线程下的变量同步。在 x86 下的指令CMPXCHG实现了CAS，前置LOCK既可以达到原子性操作。由于CAS原语的直接操作与计算机底层的联系很大，CAS原语有三个参数，内存地址，期望值，新值。我们在Java中一般不去直接写CAS相关的代码，JDK为我们封装在AtomXXX中，因此，我们直接使用就可以了。基本类型用AtomicLong来举例主要函数AtomicLong是作用是对长整形进行原子操作。在32位操作系统中，64位的long 和 double 变量由于会被JVM当作两个分离的32位来进行操作，所以不具有原子性。而使用AtomicLong能让long的操作保持原子型。123456789101112131415161718192021222324252627282930313233343536// 构造函数AtomicLong()// 创建值为initialValue的AtomicLong对象AtomicLong(long initialValue)// 以原子方式设置当前值为newValue。final void set(long newValue) // 获取当前值final long get() // 以原子方式将当前值减 1，并返回减1后的值。等价于“--num”final long decrementAndGet() // 以原子方式将当前值减 1，并返回减1前的值。等价于“num--”final long getAndDecrement() // 以原子方式将当前值加 1，并返回加1后的值。等价于“++num”final long incrementAndGet() // 以原子方式将当前值加 1，并返回加1前的值。等价于“num++”final long getAndIncrement() // 以原子方式将delta与当前值相加，并返回相加后的值。final long addAndGet(long delta) // 以原子方式将delta添加到当前值，并返回相加前的值。final long getAndAdd(long delta) // 如果当前值 == expect，则以原子方式将该值设置为update。成功返回true，否则返回false，并且不修改原值。final boolean compareAndSet(long expect, long update)// 以原子方式设置当前值为newValue，并返回旧值。final long getAndSet(long newValue)// 返回当前值对应的int值int intValue() // 获取当前值对应的long值long longValue() // 以 float 形式返回当前值float floatValue() // 以 double 形式返回当前值double doubleValue() // 最后设置为给定值。延时设置变量值，这个等价于set()方法，但是由于字段是volatile类型的，因此次字段的修改会比普通字段（非volatile字段）有稍微的性能延时（尽管可以忽略），所以如果不是想立即读取设置的新值，允许在“后台”修改值，那么此方法就很有用。如果还是难以理解，这里就类似于启动一个后台线程如执行修改新值的任务，原线程就不等待修改结果立即返回（这种解释其实是不正确的，但是可以这么理解）。final void lazySet(long newValue)// 如果当前值 == 预期值，则以原子方式将该设置为给定的更新值。JSR规范中说：以原子方式读取和有条件地写入变量但不 创建任何 happen-before 排序，因此不提供与除 weakCompareAndSet 目标外任何变量以前或后续读取或写入操作有关的任何保证。大意就是说调用weakCompareAndSet时并不能保证不存在happen-before的发生（也就是可能存在指令重排序导致此操作失败）。但是从Java源码来看，其实此方法并没有实现JSR规范的要求，最后效果和compareAndSet是等效的，都调用了unsafe.compareAndSwapInt()完成操作。final boolean weakCompareAndSet(long expect, long update)源码分析AtomicLong的代码很简单，下面仅以incrementAndGet()为例，对AtomicLong的原理进行说明。incrementAndGet()源码如下：1234567891011public final long incrementAndGet() &#123; for (;;) &#123; // 获取AtomicLong当前对应的long值 long current = get(); // 将current加1 long next = current + 1; // 通过CAS函数，更新current的值 if (compareAndSet(current, next)) return next; &#125;&#125;说明：(01) incrementAndGet()首先会根据get()获取AtomicLong对应的long值。该值是volatile类型的变量，get()的源码如下：123456// value是AtomicLong对应的long值private volatile long value;// 返回AtomicLong对应的long值public final long get() &#123; return value;&#125;(02) incrementAndGet()接着将current加1,然后通过CAS函数，将新的值赋值给value。compareAndSet()的源码如下：123public final boolean compareAndSet(long expect, long update) &#123; return unsafe.compareAndSwapLong(this, valueOffset, expect, update);&#125;compareAndSet()的作用是更新AtomicLong对应的long值。它会比较AtomicLong的原始值是否与expect相等，若相等的话，则设置AtomicLong的值为update。数组类型AtomicLongArray函数列表123456789101112131415161718192021222324252627282930313233// 创建给定长度的新 AtomicLongArray。AtomicLongArray(int length)// 创建与给定数组具有相同长度的新 AtomicLongArray，并从给定数组复制其所有元素。AtomicLongArray(long[] array)// 以原子方式将给定值添加到索引 i 的元素。long addAndGet(int i, long delta)// 如果当前值 == 预期值，则以原子方式将该值设置为给定的更新值。boolean compareAndSet(int i, long expect, long update)// 以原子方式将索引 i 的元素减1。long decrementAndGet(int i)// 获取位置 i 的当前值。long get(int i)// 以原子方式将给定值与索引 i 的元素相加。long getAndAdd(int i, long delta)// 以原子方式将索引 i 的元素减 1。long getAndDecrement(int i)// 以原子方式将索引 i 的元素加 1。long getAndIncrement(int i)// 以原子方式将位置 i 的元素设置为给定值，并返回旧值。long getAndSet(int i, long newValue)// 以原子方式将索引 i 的元素加1。long incrementAndGet(int i)// 最终将位置 i 的元素设置为给定值。void lazySet(int i, long newValue)// 返回该数组的长度。int length()// 将位置 i 的元素设置为给定值。void set(int i, long newValue)// 返回数组当前值的字符串表示形式。String toString()// 如果当前值 == 预期值，则以原子方式将该值设置为给定的更新值。boolean weakCompareAndSet(int i, long expect, long update)源码分析同AtomicLong类似incrementAndGet()源码如下：123public final long incrementAndGet(int i) &#123; return addAndGet(i, 1);&#125;说明：incrementAndGet()的作用是以原子方式将long数组的索引 i 的元素加1，并返回加1之后的值。addAndGet()源码如下：12345678910111213public long addAndGet(int i, long delta) &#123; // 检查数组是否越界 long offset = checkedByteOffset(i); while (true) &#123; // 获取long型数组的索引 offset 的原始值 long current = getRaw(offset); // 修改long型值 long next = current + delta; // 通过CAS更新long型数组的索引 offset的值。 if (compareAndSetRaw(offset, current, next)) return next; &#125;&#125;getRaw()源码如下：123private long getRaw(long offset) &#123; return unsafe.getLongVolatile(array, offset);&#125;说明：unsafe是通过Unsafe.getUnsafe()返回的一个Unsafe对象。通过Unsafe的CAS函数对long型数组的元素进行原子操作。如compareAndSetRaw()就是调用Unsafe的CAS函数，它的源码如下：123private boolean compareAndSetRaw(long offset, long expect, long update) &#123; return unsafe.compareAndSwapLong(array, offset, expect, update);&#125;说明：addAndGet()首先检查数组是否越界。如果没有越界的话，则先获取数组索引i的值；然后通过CAS函数更新i的值。引用类型AtomicReference函数列表12345678910111213141516171819// 使用 null 初始值创建新的 AtomicReference。AtomicReference()// 使用给定的初始值创建新的 AtomicReference。AtomicReference(V initialValue)// 如果当前值 == 预期值，则以原子方式将该值设置为给定的更新值。boolean compareAndSet(V expect, V update)// 获取当前值。V get()// 以原子方式设置为给定值，并返回旧值。V getAndSet(V newValue)// 最终设置为给定值。void lazySet(V newValue)// 设置为给定值。void set(V newValue)// 返回当前值的字符串表示形式。String toString()// 如果当前值 == 预期值，则以原子方式将该值设置为给定的更新值。boolean weakCompareAndSet(V expect, V update)源码123456789101112131415161718192021222324252627282930// AtomicReferenceTest.java的源码import java.util.concurrent.atomic.AtomicReference;public class AtomicReferenceTest &#123; public static void main(String[] args)&#123; // 创建两个Person对象，它们的id分别是101和102。 Person p1 = new Person(101); Person p2 = new Person(102); // 新建AtomicReference对象，初始化它的值为p1对象 AtomicReference ar = new AtomicReference(p1); // 通过CAS设置ar。如果ar的值为p1的话，则将其设置为p2。 ar.compareAndSet(p1, p2); Person p3 = (Person)ar.get(); System.out.println(\"p3 is \"+p3); System.out.println(\"p3.equals(p1)=\"+p3.equals(p1)); &#125;&#125;class Person &#123; volatile long id; public Person(long id) &#123; this.id = id; &#125; public String toString() &#123; return \"id:\"+id; &#125;&#125;AtomicReference的源码比较简单。它是通过”volatile”和”Unsafe提供的CAS函数实现”原子操作。value是volatile类型。这保证了：当某线程修改value的值时，其他线程看到的value值都是最新的value值，即修改之后的volatile的值。通过CAS设置value。这保证了：当某线程池通过CAS函数(如compareAndSet函数)设置value时，它的操作是原子的，即线程在操作value时不会被中断。对象的属性修改类型AtomicIntegerFieldUpdater简单介绍一下同上面的几种类型一样，也是通过原子和CAS的方式来保证多线程使用变量同步不会出问题。AtomicLongFieldUpdater示例代码实例：12345678910111213141516171819202122232425262728293031// LongTest.java的源码import java.util.concurrent.atomic.AtomicLongFieldUpdater;public class LongFieldTest &#123; public static void main(String[] args) &#123; // 获取Person的class对象 Class cls = Person.class; // 新建AtomicLongFieldUpdater对象，传递参数是“class对象”和“long类型在类中对应的名称” AtomicLongFieldUpdater mAtoLong = AtomicLongFieldUpdater.newUpdater(cls, \"id\"); Person person = new Person(12345678L); // 比较person的\"id\"属性，如果id的值为12345678L，则设置为1000。 mAtoLong.compareAndSet(person, 12345678L, 1000); System.out.println(\"id=\"+person.getId()); &#125;&#125;class Person &#123; volatile long id; public Person(long id) &#123; this.id = id; &#125; public void setId(long id) &#123; this.id = id; &#125; public long getId() &#123; return id; &#125;&#125;运行结果：1id=1000参考博文Java多线程系列目录(共43篇)","categories":[{"name":"多线程","slug":"多线程","permalink":"http://www.fufan.me/categories/多线程/"}],"tags":[{"name":"多线程","slug":"多线程","permalink":"http://www.fufan.me/tags/多线程/"}]},{"title":"java多线程系列（四）——CAS和AQS学习","slug":"java多线程系列（四）——CAS和AQS学习","date":"2017-05-20T12:16:00.000Z","updated":"2018-11-05T12:18:32.323Z","comments":true,"path":"2017/05/20/java多线程系列（四）——CAS和AQS学习/","link":"","permalink":"http://www.fufan.me/2017/05/20/java多线程系列（四）——CAS和AQS学习/","excerpt":"","text":"CASCAS的全称是Compare And Swap 即比较交换，其算法核心思想如下1执行函数：CAS(V,E,N)其包含3个参数V表示要更新的变量E表示预期值N表示新值如果V值等于E值，则将V的值设为N。若V值和E值不同，则说明已经有其他线程做了更新，则当前线程什么都不做。通俗的理解就是CAS操作需要我们提供一个期望值，当期望值与当前线程的变量值相同时，说明还没线程修改该值，当前线程可以进行修改，也就是执行CAS操作，但如果期望值与当前线程不符，则说明该值已被其他线程修改，此时不执行更新操作，但可以选择重新读取该变量再尝试再次修改该变量，也可以放弃操作.由于CAS操作属于乐观派，它总认为自己可以成功完成操作，当多个线程同时使用CAS操作一个变量时，只有一个会胜出，并成功更新，其余均会失败，但失败的线程并不会被挂起，仅是被告知失败，并且允许再次尝试，当然也允许失败的线程放弃操作，这点从图中也可以看出来。基于这样的原理，CAS操作即使没有锁，同样知道其他线程对共享资源操作影响，并执行相应的处理措施。同时从这点也可以看出，由于无锁操作中没有锁的存在，因此不可能出现死锁的情况，也就是说无锁操作天生免疫死锁.鲜为人知的指针: Unsafe类Unsafe类存在于sun.misc包中，其内部方法操作可以像C的指针一样直接操作内存，单从名称看来就可以知道该类是非安全的，毕竟Unsafe拥有着类似于C的指针操作，因此总是不应该首先使用Unsafe类，Java官方也不建议直接使用的Unsafe类，但我们还是很有必要了解该类，因为Java中CAS操作的执行依赖于Unsafe类的方法，注意Unsafe类中的所有方法都是native修饰的，也就是说Unsafe类中的方法都直接调用操作系统底层资源执行相应任务.CAS是一些CPU直接支持的指令，也就是我们前面分析的无锁操作，在Java中无锁操作CAS基于以下3个方法实现，在稍后讲解Atomic系列内部方法是基于下述方法的实现的。CAS的ABA问题及其解决方案假设这样一种场景，当第一个线程执行CAS(V,E,U)操作，在获取到当前变量V，准备修改为新值U前，另外两个线程已连续修改了两次变量V的值，使得该值又恢复为旧值，这样的话，我们就无法正确判断这个变量是否已被修改过这就是典型的CAS的ABA问题，一般情况这种情况发现的概率比较小，可能发生了也不会造成什么问题，比如说我们对某个做加减法，不关心数字的过程，那么发生ABA问题也没啥关系。但是在某些情况下还是需要防止的，那么该如何解决呢？在Java中解决ABA问题，我们可以使用以下两个原子类AtomicStampedReference类AtomicStampedReference原子类是一个带有时间戳的对象引用，在每次修改后，AtomicStampedReference不仅会设置新值而且还会记录更改的时间。当AtomicStampedReference设置对象值时，对象值以及时间戳都必须满足期望值才能写入成功，这也就解决了反复读写时，无法预知值是否已被修改的窘境同此类类似，还有AtomicMarkableReference类，这种方式并不能完全防止ABA问题的发生，只能减少ABA问题发生的概率。AtomicMarkableReference的实现原理与AtomicStampedReference类似，这里不再介绍。到此，我们也明白了如果要完全杜绝ABA问题的发生，我们应该使用AtomicStampedReference原子类更新对象，而对于AtomicMarkableReference来说只能减少ABA问题的发生概率，并不能杜绝。AQSCLH队列AQS内部维护着一个FIFO的队列，即CLH队列。AQS的同步机制就是依靠CLH队列实现的。CLH队列是FIFO的双端双向队列，实现公平锁。线程通过AQS获取锁失败，就会将线程封装成一个Node节点，插入队列尾。当有线程释放锁时，后尝试把队头的next节点占用锁。CLH队列结构NodeCLH队列由Node对象组成，Node是AQS中的内部类。 在CLH同步队列中，一个节点表示一个线程，它保存着线程的引用（thread）、状态（waitStatus）、前驱节点（prev）、后继节点（next） 入列addWaiter(Node.EXCLUSIVE)方法会将当前线程封装成Node节点，追加在队尾。1234567891011121314151617private Node addWaiter(Node mode) &#123; //新建Node Node node = new Node(Thread.currentThread(), mode); //快速尝试添加尾节点 Node pred = tail; if (pred != null) &#123; node.prev = pred; //CAS设置尾节点 if (compareAndSetTail(pred, node)) &#123; pred.next = node; return node; &#125; &#125; //多次尝试 enq(node); return node; &#125;addWaiter(Node node)先通过快速尝试设置尾节点，如果失败，则调用enq(Node node)方法设置尾节点123456789101112131415161718private Node enq(final Node node) &#123; //多次尝试，直到成功为止 for (;;) &#123; Node t = tail; //tail不存在，设置为首节点 if (t == null) &#123; if (compareAndSetHead(new Node())) tail = head; &#125; else &#123; //设置为尾节点 node.prev = t; if (compareAndSetTail(t, node)) &#123; t.next = node; return t; &#125; &#125; &#125; &#125;在上面代码中，两个方法都是通过一个CAS方法compareAndSetTail(Node expect, Node update)来设置尾节点，该方法可以确保节点是线程安全添加的。在enq(Node node)方法中，AQS通过“死循环”的方式来保证节点可以正确添加，只有成功添加后，当前线程才会从该方法返回，否则会一直执行下去。过程图如下：出列CLH同步队列遵循FIFO，首节点的线程释放同步状态后，将会唤醒它的后继节点（next），而后继节点将会在获取同步状态成功时将自己设置为首节点，这个过程非常简单，head执行该节点并断开原首节点的next和当前节点的prev即可，注意在这个过程是不需要使用CAS来保证的，因为只有一个线程能够成功获取到同步状态。过程图如下：Jdk的并发包提供了各种锁及同步机制，其实现的核心类是AbstractQueuedSynchronizer，我们简称为AQS框架，它为不同场景提供了实现锁及同步机制的基本框架，为同步状态的原子性管理、线程的阻塞、线程的解除阻塞及排队管理提供了一种通用的机制。Jdk的并发包（juc）的作者是Doug Lea，但其中思想却是结合了多位大师的智慧，如果你想深入理解juc的相关理论可以参考Doug Lea写的《The_java.util.concurrent_Synchronizer_Framework》论文。从这里可以找到AQS的理论基础，包括框架的基本原理、需求、设计、实现思路、用法及性能，由于这些方面篇幅较大，本文不打算涉及所有方面，主要将针对AQS类的结构及相关操作进行分析。AQS框架它维护了一个volatile int state（代表共享资源）和一个FIFO线程等待队列（多线程争用资源被阻塞时会进入此队列）状态维护AQS用的是一个32位的整型来表示同步状态的，它是用volatile修饰的：1private volatile int state;在互斥锁中它表示着线程是否已经获取了锁，0未获取，1已经获取了，大于1表示重入数。同时AQS提供了getState()、setState()、compareAndSetState()方法来获取和修改该值：可重入锁指的是在一个线程中可以多次获取同一把锁，比如：一个线程在执行一个带锁的方法，该方法中又调用了另一个需要相同锁的方法，则该线程可以直接执行调用的方法，而无需重新获得锁。synchronized也可以看做重入锁所以可重入数大于1表示该线程可能调用了多个需要当前锁的方法，或同一个线程调用了多次lock()方法。队列AQS内部维护着一个FIFO的CLH队列，所以AQS并不支持基于优先级的同步策略。至于为何要选择CLH队列，主要在于CLH锁相对于MSC锁，他更加容易处理cancel和timeout，同时他具备进出队列快、无所、畅通无阻、检查是否有线程在等待也非常容易（head != tail,头尾指针不同）。当然相对于原始的CLH队列锁，AQS采用的是一种变种的CLH队列锁：原始CLH使用的locked自旋，而AQS的CLH则是在每个node里面使用一个状态字段来控制阻塞，而不是自旋。为了可以处理timeout和cancel操作，每个node维护一个指向前驱的指针。如果一个node的前驱被cancel，这个node可以前向移动使用前驱的状态字段。head结点使用的是傀儡结点。AQS定义两种资源共享方式：Exclusive（独占，只有一个线程能执行，如ReentrantLock）和Share（共享，多个线程可同时执行，如Semaphore/CountDownLatch），即我们常说的” 独占锁” 和 “共享锁”。不同的自定义同步器争用共享资源的方式也不同。自定义同步器在实现时只需要实现共享资源state的获取与释放方式即可，至于具体线程等待队列的维护（如获取资源失败入队/唤醒出队等），AQS已经在顶层实现好了。自定义同步器实现时主要实现以下几种方法：isHeldExclusively()：该线程是否正在独占资源。只有用到condition才需要去实现它。tryAcquire(int)：独占方式。尝试获取资源，成功则返回true，失败则返回false。tryRelease(int)：独占方式。尝试释放资源，成功则返回true，失败则返回false。tryAcquireShared(int)：共享方式。尝试获取资源。负数表示失败；0表示成功，但没有剩余可用资源；正数表示成功，且有剩余资源。tryReleaseShared(int)：共享方式。尝试释放资源，成功则返回true，失败则返回false。以ReentrantLock为例，state初始化为0，表示未锁定状态。A线程lock()时，会调用tryAcquire()独占该锁并将state+1。此后，其他线程再tryAcquire()时就会失败，直到A线程unlock()到state=0（即释放锁）为止，其它线程才有机会获取该锁。当然，释放锁之前，A线程自己是可以重复获取此锁的（state会累加），这就是可重入的概念。但要注意，获取多少次就要释放多么次，这样才能保证state是能回到零态的。再以CountDownLatch以例，任务分为N个子线程去执行，state也初始化为N（注意N要与线程个数一致）。这N个子线程是并行执行的，每个子线程执行完后countDown()一次，state会CAS减1。等到所有子线程都执行完后(即state=0)，会unpark()主调用线程，然后主调用线程就会从await()函数返回，继续后余动作。一般来说，自定义同步器要么是独占方法，要么是共享方式，他们也只需实现tryAcquire-tryRelease、tryAcquireShared-tryReleaseShared中的一种即可。但AQS也支持自定义同步器同时实现独占和共享两种方式，如ReentrantReadWriteLock，他是一个读写锁。参考博文【死磕Java并发】—–J.U.C之AQS：CLH同步队列Java并发之AQS详解JAVA中的CAS","categories":[{"name":"多线程","slug":"多线程","permalink":"http://www.fufan.me/categories/多线程/"}],"tags":[{"name":"多线程","slug":"多线程","permalink":"http://www.fufan.me/tags/多线程/"}]},{"title":"Mysql深入学习系列（一）——MyISAM与InnoDB比较","slug":"MyISAM与InnoDB比较（一）","date":"2017-05-15T09:39:00.000Z","updated":"2018-11-05T14:58:09.870Z","comments":true,"path":"2017/05/15/MyISAM与InnoDB比较（一）/","link":"","permalink":"http://www.fufan.me/2017/05/15/MyISAM与InnoDB比较（一）/","excerpt":"","text":"MyISAM是MySQL的默认数据库引擎（5.5版之前），由早期的ISAM（Indexed Sequential Access Method：有索引的顺序访问方法）所改良。虽然性能极佳，但却有一个缺点：不支持事务处理（transaction）。不过，在这几年的发展下，MySQL也导入了InnoDB（另一种数据库引擎），以强化参考完整性与并发违规处理机制，后来就逐渐取代MyISAM。InnoDB，是MySQL的数据库引擎之一，为MySQL AB发布binary的标准之一。InnoDB由Innobase Oy公司所开发，2006年五月时由甲骨文公司并购。与传统的ISAM与MyISAM相比，InnoDB的最大特色就是支持了ACID兼容的事务（Transaction）功能，类似于PostgreSQL。目前InnoDB采用双轨制授权，一是GPL授权，另一是专有软件授权。MyISAM与InnoDB的区别是什么？MyISAM构成上的区别：每个MyISAM在磁盘上存储成三个文件。第一个 文件的名字以表的名字开始，扩展名指出文件类型。.frm文件存储表定义。数据文件的扩 展名为.MYD (MYData)。索引文件的扩 展名是.MYI (MYIndex)。基于磁盘的资源是InnoDB表空间数据文件和它的日志文件，InnoDB 表的 大小只受限于操作系统文件的大小，一般为 2GB事务处理上方面:MyISAM类型的表强调的是性能，其执行数 度比InnoDB类型更快，但是不提供事务支持InnoDB提供事务支持事务，外部键等高级 数据库功能SELECTUPDATEINSERTDelete如果执行大量的SELECT，MyISAM是更好的选择1.如果你的数据执行大量的INSERT或UPDATE，出于性能方面的考虑，应该使用InnoDB表2.DELETE FROM table时，InnoDB不会重新建立表，而是一行一行的 删除。3.LOAD TABLE FROM MASTER操作对InnoDB是不起作用的，解决方法是首先把InnoDB表改成MyISAM表，导入数据后再改成InnoDB表，但是对于使用的额外的InnoDB特性（例如外键）的表不适用对AUTO_INCREMENT的 操作每表一个AUTO_INCREMEN列的内部处理。MyISAM为INSERT和UPDATE操 作自动更新这一列。这使得AUTO_INCREMENT列更快（至少10%）。在序列顶的值被删除之后就不 能再利用。(当AUTO_INCREMENT列被定义为多列索引的最后一列， 可以出现重使用从序列顶部删除的值的情况）。AUTO_INCREMENT值可用ALTER TABLE或myisamch来重置对于AUTO_INCREMENT类型的字段，InnoDB中必须包含只有该字段的索引，但 是在MyISAM表中，可以和其他字段一起建立联 合索引更好和更快的auto_increment处理如果你为一个表指定AUTO_INCREMENT列，在数据词典里的InnoDB表句柄包含一个名为自动增长计数 器的计数器，它被用在为该列赋新值。自动增长计数 器仅被存储在主内存中，而不是存在磁盘上表的具体行数select count() from table,MyISAM只要简单的读出保存好的行数，注意的是，当count()语句包含 where条件时，两种表的操作是一样的InnoDB 中不 保存表的具体行数，也就是说，执行select count(*) from table时，InnoDB要扫描一遍整个表来计算有多少行锁表锁提供行锁(locking on row level)，提供与 Oracle 类型一致的不加锁读取(non-locking read in SELECTs)，另外，InnoDB表的行锁也不是绝对的，如果在执 行一个SQL语句时MySQL不能确定要扫描的范围，InnoDB表同样会锁全表，例如update table set num=1 where name like “%aaa%”存储结构MyISAM：每个MyISAM在磁盘上存储成三个文件。第一个文件的名字以表的名字开始，扩展名指出文件类型。.frm文件存储表定义。数据文件的扩展名为.MYD (MYData)。索引文件的扩展名是.MYI (MYIndex)。InnoDB：所有的表都保存在同一个数据文件中（也可能是多个文件，或者是独立的表空间文件），InnoDB表的大小只受限于操作系统文件的大小，一般为2GB。存储空间MyISAM：可被压缩，存储空间较小。支持三种不同的存储格式：静态表(默认，但是注意数据末尾不能有空格，会被去掉)、动态表、压缩表。InnoDB：需要更多的内存和存储，它会在主内存中建立其专用的缓冲池用于高速缓冲数据和索引。可移植性、备份及恢复MyISAM：数据是以文件的形式存储，所以在跨平台的数据转移中会很方便。在备份和恢复时可单独针对某个表进行操作。InnoDB：免费的方案可以是拷贝数据文件、备份 binlog，或者用 mysqldump，在数据量达到几十G的时候就相对痛苦了。事务支持MyISAM：强调的是性能，每次查询具有原子性,其执行数度比InnoDB类型更快，但是不提供事务支持。InnoDB：提供事务支持事务，外部键等高级数据库功能。 具有事务(commit)、回滚(rollback)和崩溃修复能力(crash recovery capabilities)的事务安全(transaction-safe (ACID compliant))型表。AUTO_INCREMENTMyISAM：可以和其他字段一起建立联合索引。引擎的自动增长列必须是索引，如果是组合索引，自动增长可以不是第一列，他可以根据前面几列进行排序后递增。InnoDB：InnoDB中必须包含只有该字段的索引。引擎的自动增长列必须是索引，如果是组合索引也必须是组合索引的第一列。表锁差异MyISAM：只支持表级锁，用户在操作myisam表时，select，update，delete，insert语句都会给表自动加锁，如果加锁以后的表满足insert并发的情况下，可以在表的尾部插入新的数据。InnoDB：支持事务和行级锁，是innodb的最大特色。行锁大幅度提高了多用户并发操作的新能。但是InnoDB的行锁，只是在WHERE的主键是有效的，非主键的WHERE都会锁全表的。全文索引MyISAM：支持 FULLTEXT类型的全文索引InnoDB：不支持FULLTEXT类型的全文索引，但是innodb可以使用sphinx插件支持全文索引，并且效果更好。表主键MyISAM：允许没有任何索引和主键的表存在，索引都是保存行的地址。InnoDB：如果没有设定主键或者非空唯一索引，就会自动生成一个6字节的主键(用户不可见)，数据是主索引的一部分，附加索引保存的是主索引的值。表的具体行数MyISAM：保存有表的总行数，如果select count() from table;会直接取出出该值。InnoDB：没有保存表的总行数，如果使用select count() from table；就会遍历整个表，消耗相当大，但是在加了wehre条件后，myisam和innodb处理的方式都一样。CURD操作MyISAM：如果执行大量的SELECT，MyISAM是更好的选择。InnoDB：如果你的数据执行大量的INSERT或UPDATE，出于性能方面的考虑，应该使用InnoDB表。DELETE 从性能上InnoDB更优，但DELETE FROM table时，InnoDB不会重新建立表，而是一行一行的删除，在innodb上如果要清空保存有大量数据的表，最好使用truncate table这个命令。外键MyISAM：不支持InnoDB：支持通过上述的分析，基本上可以考虑使用InnoDB来替代MyISAM引擎了，原因是InnoDB自身很多良好的特点，比如事务支持、存储 过程、视图、行级锁定等等，在并发很多的情况下，相信InnoDB的表现肯定要比MyISAM强很多。另外，任何一种表都不是万能的，只用恰当的针对业务类型来选择合适的表类型，才能最大的发挥MySQL的性能优势。如果不是很复杂的Web应用，非关键应用，还是可以继续考虑MyISAM的，这个具体情况可以自己斟酌。总结MyISAM：每个MyISAM在磁盘上存储成三个文件。第一个文件的名字以表的名字开始，扩展名指出文件类型。.frm文件存储表定义。数据文件的扩展名为.MYD (MYData)。MyISAM表格可以被压缩，而且它们支持全文搜索。不支持事务，而且也不支持外键。如果事物回滚将造成不完全回滚，不具有原子性。在进行updata时进行表锁，并发量相对较小。如果执行大量的SELECT，MyISAM是更好的选择。MyISAM的索引和数据是分开的，并且索引是有压缩的，内存使用率就对应提高了不少。能加载更多索引，而Innodb是索引和数据是紧密捆绑的，没有使用压缩从而会造成Innodb比MyISAM体积庞大不小MyISAM缓存在内存的是索引，不是数据。而InnoDB缓存在内存的是数据，相对来说，服务器内存越大，InnoDB发挥的优势越大。优点：查询数据相对较快，适合大量的select，可以全文索引。缺点：不支持事务，不支持外键，并发量较小，不适合大量updateInnoDB这种类型是事务安全的。.它与BDB类型具有相同的特性,它们还支持外键。InnoDB表格速度很快。具有比BDB还丰富的特性,因此如果需要一个事务安全的存储引擎，建议使用它。在update时表进行行锁，并发量相对较大。如果你的数据执行大量的INSERT或UPDATE，出于性能方面的考虑，应该使用InnoDB表。优点：支持事务，支持外键，并发量较大，适合大量update缺点：查询数据相对较快，不适合大量的select对于支持事物的InnoDB类型的表，影响速度的主要原因是AUTOCOMMIT默认设置是打开的，而且程序没有显式调用BEGIN 开始事务，导致每插入一条都自动Commit，严重影响了速度。可以在执行sql前调用begin，多条sql形成一个事物（即使autocommit打开也可以），将大大提高性能。基本的差别为：MyISAM类型不支持事务处理等高级处理，而InnoDB类型支持。MyISAM类型的表强调的是性能，其执行数度比InnoDB类型更快，但是不提供事务支持，而InnoDB提供事务支持已经外部键等高级数据库功能。参考博文MySQL存储引擎中的MyISAM和InnoDB区别详解MySQL存储引擎之Myisam和Innodb总结性梳理","categories":[{"name":"mysql","slug":"mysql","permalink":"http://www.fufan.me/categories/mysql/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://www.fufan.me/tags/mysql/"}]},{"title":"分布式一致性——从CAP到BASE","slug":"分布式一致性——从CAP到BASE","date":"2017-05-11T07:24:00.000Z","updated":"2018-11-05T07:25:25.467Z","comments":true,"path":"2017/05/11/分布式一致性——从CAP到BASE/","link":"","permalink":"http://www.fufan.me/2017/05/11/分布式一致性——从CAP到BASE/","excerpt":"","text":"问题的提出火车站售票假如说我们的终端用户是一位经常坐火车的旅行家，通常他是去车站的售票处购买车 票，然后拿着车票去检票口，再坐上火车，开始一段美好的旅行—-一切似乎都是那么和谐。想象一下，如果他选择的目的地是杭州，而某一趟开往杭州的火车 只剩下最后一张车票，可能在同一时刻，不同售票窗口的另一位乘客也购买了同一张车票。假如说售票系统没有进行一致性的保障，两人都购票成功了。而在检票口 检票的时候，其中一位乘客会被告知他的车票无效—-当然，现代的中国铁路售票系统已经很少出现这样的问题了。但在这个例子中我们可以看出，终端用户对 于系统的需求非常简单：“请售票给我，如果没有余票了，请在售票的时候就告诉我票是无效的”这就对购票系统提出了严格的一致性要求—-系统的数据（本例中指的就是那趟开往杭州的火车的余票数）无论在哪个售票窗口，每时每刻都必须是准确无误的！银行转账假如我们的终端用户是一位刚毕业的大学生，通常在拿到第一个月工资的时候，都会选 择向家里汇款。当他来到银行柜台，完成转账操作后，银行的柜台服务员会友善地提醒他：”您的转账将在N个工作日后到账！”。此时这名毕业生有一定的沮丧， 会对那名柜台服务员叮嘱：”好吧，多久没关系，钱不要少就好了！”—-这也成为了几乎所有用户对于现代银行系统最基本的需求网上购物假如说我们的终端用户是一位网购达人，当他看见一件库存量为5的心仪商品，会迅速地确认购买，写下收货地址，然后下单—-然而，在下单的那个瞬间，系统可能会告知该用户：”库存量不足！”。此时绝大部分消费者都会抱怨自己动作太慢，使得心爱的商品被其他人抢走了。但其实有过网购系统开发经验的工程师一定明白，在商品详情页上显示的那个库存量，通常不是该商品的真实库存量，只有在真正下单购买的时候，系统才会检查该商品的真实库存量。但是，谁在意呢？问题的解读对于上面三个例子，相信大家一定看出来了，我们的终端用户在使用不同的计算机产品时对于数据一致性的需求是不一样的：1、有些系统，既要快速地响应用户，同时还要保证系统的数据对于任意客户端都是真实可靠的，就像火车站售票系统2、有些系统，需要为用户保证绝对可靠的数据安全，虽然在数据一致性上存在延时，但最终务必保证严格的一致性，就像银行的转账系统3、有些系统，虽然向用户展示了一些可以说是”错误”的数据，但是在整个系统使用过程中，一定会在某一个流程上对系统数据进行准确无误的检查，从而避免用户发生不必要的损失，就像网购系统分布式一致性在分布式系统中要解决的一个重要问题就是数据的复制。在我们的日常开发经验中，相 信很多开发人员都遇到过这样的问题：假设客户端C1将系统中的一个值K由V1更新为V2，但客户端C2无法立即读取到K的最新值，需要在一段时间之后才能 读取到。这很正常，因为数据库复制之间存在延时。分布式系统对于数据的复制需求一般都来自于以下两个原因：1、为了增加系统的可用性，以防止单点故障引起的系统不可用2、提高系统的整体性能，通过负载均衡技术，能够让分布在不同地方的数据副本都能够为用户提供服务数据复制在可用性和性能方面给分布式系统带来的巨大好处是不言而喻的，然而数据复制所带来的一致性挑战，也是每一个系统研发人员不得不面对的。所谓分布一致性问题，是指在分布式环境中引入数据复制机制之后，不同数据节点之间 可能出现的，并无法依靠计算机应用程序自身解决的数据不一致的情况。简单讲，数据一致性就是指在对一个副本数据进行更新的时候，必须确保也能够更新其他的 副本，否则不同副本之间的数据将不一致。那么如何解决这个问题？一种思路是”既然是由于延时动作引起的问题，那我可以将写入的动作阻塞，直到数据复制完成后，才完成写入动作”。 没错，这似乎能解决问题，而且有一些系统的架构也确实直接使用了这个思路。但这个思路在解决一致性问题的同时，又带来了新的问题：写入的性能。如果你的应 用场景有非常多的写请求，那么使用这个思路之后，后续的写请求都将会阻塞在前一个请求的写操作上，导致系统整体性能急剧下降。总得来说，我们无法找到一种能够满足分布式系统所有系统属性的分布式一致性解决方案。因此，如何既保证数据的一致性，同时又不影响系统运行的性能，是每一个分布式系统都需要重点考虑和权衡的。于是，一致性级别由此诞生：强一致性这种一致性级别是最符合用户直觉的，它要求系统写入什么，读出来的也会是什么，用户体验好，但实现起来往往对系统的性能影响大弱一致性这种一致性级别约束了系统在写入成功后，不承诺立即可以读到写入的值，也不久承诺多久之后数据能够达到一致，但会尽可能地保证到某个时间级别（比如秒级别）后，数据能够达到一致状态最终一致性最终一致性是弱一致性的一个特例，系统会保证在一定时间内，能够达到一个数据一致的状态。这里之所以将最终一致性单独提出来，是因为它是弱一致性中非常推崇的一种一致性模型，也是业界在大型分布式系统的数据一致性上比较推崇的模型分布式环境的各种问题分布式系统体系结构从其出现之初就伴随着诸多的难题和挑战：通信异常从集中式向分布式演变的过程中，必然引入网络因素，由于网络本身的不可靠性，因此 也引入了额外的问题。分布式系统需要在各个节点之间进行网络通信，因此每次网络通信都会伴随着网络不可用的风险，网络光纤、路由器或是DNS等硬件设备或 是系统不可用都会导致最终分布式系统无法顺利完成一次网络通信。另外，即使分布式系统各个节点之间的网络通信能够正常进行，其延时也会大于单机操作。通常 我们认为现代计算机体系结构中，单机内存访问的延时在纳秒数量级（通常是10ns），而正常的一次网络通信的延迟在0.1~1ms左右（相当于内存访问延 时的105倍），如此巨大的延时差别，也会影响到消息的收发过程，因此消息丢失和消息延迟变得非常普遍网络分区当网络由于发生异常情况，导致分布式系统中部分节点之间的网络延时不断增大，最终导致组成分布式系统的所有节点中，只有部分节点之间能够正常通信，而另一些节点则不能—-我们将这个现象称为网络分区。当网络分区出现时，分布式系统会出现局部小集群，在极端情况下，这些局部小集群会独立完成原本需要整个分布式系统才能完成的功能，包括对数据的事物处理，这就对分布式一致性提出了非常大的挑战三态上面两点，我们已经了解到在分布式环境下，网络可能会出现各式各样的问题，因此分布式系统的每一次请求与响应，存在特有的三态概念，即成功、失败、超时。 在传统的单机系统中，应用程序在调用一个函数之后，能够得到一个非常明确的响应：成功或失败。而在分布式系统中，由于网络是不可靠的，虽然在绝大部分情况 下，网络通信也能够接受到成功或失败的响应，当时当网络出现异常的情况下，就可能会出现超时现象，通常有以下两种情况：（1）由于网络原因，该请求并没有被成功地发送到接收方，而是在发送过程中就发生了消息丢失现象（2）该请求成功地被接收方接收后，进行了处理，但是在将响应反馈给发送方的过程中，发生了消息丢失现象当出现这样的超时现象时，网络通信的发起方是无法确定当前请求是否被成功处理的节点故障节点故障则是分布式环境下另一个比较常见的问题，指的是组成分布式系统的服务器节点出现的宕机或”僵死”现象，通常根据经验来说，每个节点都有可能出现故障，并且每天都在发生分布式事物随着分布式计算的发展，事物在分布式计算领域也得到了广泛的应用。在单机数据库中，我们很容易能够实现一套满足ACID特性的事物处理系统，但在分布式数据库中，数据分散在各台不同的机器上，如何对这些数据进行分布式的事物处理具有非常大的挑战。分布式事物是指事物的参与者、支持事物的服务器、资源服务器以及事物管理器分别位于分布式系统的不同节点上，通常一个分布式事物中会涉及对多个数据源或业务系统的操作。可以设想一个最典型的分布式事物场景：一个跨银行的转账操作涉及调用两个异地的银 行服务，其中一个是本地银行提供的取款服务，另一个则是目标银行提供的存款服务，这两个服务本身是无状态并且相互独立的，共同构成了一个完整的分布式事 物。如果从本地银行取款成功，但是因为某种原因存款服务失败了，那么就必须回滚到取款之前的状态，否则用户可能会发现自己的钱不翼而飞了。从这个例子可以看到，一个分布式事务可以看做是多个分布式的操作序列组成的，例如 上面例子的取款服务和存款服务，通常可以把这一系列分布式的操作序列称为子事物。因此，分布式事务也可以被定义为一种嵌套型的事物，同时也就具有了 ACID事物特性。但由于在分布式事务中，各个子事物的执行是分布式的，因此要实现一种能够保证ACID特性的分布式事物处理系统就显得格外复杂。CAP理论一个经典的分布式系统理论。CAP理论告诉我们：一个分布式系统不可能同时满足一致性（C：Consistency）、可用性（A：Availability）和分区容错性（P：Partition tolerance）这三个基本需求，最多只能同时满足其中两项。1、一致性在分布式环境下，一致性是指数据在多个副本之间能否保持一致的特性。在一致性的需求下，当一个系统在数据一致的状态下执行更新操作后，应该保证系统的数据仍然处于一直的状态。对于一个将数据副本分布在不同分布式节点上的系统来说，如果对第一个节点的数据进 行了更新操作并且更新成功后，却没有使得第二个节点上的数据得到相应的更新，于是在对第二个节点的数据进行读取操作时，获取的依然是老数据（或称为脏数 据），这就是典型的分布式数据不一致的情况。在分布式系统中，如果能够做到针对一个数据项的更新操作执行成功后，所有的用户都可以读取到其最新的值，那么 这样的系统就被认为具有强一致性2、可用性可用性是指系统提供的服务必须一直处于可用的状态，对于用户的每一个操作请求总是能够在有限的时间内返回结果。这里的重点是”有限时间内”和”返回结果”。“有限时间内”是指，对于用户的一个操作请求，系统必须能够在指定的时间内返回对 应的处理结果，如果超过了这个时间范围，那么系统就被认为是不可用的。另外，”有限的时间内”是指系统设计之初就设计好的运行指标，通常不同系统之间有很 大的不同，无论如何，对于用户请求，系统必须存在一个合理的响应时间，否则用户便会对系统感到失望。“返回结果”是可用性的另一个非常重要的指标，它要求系统在完成对用户请求的处理后，返回一个正常的响应结果。正常的响应结果通常能够明确地反映出队请求的处理结果，即成功或失败，而不是一个让用户感到困惑的返回结果。3、分区容错性分区容错性约束了一个分布式系统具有如下特性：分布式系统在遇到任何网络分区故障的时候，仍然需要能够保证对外提供满足一致性和可用性的服务，除非是整个网络环境都发生了故障。网络分区是指在分布式系统中，不同的节点分布在不同的子网络（机房或异地网络） 中，由于一些特殊的原因导致这些子网络出现网络不连通的状况，但各个子网络的内部网络是正常的，从而导致整个系统的网络环境被切分成了若干个孤立的区域。 需要注意的是，组成一个分布式系统的每个节点的加入与退出都可以看作是一个特殊的网络分区。既然一个分布式系统无法同时满足一致性、可用性、分区容错性三个特点，所以我们就需要抛弃一样：选 择说 明CA放弃分区容错性，加强一致性和可用性，其实就是传统的单机数据库的选择AP放弃一致性（这里说的一致性是强一致性），追求分区容错性和可用性，这是很多分布式系统设计时的选择，例如很多NoSQL系统就是如此CP放弃可用性，追求一致性和分区容错性，基本不会选择，网络问题会直接让整个系统不可用需要明确的一点是，对于一个分布式系统而言，分区容错性是一个最基本的要求。因为 既然是一个分布式系统，那么分布式系统中的组件必然需要被部署到不同的节点，否则也就无所谓分布式系统了，因此必然出现子网络。而对于分布式系统而言，网 络问题又是一个必定会出现的异常情况，因此分区容错性也就成为了一个分布式系统必然需要面对和解决的问题。因此系统架构师往往需要把精力花在如何根据业务 特点在C（一致性）和A（可用性）之间寻求平衡。BASE理论BASE是Basically Available（基本可用）、Soft state（软状态）和Eventually consistent（最终一致性）三个短语的缩写。BASE理论是对CAP中一致性和可用性权衡的结果，其来源于对大规模互联网系统分布式实践的总结， 是基于CAP定理逐步演化而来的。BASE理论的核心思想是：即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。接下来看一下BASE中的三要素：基本可用基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性—-注意，这绝不等价于系统不可用。比如：（1）响应时间上的损失。正常情况下，一个在线搜索引擎需要在0.5秒之内返回给用户相应的查询结果，但由于出现故障，查询结果的响应时间增加了1~2秒（2）系统功能上的损失：正常情况下，在一个电子商务网站上进行购物的时候，消费者几乎能够顺利完成每一笔订单，但是在一些节日大促购物高峰的时候，由于消费者的购物行为激增，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面软状态软状态指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时最终一致性最终一致性强调的是所有的数据副本，在经过一段时间的同步之后，最终都能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。总的来说，BASE理论面向的是大型高可用可扩展的分布式系统，和传统的事物ACID特性是相反的，它完全不同于ACID的强一致性模型，而是通过牺牲强一致性来获得可用性，并允许数据在一段时间内是不一致的，但最终达到一致状态。但同时，在实际的分布式场景中，不同业务单元和组件对数据一致性的要求是不同的，因此在具体的分布式系统架构设计过程中，ACID特性和BASE理论往往又会结合在一起。","categories":[{"name":"分布式","slug":"分布式","permalink":"http://www.fufan.me/categories/分布式/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"http://www.fufan.me/tags/分布式/"}]},{"title":"消息队列和rpc的比较","slug":"消息队列和rpc的比较","date":"2017-05-10T06:46:00.000Z","updated":"2018-11-05T06:54:35.357Z","comments":true,"path":"2017/05/10/消息队列和rpc的比较/","link":"","permalink":"http://www.fufan.me/2017/05/10/消息队列和rpc的比较/","excerpt":"","text":"消息队列和RPC远程服务调用是微服务目前使用最多的处理任务和调用服务的两种方式，然而什么场景下适合用哪个是非常关键的。消息队列概述消息队列中间件是分布式系统中重要的组件，主要解决应用解耦，异步消息，流量削锋等问题，实现高性能，高可用，可伸缩和最终一致性架构。目前使用较多的消息队列有ActiveMQ，RabbitMQ，ZeroMQ，Kafka，MetaMQ，RocketMQ，当然也可以用有些缓存中间件来模拟消息队列，如redis。应用场景1. 异步处理场景说明：用户注册后，需要发注册邮件和注册短信。传统的做法有两种 1.串行的方式；2.并行方式a、串行方式：将注册信息写入数据库成功后，发送注册邮件，再发送注册短信。以上三个任务全部完成后，返回给客户端。b、并行方式：将注册信息写入数据库成功后，发送注册邮件的同时，发送注册短信。以上三个任务完成后，返回给客户端。与串行的差别是，并行的方式可以提高处理的时间假设三个业务节点每个使用50毫秒钟，不考虑网络等其他开销，则串行方式的时间是150毫秒，并行的时间可能是100毫秒。因为CPU在单位时间内处理的请求数是一定的，假设CPU1秒内吞吐量是100次。则串行方式1秒内CPU可处理的请求量是7次（1000/150）。并行方式处理的请求量是10次（1000/100）小结：如以上案例描述，传统的方式系统的性能（并发量，吞吐量，响应时间）会有瓶颈。如何解决这个问题呢？引入消息队列，将不是必须的业务逻辑，异步处理。改造后的架构如下：按照以上约定，用户的响应时间相当于是注册信息写入数据库的时间，也就是50毫秒。注册邮件，发送短信写入消息队列后，直接返回，因此写入消息队列的速度很快，基本可以忽略，因此用户的响应时间可能是50毫秒。因此架构改变后，系统的吞吐量提高到每秒20 QPS。比串行提高了3倍，比并行提高了两倍。2. 应用解耦场景说明：用户下单后，订单系统需要通知库存系统。传统的做法是，订单系统调用库存系统的接口。如下图：传统模式的缺点：假如库存系统无法访问，则订单减库存将失败，从而导致订单失败，订单系统与库存系统耦合如何解决以上问题呢？引入应用消息队列后的方案，如下图：订单系统：用户下单后，订单系统完成持久化处理，将消息写入消息队列，返回用户订单下单成功库存系统：订阅下单的消息，采用拉/推的方式，获取下单信息，库存系统根据下单信息，进行库存操作假如：在下单时库存系统不能正常使用。也不影响正常下单，因为下单后，订单系统写入消息队列就不再关心其他的后续操作了。实现订单系统与库存系统的应用解耦3. 流量削锋流量削锋也是消息队列中的常用场景，一般在秒杀或团抢活动中使用广泛。应用场景：秒杀活动，一般会因为流量过大，导致流量暴增，应用挂掉。为解决这个问题，一般需要在应用前端加入消息队列。a、可以控制活动的人数b、可以缓解短时间内高流量压垮应用用户的请求，服务器接收后，首先写入消息队列。假如消息队列长度超过最大数量，则直接抛弃用户请求或跳转到错误页面。秒杀业务根据消息队列中的请求信息，再做后续处理4. 日志处理日志处理是指将消息队列用在日志处理中，比如Kafka的应用，解决大量日志传输的问题。架构简化如下日志采集客户端，负责日志数据采集，定时写受写入Kafka队列Kafka消息队列，负责日志数据的接收，存储和转发日志处理应用：订阅并消费kafka队列中的日志数据5. 消息通讯消息通讯是指，消息队列一般都内置了高效的通信机制，因此也可以用在纯的消息通讯。比如实现点对点消息队列，或者聊天室等点对点通讯：客户端A和客户端B使用同一队列，进行消息通讯。聊天室通讯：客户端A，客户端B，客户端N订阅同一主题，进行消息发布和接收。实现类似聊天室效果。以上实际是消息队列的两种消息模式，点对点或发布订阅模式。模型为示意图，供参考。中间件项目架构实例电商系统消息队列采用高可用，可持久化的消息中间件。比如Active MQ，Rabbit MQ，Rocket Mq。（1）应用将主干逻辑处理完成后，写入消息队列。消息发送是否成功可以开启消息的确认模式。（消息队列返回消息接收成功状态后，应用再返回，这样保障消息的完整性）（2）扩展流程（发短信，配送处理）订阅队列消息。采用推或拉的方式获取消息并处理。（3）消息将应用解耦的同时，带来了数据一致性问题，可以采用最终一致性方式解决。比如主数据写入数据库，扩展应用根据消息队列，并结合数据库方式实现基于消息队列的后续处理。日志收集系统分为Zookeeper注册中心，日志收集客户端，Kafka集群和Storm集群（OtherApp）四部分组成。Zookeeper注册中心，提出负载均衡和地址查找服务日志收集客户端，用于采集应用系统的日志，并将数据推送到kafka队列Kafka集群：接收，路由，存储，转发等消息处理Storm集群：与OtherApp处于同一级别，采用拉的方式消费队列中的数据消息模型讲消息队列就不得不提JMS 。JMS（Java Message Service,Java消息服务）API是一个消息服务的标准/规范，允许应用程序组件基于JavaEE平台创建、发送、接收和读取消息。它使分布式通信耦合度更低，消息服务更加可靠以及异步性。在JMS标准中，有两种消息模型P2P（Point to Point）,Publish/Subscribe(Pub/Sub)。p2p模式P2P模式包含三个角色：消息队列（Queue），发送者(Sender)，接收者(Receiver)。每个消息都被发送到一个特定的队列，接收者从队列中获取消息。队列保留着消息，直到他们被消费或超时。P2P的特点每个消息只有一个消费者（Consumer）(即一旦被消费，消息就不再在消息队列中)发送者和接收者之间在时间上没有依赖性，也就是说当发送者发送了消息之后，不管接收者有没有正在运行，它不会影响到消息被发送到队列接收者在成功接收消息之后需向队列应答成功如果希望发送的每个消息都会被成功处理的话，那么需要P2P模式。Pub/sub模式包含三个角色主题（Topic），发布者（Publisher），订阅者（Subscriber） 多个发布者将消息发送到Topic,系统将这些消息传递给多个订阅者。Pub/Sub的特点每个消息可以有多个消费者发布者和订阅者之间有时间上的依赖性。针对某个主题（Topic）的订阅者，它必须创建一个订阅者之后，才能消费发布者的消息为了消费消息，订阅者必须保持运行的状态为了缓和这样严格的时间相关性，JMS允许订阅者创建一个可持久化的订阅。这样，即使订阅者没有被激活（运行），它也能接收到发布者的消息。如果希望发送的消息可以不被做任何处理、或者只被一个消息者处理、或者可以被多个消费者处理的话，那么可以采用Pub/Sub模型。常用消息队列一般商用的容器，比如WebLogic，JBoss，都支持JMS标准，开发上很方便。但免费的比如Tomcat，Jetty等则需要使用第三方的消息中间件。本部分内容介绍常用的消息中间件（Active MQ,Rabbit MQ，Zero MQ,Kafka）以及他们的特点。ActiveMqActiveMQ 是Apache出品，最流行的，能力强劲的开源消息总线。ActiveMQ 是一个完全支持JMS1.1和J2EE 1.4规范的 JMS Provider实现，尽管JMS规范出台已经是很久的事情了，但是JMS在当今的J2EE应用中间仍然扮演着特殊的地位。ActiveMQ特性如下：⒈ 多种语言和协议编写客户端。语言: Java,C,C++,C#,Ruby,Perl,Python,PHP。应用协议： OpenWire,Stomp REST,WS Notification,XMPP,AMQP⒉ 完全支持JMS1.1和J2EE 1.4规范 （持久化，XA消息，事务)⒊ 对Spring的支持，ActiveMQ可以很容易内嵌到使用Spring的系统里面去，而且也支持Spring2.0的特性⒋ 通过了常见J2EE服务器（如 Geronimo,JBoss 4,GlassFish,WebLogic)的测试，其中通过JCA 1.5 resource adaptors的配置，可以让ActiveMQ可以自动的部署到任何兼容J2EE 1.4 商业服务器上⒌ 支持多种传送协议：in-VM,TCP,SSL,NIO,UDP,JGroups,JXTA⒍ 支持通过JDBC和journal提供高速的消息持久化⒎ 从设计上保证了高性能的集群，客户端-服务器，点对点⒏ 支持Ajax⒐ 支持与Axis的整合⒑ 可以很容易得调用内嵌JMS provider，进行测试KafkaKafka是一种高吞吐量的分布式发布订阅消息系统，它可以处理消费者规模的网站中的所有动作流数据。 这种动作（网页浏览，搜索和其他用户的行动）是在现代网络上的许多社会功能的一个关键因素。 这些数据通常是由于吞吐量的要求而通过处理日志和日志聚合来解决。 对于像Hadoop的一样的日志数据和离线分析系统，但又要求实时处理的限制，这是一个可行的解决方案。Kafka的目的是通过Hadoop的并行加载机制来统一线上和离线的消息处理，也是为了通过集群机来提供实时的消费。Kafka是一种高吞吐量的分布式发布订阅消息系统，有如下特性：通过O(1)的磁盘数据结构提供消息的持久化，这种结构对于即使数以TB的消息存储也能够保持长时间的稳定性能。（文件追加的方式写入数据，过期的数据定期删除）高吞吐量：即使是非常普通的硬件Kafka也可以支持每秒数百万的消息支持通过Kafka服务器和消费机集群来分区消息支持Hadoop并行数据加载Kafka相关概念BrokerKafka集群包含一个或多个服务器，这种服务器被称为broker[5]Topic每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic。（物理上不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个broker上但用户只需指定消息的Topic即可生产或消费数据而不必关心数据存于何处）PartitionParition是物理上的概念，每个Topic包含一个或多个Partition.Producer负责发布消息到Kafka brokerConsumer消息消费者，向Kafka broker读取消息的客户端。Consumer Group每个Consumer属于一个特定的Consumer Group（可为每个Consumer指定group name，若不指定group name则属于默认的group）。一般应用在大数据日志处理或对实时性（少量延迟），可靠性（少量丢数据）要求稍低的场景使用。RPC远程调用一. 区别1. 消息队列能够积压消息,让消费者可以按照自己的节奏处理消息,但是RPC不能.2. 消息队列是一个异步的过程(生产者发送消息之后,不会等待消息的处理),RPC是一个同步的过程.3. 消息队列的生产者不能得知谁消费了消息,消费结果是否成功,而RPC的调用者明确知道被调用者是谁,处理结果也能获取到.4. 由于消息队列在生产者和消费者之间还有一个queue节点,系统性能除了受自身因素影响外还受queue节点影响,而RPC没有中间节点,,系统性能只受自己的影响.二. 适用场景由异同大致就能理解出两者的适用场景是什么:1. 消息队列能够让服务器的负载不会过高,降低了并发度,所以效率受到了影响,又由于消息队列是一个异步的过程,且生产者不能得知消费者的信息,所以消息队列一般用于实时性要求不高的花费时间的操作.2. RPC是一个同步的过程,可能会因为突然高的并发量导致系统出问题,但是RPC具有很高的实时性,所以他一般用户需要立即返回结果的操作.参考博文关于消息队列的使用","categories":[{"name":"消息队列","slug":"消息队列","permalink":"http://www.fufan.me/categories/消息队列/"}],"tags":[{"name":"消息队列","slug":"消息队列","permalink":"http://www.fufan.me/tags/消息队列/"}]},{"title":"java多线程系列（三）——锁","slug":"java多线程系列（三）——锁","date":"2017-05-04T17:58:00.000Z","updated":"2018-11-04T18:00:43.820Z","comments":true,"path":"2017/05/05/java多线程系列（三）——锁/","link":"","permalink":"http://www.fufan.me/2017/05/05/java多线程系列（三）——锁/","excerpt":"","text":"这里整理了Java中的各种锁：公平锁、非公平锁、自旋锁、可重入锁、偏向锁、轻量级锁、重量级锁、读写锁、互斥锁等待。公平锁和非公平锁公平锁是指多个线程在等待同一个锁时，必须按照申请锁的先后顺序来一次获得锁。公平锁的好处是等待锁的线程不会饿死，但是整体效率相对低一些；非公平锁的好处是整体效率相对高一些，但是有些线程可能会饿死或者说很早就在等待锁，但要等很久才会获得锁。其中的原因是公平锁是严格按照请求所的顺序来排队获得锁的，而非公平锁时可以抢占的，即如果在某个时刻有线程需要获取锁，而这个时候刚好锁可用，那么这个线程会直接抢占，而这时阻塞在等待队列的线程则不会被唤醒。new ReentrantLock(true)，用参数来觉得是否为公平锁。自旋锁Java的线程是映射到操作系统的原生线程之上的，如果要阻塞或唤醒一个线程，都需要操作系统来帮忙完成，这就需要从用户态转换到核心态中，因此状态装换需要耗费很多的处理器时间，对于代码简单的同步块（如被synchronized修饰的getter()和setter()方法），状态转换消耗的时间有可能比用户代码执行的时间还要长。自旋等待不能代替阻塞。自旋等待本身虽然避免了线程切换的开销，但它是要占用处理器时间的，因此，如果锁被占用的时间很短，自旋当代的效果就会非常好，反之，如果锁被占用的时间很长，那么自旋的线程只会拜拜浪费处理器资源。因此，自旋等待的时间必须要有一定的限度，如果自旋超过了限定次数（默认是10次，可以使用-XX:PreBlockSpin来更改）没有成功获得锁，就应当使用传统的方式去挂起线程了。自旋锁在JDK1.4.2中引入，使用-XX:+UseSpinning来开启。JDK6中已经变为默认开启，并且引入了自适应的自旋锁。自适应意味着自旋的时间不在固定了，而是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定。自旋是在轻量级锁中使用的，在重量级锁中，线程不使用自旋。如果在同一个锁对象上，自旋等待刚刚成功获得过锁，并且持有锁的线程正在运行中，那么虚拟机就会认为这次自旋也是很有可能再次成功，进而它将允许自旋等待持续相对更长的时间，比如100次循环。另外，如果对于某个锁，自旋很少成功获得过，那在以后要获取这个锁时将可能省略掉自旋过程，以避免浪费处理器资源。可重入锁可重入锁，也叫做递归锁，指的是同一线程外层函数获得锁之后 ，内层递归函数仍然有获取该锁的代码，但不受影响。在JAVA环境下 ReentrantLock 和synchronized 都是可重入锁。可重入锁最大的作用是避免死锁。类锁和对象锁类锁：作用在类上的，两种表现形式，一是静态方法被synchronized修饰，二是通过锁住类名.class的代码块方式1234public static synchronized void method1()&#123;&#125;public void method2()&#123; synchronized(LockStrategy.class)&#123;&#125; &#125;对象锁：普通方法被synchronized修饰，二是通过锁住object的代码块方式12345678910public synchronized void method4()&#123;&#125;public void method5()&#123; synchronized(this)&#123;&#125;&#125;public void method6()&#123; synchronized(object1)&#123;&#125;&#125;偏向锁、轻量级锁、重量级锁synchronized的偏向锁、轻量级锁以及重量级锁是通过Java对象头实现的。博主在Java对象大小内幕浅析中提到了Java对象的内存布局分为：对象头、实例数据和对其填充，而对象头又可以分为”Mark Word”和类型指针klass。”Mark Word”是关键，默认情况下，其存储对象的HashCode、分代年龄和锁标记位。偏向锁是JDK6中引入的一项锁优化，它的目的是消除数据在无竞争情况下的同步原语，进一步提高程序的运行性能。偏向锁会偏向于第一个获得它的线程，如果在接下来的执行过程中，该锁没有被其他的线程获取，则持有偏向锁的线程将永远不需要同步。大多数情况下，锁不仅不存在多线程竞争，而且总是由同一线程多次获得，为了让线程获得锁的代价更低而引入了偏向锁。当锁对象第一次被线程获取的时候，线程使用CAS操作把这个锁的线程ID记录再对象Mark Word之中，同时置偏向标志位1。以后该线程在进入和退出同步块时不需要进行CAS操作来加锁和解锁，只需要简单地测试一下对象头的Mark Word里是否存储着指向当前线程的偏向锁。如果测试成功，表示线程已经获得了锁。如果线程使用CAS操作时失败则表示该锁对象上存在竞争并且这个时候另外一个线程获得偏向锁的所有权。当到达全局安全点（safepoint，这个时间点上没有正在执行的字节码）时获得偏向锁的线程被挂起，膨胀为轻量级锁（涉及Monitor Record，Lock Record相关操作，这里不展开），同时被撤销偏向锁的线程继续往下执行同步代码。当有另外一个线程去尝试获取这个锁时，偏向模式就宣告结束。线程在执行同步块之前，JVM会先在当前线程的栈帧中创建用于存储锁记录(Lock Record)的空间，并将对象头中的Mard Word复制到锁记录中，官方称为Displaced Mark Word。然后线程尝试使用CAS将对象头中的Mark Word替换为指向锁记录的指针。如果成功，当前线程获得锁，如果失败，表示其他线程竞争锁，当前线程便尝试使用自旋来获取锁。如果自旋失败则锁会膨胀成重量级锁。如果自旋成功则依然处于轻量级锁的状态。轻量级锁的解锁过程也是通过CAS操作来进行的，如果对象的Mark Word仍然指向线程的锁记录，那就用CAS操作把对象当前的Mark Word和线程中赋值的Displaced Mark Word替换回来，如果替换成功，整个同步过程就完成了，如果替换失败，就说明有其他线程尝试过获取该锁，那就要在释放锁的同时，唤醒被挂起的线程。轻量级锁提升程序同步性能的依据是：对于绝大部分的锁，在整个同步周期内都是不存在竞争的（区别于偏向锁）。这是一个经验数据。如果没有竞争，轻量级锁使用CAS操作避免了使用互斥量的开销，但如果存在锁竞争，除了互斥量的开销外，还额外发生了CAS操作，因此在有竞争的情况下，轻量级锁比传统的重量级锁更慢。整个synchronized锁流程如下：检测Mark Word里面是不是当前线程的ID，如果是，表示当前线程处于偏向锁如果不是，则使用CAS将当前线程的ID替换Mard Word，如果成功则表示当前线程获得偏向锁，置偏向标志位1如果失败，则说明发生竞争，撤销偏向锁，进而升级为轻量级锁。当前线程使用CAS将对象头的Mark Word替换为锁记录指针，如果成功，当前线程获得锁如果失败，表示其他线程竞争锁，当前线程便尝试使用自旋来获取锁。如果自旋成功则依然处于轻量级状态。如果自旋失败，则升级为重量级锁。悲观锁和乐观锁悲观锁：假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作。乐观锁：假定不会发生并发冲突，只在提交操作时检测是否违反数据完整性。（使用版本号或者时间戳来配合实现）共享锁和排它锁共享锁：如果事务T对数据A加上共享锁后，则其他事务只能对A再加共享锁，不能加排它锁。获准共享锁的事务只能读数据，不能修改数据。排它锁：如果事务T对数据A加上排它锁后，则其他事务不能再对A加任何类型的锁。获得排它锁的事务即能读数据又能修改数据。读写锁读写锁是一个资源能够被多个读线程访问，或者被一个写线程访问但不能同时存在读线程。Java当中的读写锁通过ReentrantReadWriteLock实现。具体使用方法这里不展开。互斥锁所谓互斥锁就是指一次最多只能有一个线程持有的锁。在JDK中synchronized和JUC的Lock就是互斥锁。无锁要保证现场安全，并不是一定就要进行同步，两者没有因果关系。同步只是保证共享数据争用时的正确性的手段，如果一个方法本来就不涉及共享数据，那它自然就无须任何同步措施去保证正确性，因此会有一些代码天生就是线程安全的。无状态编程。无状态代码有一些共同的特征：不依赖于存储在对上的数据和公用的系统资源、用到的状态量都由参数中传入、不调用非无状态的方法等。可以参考Servlet。线程本地存储。可以参考ThreadLocalvolatileCAS协程：在单线程里实现多任务的调度，并在单线程里维持多个任务间的切换","categories":[{"name":"多线程","slug":"多线程","permalink":"http://www.fufan.me/categories/多线程/"}],"tags":[{"name":"多线程","slug":"多线程","permalink":"http://www.fufan.me/tags/多线程/"}]},{"title":"java多线程系列（二）——关键字和方法","slug":"java多线程系列（二）——关键字和方法","date":"2017-05-01T13:47:00.000Z","updated":"2018-11-04T14:45:32.414Z","comments":true,"path":"2017/05/01/java多线程系列（二）——关键字和方法/","link":"","permalink":"http://www.fufan.me/2017/05/01/java多线程系列（二）——关键字和方法/","excerpt":"","text":"关键字synchronized用途synchronized关键字解决的是多个线程之间访问资源的同步性，synchronized关键字可以保证被它修饰的方法或者代码块在任意时刻只能有一个线程执行。另外，在 Java 早期版本中，synchronized属于重量级锁，效率低下，因为监视器锁（monitor）是依赖于底层的操作系统的 Mutex Lock 来实现的，Java 的线程是映射到操作系统的原生线程之上的。如果要挂起或者唤醒一个线程，都需要操作系统帮忙完成，而操作系统实现线程之间的切换时需要从用户态转换到内核态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高，这也是为什么早期的 synchronized 效率低的原因。庆幸的是在 Java 6 之后 Java 官方对从 JVM 层面对synchronized 较大优化，所以现在的 synchronized 锁效率也优化得很不错了。JDK1.6对锁的实现引入了大量的优化，如自旋锁、适应性自旋锁、锁消除、锁粗化、偏向锁、轻量级锁等技术来减少锁操作的开销。使用方式修饰实例方法，作用于当前对象实例加锁，进入同步代码前要获得当前对象实例的锁修饰静态方法，作用于当前类对象加锁，进入同步代码前要获得当前类对象的锁 。也就是给当前类加锁，会作用于类的所有对象实例，因为静态成员不属于任何一个实例对象，是类成员（ static 表明这是该类的一个静态资源，不管new了多少个对象，只有一份，所以对该类的所有对象都加了锁）。所以如果一个线程A调用一个实例对象的非静态 synchronized 方法，而线程B需要调用这个实例对象所属类的静态 synchronized 方法，是允许的，不会发生互斥现象，因为访问静态 synchronized 方法占用的锁是当前类的锁，而访问非静态 synchronized 方法占用的锁是当前实例对象锁。修饰代码块，指定加锁对象，对给定对象加锁，进入同步代码库前要获得给定对象的锁。 和 synchronized 方法一样，synchronized(this)代码块也是锁定当前对象的。synchronized 关键字加到 static 静态方法和 synchronized(class)代码块上都是是给 Class 类上锁。这里再提一下：synchronized关键字加到非 static 静态方法上是给对象实例上锁。另外需要注意的是：尽量不要使用 synchronized(String a) 因为JVM中，字符串常量池具有缓冲功能！如写一个单例类的实现：123456789101112131415161718192021public class Singleton &#123; private volatile static Singleton uniqueInstance; private Singleton() &#123; &#125; public static Singleton getUniqueInstance() &#123; //先判断对象是否已经实例过，没有实例化过才进入加锁代码 if (uniqueInstance == null) &#123; //类对象加锁 synchronized (Singleton.class) &#123; if (uniqueInstance == null) &#123; uniqueInstance = new Singleton(); &#125; &#125; &#125; return uniqueInstance; &#125;&#125;uniqueInstance 采用 volatile 关键字修饰也是很有必要的， uniqueInstance = new Singleton(); 这段代码其实是分为三步执行：为 uniqueInstance 分配内存空间初始化 uniqueInstance将 uniqueInstance 指向分配的内存地址但是由于 JVM 具有指令重排的特性，执行顺序有可能变成 1-&gt;3-&gt;2。指令重排在单线程环境下不会出先问题，但是在多线程环境下会导致一个线程获得还没有初始化的实例。例如，线程 T1 执行了 1 和 3，此时 T2 调用 getUniqueInstance() 后发现 uniqueInstance 不为空，因此返回 uniqueInstance，但此时 uniqueInstance 还未被初始化。使用 volatile 可以禁止 JVM 的指令重排，保证在多线程环境下也能正常运行。synchronized和ReenTrantLock 的区别两者都是可重入锁synchronized 依赖于 JVM 而 ReenTrantLock 依赖于 APIReenTrantLock 比 synchronized 增加了一些高级功能ReenTrantLock提供了一种能够中断等待锁的线程的机制，通过lock.lockInterruptibly()来实现这个机制。也就是说正在等待的线程可以选择放弃等待，改为处理其他事情。ReenTrantLock可以指定是公平锁还是非公平锁。而synchronized只能是非公平锁。所谓的公平锁就是先等待的线程先获得锁。 ReenTrantLock默认情况是非公平的，可以通过 ReenTrantLock类的ReentrantLock(boolean fair)构造方法来制定是否是公平的。synchronized关键字与wait()和notify/notifyAll()方法相结合可以实现等待/通知机制，ReentrantLock类当然也可以实现，但是需要借助于Condition接口与newCondition() 方法。Condition是JDK1.5之后才有的，它具有很好的灵活性，比如可以实现多路通知功能也就是在一个Lock对象中可以创建多个Condition实例（即对象监视器），线程对象可以注册在指定的Condition中，从而可以有选择性的进行线程通知，在调度线程上更加灵活。 在使用notify/notifyAll()方法进行通知时，被通知的线程是由 JVM 选择的，用ReentrantLock类结合Condition实例可以实现“选择性通知” ，这个功能非常重要，而且是Condition接口默认提供的。而synchronized关键字就相当于整个Lock对象中只有一个Condition实例，所有的线程都注册在它一个身上。如果执行notifyAll()方法的话就会通知所有处于等待状态的线程这样会造成很大的效率问题，而Condition实例的signalAll()方法 只会唤醒注册在该Condition实例中的所有等待线程。性能已不是选择标准volatile内存可见性所谓可见性，是指当一条线程修改了共享变量的值，新值对于其他线程来说是可以立即得知的。很显然，上述的例子中是没有办法做到内存可见性的。java虚拟机有自己的内存模型（Java Memory Model，JMM），JMM可以屏蔽掉各种硬件和操作系统的内存访问差异，以实现让java程序在各种平台下都能达到一致的内存访问效果。JMM决定一个线程对共享变量的写入何时对另一个线程可见，JMM定义了线程和主内存之间的抽象关系：共享变量存储在主内存(Main Memory)中，每个线程都有一个私有的本地内存（Local Memory），本地内存保存了被该线程使用到的主内存的副本拷贝，线程对变量的所有操作都必须在工作内存中进行，而不能直接读写主内存中的变量。volatile具备两种特性，第一就是保证共享变量对所有线程的可见性。将一个共享变量声明为volatile后，会有以下效应：当写一个volatile变量时，JMM会把该线程对应的本地内存中的变量强制刷新到主内存中去；这个写会操作会导致其他线程中的缓存无效。不能解决原子性但是需要注意的是，我们一直在拿volatile和synchronized做对比，仅仅是因为这两个关键字在某些内存语义上有共通之处，volatile并不能完全替代synchronized，它依然是个轻量级锁，在很多场景下，volatile并不能胜任。看下这个例子：12345678910111213141516171819202122232425package test;import java.util.concurrent.CountDownLatch;public class Counter &#123; public static volatile int num = 0; //使用CountDownLatch来等待计算线程执行完 static CountDownLatch countDownLatch = new CountDownLatch(30); public static void main(String []args) throws InterruptedException &#123; //开启30个线程进行累加操作 for(int i=0;i&lt;30;i++)&#123; new Thread()&#123; public void run()&#123; for(int j=0;j&lt;10000;j++)&#123; num++;//自加操作 &#125; countDownLatch.countDown(); &#125; &#125;.start(); &#125; //等待计算线程执行完 countDownLatch.await(); System.out.println(num); &#125;&#125;执行结果：1224291如果用volatile修饰的共享变量可以保证可见性，那么结果不应该是300000么?问题就出在num++这个操作上，因为num++不是个原子性的操作，而是个复合操作。我们可以简单讲这个操作理解为由这三步组成:读取加一赋值所以，在多线程环境下，有可能线程A将num读取到本地内存中，此时其他线程可能已经将num增大了很多，线程A依然对过期的num进行自加，重新写到主存中，最终导致了num的结果不合预期，而是小于30000。禁止指令重排序jvm在执行代码块的时候，会通过一些策略对代码顺序进行重排序来优化程序，不过有两个原则：重排序操作不会对存在数据依赖关系的操作进行重排序。比如：a=1;b=a; 这个指令序列，由于第二个操作依赖于第一个操作，所以在编译时和处理器运行时这两个操作不会被重排序。重排序是为了优化性能，但是不管怎么重排序，单线程下程序的执行结果不能被改变比如：a=1;b=2;c=a+b这三个操作，第一步（a=1)和第二步(b=2)由于不存在数据依赖关系，所以可能会发生重排序，但是c=a+b这个操作是不会被重排序的，因为需要保证最终的结果一定是c=a+b=3。重排序在单线程模式下是一定会保证最终结果的正确性，但是在多线程环境下，问题就出来了，volatile就可以解决这个问题。重排序在单线程模式下是一定会保证最终结果的正确性，但是在多线程环境下，问题就出来了，来看个例子，我们对第一个TestVolatile的例子稍稍改进，再增加个共享变量a12345678910111213141516171819202122public class TestVolatile &#123; int a = 1; boolean status = false; /** * 状态切换为true */ public void changeStatus()&#123; a = 2;//1 status = true;//2 &#125; /** * 若状态为true，则running。 */ public void run()&#123; if(status)&#123;//3 int b = a+1;//4 System.out.println(b); &#125; &#125;&#125;假设线程A执行changeStatus后，线程B执行run，我们能保证在4处，b一定等于3么？答案依然是无法保证！也有可能b仍然为2。上面我们提到过，为了提供程序并行度，编译器和处理器可能会对指令进行重排序，而上例中的1和2由于不存在数据依赖关系，则有可能会被重排序，先执行status=true再执行a=2。而此时线程B会顺利到达4处，而线程A中a=2这个操作还未被执行，所以b=a+1的结果也有可能依然等于2。使用volatile关键字修饰共享变量便可以禁止这种重排序。若用volatile修饰共享变量，在编译时，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序volatile禁止指令重排序也有一些规则，简单列举一下：1. 当第二个操作是voaltile写时，无论第一个操作是什么，都不能进行重排序2. 当地一个操作是volatile读时，不管第二个操作是什么，都不能进行重排序3. 当第一个操作是volatile写时，第二个操作是volatile读时，不能进行重排序与synchronized的区别和使用在某些场景下，使用synchronized关键字和volatile是等价的：写入变量值时候不依赖变量的当前值，或者能够保证只有一个线程修改变量值。写入的变量值不依赖其他变量的参与。读取变量值时候不能因为其他原因进行加锁。volatile本质是在告诉jvm当前变量在寄存器中的值是不确定的,需要从主存中读取,synchronized则是锁定当前变量,只有当前线程可以访问该变量,其他线程被阻塞住.volatile仅能使用在变量级别,synchronized则可以使用在变量,方法.volatile仅能实现变量的修改可见性,而synchronized则可以保证变量的修改可见性和原子性.volatile不会造成线程的阻塞,而synchronized可能会造成线程的阻塞.当一个域的值依赖于它之前的值时，volatile就无法工作了，如n=n+1,n++等。如果某个域的值受到其他域的值的限制，那么volatile也无法工作，如Range类的lower和upper边界，必须遵循lower&lt;=upper的限制。总结简单总结下，volatile是一种轻量级的同步机制，它主要有两个特性：一是保证共享变量对所有线程的可见性；二是禁止指令重排序优化。同时需要注意的是，volatile对于单个的共享变量的读/写具有原子性，但是像num++这种复合操作，volatile无法保证其原子性。我们可以通过java本身提供的AtomicInteger来解决该问题，其具体实现看后面的博文。wait、notify和notifyAllwait()的作用是让当前线程进入等待状态，同时，wait()也会让当前线程释放它所持有的锁。“直到其他线程调用此对象的 notify() 方法或 notifyAll() 方法”，当前线程被唤醒(进入“就绪状态”)notify()和notifyAll()的作用，则是唤醒当前对象上的等待线程；notify()是唤醒单个线程，而notifyAll()是唤醒所有的线程。wait(long timeout)让当前线程处于“等待(阻塞)状态”，“直到其他线程调用此对象的notify()方法或 notifyAll() 方法，或者超过指定的时间量”，当前线程被唤醒(进入“就绪状态”)。wait()、notify/notifyAll() 方法是Object的本地final方法，无法被重写。wait()使当前线程阻塞，前提是 必须先获得锁，一般配合synchronized 关键字使用，即，一般在synchronized 同步代码块里使用 wait()、notify/notifyAll() 方法。wait()使当前线程阻塞，前提是 必须先获得锁，一般配合synchronized 关键字使用，即，一般在synchronized 同步代码块里使用 wait()、notify/notifyAll() 方法。当线程执行wait()方法时候，会释放当前的锁，然后让出CPU，进入等待状态。只有当 notify/notifyAll() 被执行时候，才会唤醒一个或多个正处于等待状态的线程，然后继续往下执行，直到执行完synchronized 代码块的代码或是中途遇到wait() ，再次释放锁。notify 和 notifyAll的区别notify方法只唤醒一个等待（对象的）线程并使该线程开始执行。所以如果有多个线程等待一个对象，这个方法只会唤醒其中一个线程，选择哪个线程取决于操作系统对多线程管理的实现。notifyAll 会唤醒所有等待(对象的)线程，尽管哪一个线程将会第一个处理取决于操作系统的实现。如果当前情况下有多个线程需要被唤醒，推荐使用notifyAll 方法。比如在生产者-消费者里面的使用，每次都需要唤醒所有的消费者或是生产者，以判断程序是否可以继续往下执行。","categories":[{"name":"多线程","slug":"多线程","permalink":"http://www.fufan.me/categories/多线程/"}],"tags":[{"name":"多线程","slug":"多线程","permalink":"http://www.fufan.me/tags/多线程/"}]},{"title":"java多线程系列（一）——多线程基础","slug":"java多线程系列（一）——多线程基础","date":"2017-04-30T03:39:00.000Z","updated":"2018-11-05T15:10:11.598Z","comments":true,"path":"2017/04/30/java多线程系列（一）——多线程基础/","link":"","permalink":"http://www.fufan.me/2017/04/30/java多线程系列（一）——多线程基础/","excerpt":"","text":"多线程基础基本概念和术语进程所谓进程就是运行在操作系统的一个任务，进程是计算机任务调度的一个单位，操作系统在启动一个程序的时候，会为其创建一个进程，JVM就是一个进程。进程与进程之间是相互隔离的，每个进程都有独立的内存空间。计算机实现并发的原理是：CPU分时间片，交替执行，宏观并行，微观串行。同理，在进程的基础上分出更小的任务调度单元就是线程，我们所谓的多线程就是一个进程并发多个线程。线程在上面我们提到，一个进程可以并发出多个线程，而线程就是最小的任务执行单元，具体来说，一个程序顺序执行的流程就是一个线程，我们常见的main就是一个线程（主线程）。java多线程每一个Java的应用都至少包含一个线程——主线程。尽管后台也会存在一些其他的线程，例如内存管理，系统管理，信号处理等等，但是从应用来看，主函数是第一个线程，并且我们可以从其中创建多个线程。多线程指的是2个或者更多的线程来在一个程序中并发地执行任务。单处理的电脑只能在同一时间执行一个线程，时间分片是操作系统给不同的进程线程用来共享处理器时间的。线程的优先级java 中的线程优先级的范围是1～10，默认的优先级是5。“高优先级线程”会优先于“低优先级线程”执行用户线程和守护线程所谓守护线程是指在程序运行的时候在后台提供一种通用服务的线程，比如垃圾回收线程就是一个很称职的守护者，并且这种线程并不属于程序中不可或缺的部分。因 此，当所有的非守护线程结束时，程序也就终止了，同时会杀死进程中的所有守护线程。反过来说，只要任何非守护线程还在运行，程序就不会终止。守护线程和用户线程的没啥本质的区别：唯一的不同之处就在于虚拟机的离开：如果用户线程已经全部退出运行了，只剩下守护线程存在了，虚拟机也就退出了。 因为没有了被守护者，守护线程也就没有工作可做了，也就没有继续运行程序的必要了。线程安全线程安全概念：当多个线程访问某一个类（对象或方法）时，这个类始终能表现出正确的行为，那么这个类（对象或方法）就是线程安全的。锁线程安全就是多线程访问时，采用了加锁机制，当一个线程访问该类的某个数据时，进行保护，其他线程不能进行访问直到该线程读取完，其他线程才可使用。不会出现数据不一致或者数据污染。 线程不安全就是不提供数据访问保护，有可能出现多个线程先后更改数据造成所得到的数据是脏数据。这里的加锁机制常见的如：synchronized线程的状态Java中的线程的生命周期大体可分为5种状态。新建(NEW)：新创建了一个线程对象。可运行(RUNNABLE)：线程对象创建后，其他线程(比如main线程）调用了该对象的start()方法。该状态的线程位于可运行线程池中，等待被线程调度选中，获取cpu 的使用权 。运行(RUNNING)：可运行状态(runnable)的线程获得了cpu 时间片（timeslice） ，执行程序代码。阻塞(BLOCKED)：阻塞状态是指线程因为某种原因放弃了cpu 使用权，也即让出了cpu timeslice，暂时停止运行。直到线程进入可运行(runnable)状态，才有机会再次获得cpu timeslice 转到运行(running)状态。阻塞的情况分三种：(一). 等待阻塞：运行(running)的线程执行o.wait()方法，JVM会把该线程放入等待队列(waitting queue)中。(二). 同步阻塞：运行(running)的线程在获取对象的同步锁时，若该同步锁被别的线程占用，则JVM会把该线程放入锁池(lock pool)中。(三). 其他阻塞：运行(running)的线程执行Thread.sleep(long ms)或t.join()方法，或者发出了I/O请求时，JVM会把该线程置为阻塞状态。当sleep()状态超时、join()等待线程终止或者超时、或者I/O处理完毕时，线程重新转入可运行(runnable)状态。死亡(DEAD)：线程run()、main() 方法执行结束，或者因异常退出了run()方法，则该线程结束生命周期。死亡的线程不可再次复生。线程状态切换详解线程的状态图初始状态实现Runnable接口和继承Thread可以得到一个线程类，new一个实例出来，线程就进入了初始状态可运行状态可运行状态只是说你资格运行，调度程序没有挑选到你，你就永远是可运行状态。调用线程的start()方法，此线程进入可运行状态。当前线程sleep()方法结束，其他线程join()结束，等待用户输入完毕，某个线程拿到对象锁，这些线程也将进入可运行状态。当前线程时间片用完了，调用当前线程的yield()方法，当前线程进入可运行状态。锁池里的线程拿到对象锁后，进入可运行状态。运行状态线程调度程序从可运行池中选择一个线程作为当前线程时线程所处的状态。这也是线程进入运行状态的唯一一种方式。死亡状态当线程的run()方法完成时，或者主线程的main()方法完成时，我们就认为它死去。这个线程对象也许是活的，但是，它已经不是一个单独执行的线程。线程一旦死亡，就不能复生。在一个死去的线程上调用start()方法，会抛出java.lang.IllegalThreadStateException异常。阻塞状态当前线程T调用Thread.sleep()方法，当前线程进入阻塞状态。运行在当前线程里的其它线程t2调用join()方法，当前线程进入阻塞状态。等待用户输入的时候，当前线程进入阻塞状态。等待队列调用obj的wait(), notify()方法前，必须获得obj锁，也就是必须写在synchronized(obj) 代码段内。与等待队列相关的步骤和图线程1获取对象A的锁，正在使用对象A。线程1调用对象A的wait()方法。线程1释放对象A的锁，并马上进入等待队列。锁池里面的对象争抢对象A的锁。线程5获得对象A的锁，进入synchronized块，使用对象A。线程5调用对象A的notifyAll()方法，唤醒所有线程，所有线程进入锁池。||||| 线程5调用对象A的notify()方法，唤醒一个线程，不知道会唤醒谁，被唤醒的那个线程进入锁池。notifyAll()方法所在synchronized结束，线程5释放对象A的锁。锁池里面的线程争抢对象锁，但线程1什么时候能抢到就不知道了。||||| 原本锁池+第6步被唤醒的线程一起争抢对象锁。锁池状态当前线程想调用对象A的同步方法时，发现对象A的锁被别的线程占有，此时当前线程进入锁池状态。简言之，锁池里面放的都是想争夺对象锁的线程。当一个线程1被另外一个线程2唤醒时，1线程进入锁池状态，去争夺对象锁。锁池是在同步的环境下才有的概念，一个对象对应一个锁池。apiThread.sleep(long millis)，一定是当前线程调用此方法，当前线程进入阻塞，但不释放对象锁，millis后线程自动苏醒进入可运行状态。作用：给其它线程执行机会的最佳方式。Thread.yield()，一定是当前线程调用此方法，当前线程放弃获取的cpu时间片，由运行状态变会可运行状态，让OS再次选择线程。作用：让相同优先级的线程轮流执行，但并不保证一定会轮流执行。实际中无法保证yield()达到让步目的，因为让步的线程还有可能被线程调度程序再次选中。Thread.yield()不会导致阻塞。t.join()/t.join(long millis)，当前线程里调用其它线程1的join方法，当前线程阻塞，但不释放对象锁，直到线程1执行完毕或者millis时间到，当前线程进入可运行状态。obj.wait()，当前线程调用对象的wait()方法，当前线程释放对象锁，进入等待队列。依靠notify()/notifyAll()唤醒或者wait(long timeout)timeout时间到自动唤醒。obj.notify()唤醒在此对象监视器上等待的单个线程，选择是任意性的。notifyAll()唤醒在此对象监视器上等待的所有线程api部分的内容会在之后的部分详解","categories":[{"name":"多线程","slug":"多线程","permalink":"http://www.fufan.me/categories/多线程/"}],"tags":[{"name":"多线程","slug":"多线程","permalink":"http://www.fufan.me/tags/多线程/"}]},{"title":"java常用集合源码分析之Map（二）","slug":"java常用集合源码分析之Map（二）","date":"2017-04-24T18:48:00.000Z","updated":"2018-11-02T18:59:18.187Z","comments":true,"path":"2017/04/25/java常用集合源码分析之Map（二）/","link":"","permalink":"http://www.fufan.me/2017/04/25/java常用集合源码分析之Map（二）/","excerpt":"","text":"HashMap以下基于 JDK1.7 分析。如图所示，HashMap 底层是基于数组和链表实现的。其中有两个重要的参数：容量负载因子容量的默认大小是 16，负载因子是 0.75，当 HashMap 的 size &gt; 16*0.75 时就会发生扩容(容量和负载因子都可以自由调整)。put 方法首先会将传入的 Key 做 hash 运算计算出 hashcode,然后根据数组长度取模计算出在数组中的 index 下标。由于在计算中位运算比取模运算效率高的多，所以 HashMap 规定数组的长度为 2^n 。这样用 2^n - 1 做位运算与取模效果一致，并且效率还要高出许多。由于数组的长度有限，所以难免会出现不同的 Key 通过运算得到的 index 相同，这种情况可以利用链表来解决，HashMap 会在 table[index]处形成链表，采用头插法将数据插入到链表中。get 方法get 和 put 类似，也是将传入的 Key 计算出 index ，如果该位置上是一个链表就需要遍历整个链表，通过 key.equals(k) 来找到对应的元素。遍历方式12345Iterator&lt;Map.Entry&lt;String, Integer&gt;&gt; entryIterator = map.entrySet().iterator(); while (entryIterator.hasNext()) &#123; Map.Entry&lt;String, Integer&gt; next = entryIterator.next(); System.out.println(\"key=\" + next.getKey() + \" value=\" + next.getValue()); &#125;123456Iterator&lt;String&gt; iterator = map.keySet().iterator(); while (iterator.hasNext())&#123; String key = iterator.next(); System.out.println(\"key=\" + key + \" value=\" + map.get(key)); &#125;123map.forEach((key,value)-&gt;&#123; System.out.println(\"key=\" + key + \" value=\" + value);&#125;);强烈建议使用第一种 EntrySet 进行遍历。第一种可以把 key value 同时取出，第二种还得需要通过 key 取一次 value，效率较低, 第三种需要 JDK1.8 以上，通过外层遍历 table，内层遍历链表或红黑树。notice在并发环境下使用 HashMap 容易出现死循环。并发场景发生扩容，调用 resize() 方法里的 rehash() 时，容易出现环形链表。这样当获取一个不存在的 key 时，计算出的 index 正好是环形链表的下标时就会出现死循环。所以 HashMap 只能在单线程中使用，并且尽量的预设容量，尽可能的减少扩容。在 JDK1.8 中对 HashMap 进行了优化：当 hash 碰撞之后写入链表的长度超过了阈值(默认为8)，链表将会转换为红黑树。假设 hash 冲突非常严重，一个数组后面接了很长的链表，此时重新的时间复杂度就是 O(n) 。如果是红黑树，时间复杂度就是 O(logn) 。大大提高了查询效率。线程不安全表现及原因resize死循环，形成环形链表put、addEntry、resize等方法不同步具体查看 谈谈HashMap线程不安全的体现ConcurrentHashMap由于 HashMap 是一个线程不安全的容器，因此需要支持线程安全的并发容器 ConcurrentHashMap 。JDK1.7 实现数据结构如图所示，是由 Segment 数组、HashEntry 数组组成，和 HashMap 一样，仍然是数组加链表组成。ConcurrentHashMap 采用了分段锁技术，其中 Segment 继承于 ReentrantLock。不会像 HashTable 那样不管是 put 还是 get 操作都需要做同步处理，理论上 ConcurrentHashMap 支持 CurrencyLevel (Segment 数组数量)的线程并发。每当一个线程占用锁访问一个 Segment 时，不会影响到其他的 Segment。get 方法ConcurrentHashMap 的 get 方法是非常高效的，因为整个过程都不需要加锁。只需要将 Key 通过 Hash 之后定位到具体的 Segment ，再通过一次 Hash 定位到具体的元素上。由于 HashEntry 中的 value 属性是用 volatile 关键词修饰的，保证了内存可见性，所以每次获取时都是最新值(volatile 相关知识点)。put 方法内部 HashEntry 类 ：12345678910111213static final class HashEntry&lt;K,V&gt; &#123; final int hash; final K key; volatile V value; volatile HashEntry&lt;K,V&gt; next; HashEntry(int hash, K key, V value, HashEntry&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125;&#125;虽然 HashEntry 中的 value 是用 volatile 关键词修饰的，但是并不能保证并发的原子性，所以 put 操作时仍然需要加锁处理。首先也是通过 Key 的 Hash 定位到具体的 Segment，在 put 之前会进行一次扩容校验。这里比 HashMap 要好的一点是：HashMap 是插入元素之后再看是否需要扩容，有可能扩容之后后续就没有插入就浪费了本次扩容(扩容非常消耗性能)。而 ConcurrentHashMap 不一样，它是在将数据插入之前检查是否需要扩容，之后再做插入操作。size 方法每个 Segment 都有一个 volatile 修饰的全局变量 count ,求整个 ConcurrentHashMap 的 size 时很明显就是将所有的 count 累加即可。但是 volatile 修饰的变量却不能保证多线程的原子性，所有直接累加很容易出现并发问题。但如果每次调用 size 方法将其余的修改操作加锁效率也很低。所以做法是先尝试两次将 count 累加，如果容器的 count 发生了变化再加锁来统计 size。至于 ConcurrentHashMap 是如何知道在统计时大小发生了变化呢，每个 Segment 都有一个 modCount 变量，每当进行一次 put remove 等操作，modCount 将会 +1。只要 modCount 发生了变化就认为容器的大小也在发生变化。JDK1.8 实现1.8 中的 ConcurrentHashMap 数据结构和实现与 1.7 还是有着明显的差异。其中抛弃了原有的 Segment 分段锁，而采用了 CAS + synchronized 来保证并发安全性。1234567891011121314151617181920212223242526272829static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; volatile V val; volatile Node&lt;K,V&gt; next; Node(int hash, K key, V val, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.val = val; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return val; &#125; public final int hashCode() &#123; return key.hashCode() ^ val.hashCode(); &#125; public final String toString()&#123; return key + \"=\" + val; &#125; public final V setValue(V value) &#123; throw new UnsupportedOperationException(); &#125; public final boolean equals(Object o) &#123; Object k, v, u; Map.Entry&lt;?,?&gt; e; return ((o instanceof Map.Entry) &amp;&amp; (k = (e = (Map.Entry&lt;?,?&gt;)o).getKey()) != null &amp;&amp; (v = e.getValue()) != null &amp;&amp; (k == key || k.equals(key)) &amp;&amp; (v == (u = val) || v.equals(u))); &#125;也将 1.7 中存放数据的 HashEntry 改为 Node，但作用都是相同的。其中的 val next 都用了 volatile 修饰，保证了可见性。put 方法重点来看看 put 函数：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263final V putVal(K key, V value, boolean onlyIfAbsent) &#123; if (key == null || value == null) throw new NullPointerException(); int hash = spread(key.hashCode()); int binCount = 0; for (Node&lt;K,V&gt;[] tab = table;;) &#123; Node&lt;K,V&gt; f; int n, i, fh; if (tab == null || (n = tab.length) == 0) tab = initTable(); else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin &#125; else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); else &#123; V oldVal = null; synchronized (f) &#123; if (tabAt(tab, i) == f) &#123; if (fh &gt;= 0) &#123; binCount = 1; for (Node&lt;K,V&gt; e = f;; ++binCount) &#123; K ek; if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; &#125; Node&lt;K,V&gt; pred = e; if ((e = e.next) == null) &#123; pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; &#125; &#125; &#125; else if (f instanceof TreeBin) &#123; Node&lt;K,V&gt; p; binCount = 2; if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) &#123; oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125; &#125; if (binCount != 0) &#123; if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break; &#125; &#125; &#125; addCount(1L, binCount); return null;&#125;根据 key 计算出 hashcode 。判断是否需要进行初始化。f 即为当前 key 定位出的 Node，如果为空表示当前位置可以写入数据，利用 CAS 尝试写入，失败则自旋保证成功。如果当前位置的 hashcode == MOVED == -1,则需要进行扩容。如果都不满足，则利用 synchronized 锁写入数据。如果数量大于 TREEIFY_THRESHOLD 则要转换为红黑树。get 方法12345678910111213141516171819public V get(Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; int h = spread(key.hashCode()); if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) &#123; if ((eh = e.hash) == h) &#123; if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) return e.val; &#125; else if (eh &lt; 0) return (p = e.find(h, key)) != null ? p.val : null; while ((e = e.next) != null) &#123; if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; &#125; &#125; return null;&#125;根据计算出来的 hashcode 寻址，如果就在桶上那么直接返回值。如果是红黑树那就按照树的方式获取值。都不满足那就按照链表的方式遍历获取值。总结1.8 在 1.7 的数据结构上做了大的改动，采用红黑树之后可以保证查询效率（O(logn)），甚至取消了 ReentrantLock 改为了 synchronized，这样可以看出在新版的 JDK 中对 synchronized 优化是很到位的。TreeMap简介TreeMap 是一个有序的key-value集合，它是通过红黑树实现的。TreeMap 继承于AbstractMap，所以它是一个Map，即一个key-value集合。TreeMap 实现了NavigableMap接口，意味着它支持一系列的导航方法。比如返回有序的key集合。TreeMap 实现了Cloneable接口，意味着它能被克隆。TreeMap 实现了java.io.Serializable接口，意味着它支持序列化。TreeMap基于红黑树（Red-Black tree）实现。该映射根据其键的自然顺序进行排序，或者根据创建映射时提供的 Comparator 进行排序，具体取决于使用的构造方法。TreeMap的基本操作 containsKey、get、put 和 remove 的时间复杂度是 log(n) 。另外，TreeMap是非同步的。 它的iterator 方法返回的迭代器是fail-fastl的。构造函数1234567891011// 默认构造函数。使用该构造函数，TreeMap中的元素按照自然排序进行排列。TreeMap()// 创建的TreeMap包含MapTreeMap(Map&lt;? extends K, ? extends V&gt; copyFrom)// 指定Tree的比较器TreeMap(Comparator&lt;? super K&gt; comparator)// 创建的TreeSet包含copyFromTreeMap(SortedMap&lt;K, ? extends V&gt; copyFrom)TreeMap与Map关系如下图：从图中可以看出：TreeMap实现继承于AbstractMap，并且实现了NavigableMap接口。TreeMap的本质是R-B Tree(红黑树)，它包含几个重要的成员变量： root, size, comparator。root 是红黑数的根节点。它是Entry类型，Entry是红黑数的节点，它包含了红黑数的6个基本组成成分：key(键)、value(值)、left(左孩子)、right(右孩子)、parent(父节点)、color(颜色)。Entry节点根据key进行排序，Entry节点包含的内容为value。红黑数排序时，根据Entry中的key进行排序；Entry中的key比较大小是根据比较器comparator来进行判断的。size是红黑数中节点的个数。源码解析可以参看此blog遍历方式遍历TreeMap的键值对，使用iterator遍历TreeMap的键，调用keySet方法遍历TreeMap的值，调用values方法","categories":[{"name":"集合","slug":"集合","permalink":"http://www.fufan.me/categories/集合/"}],"tags":[{"name":"集合","slug":"集合","permalink":"http://www.fufan.me/tags/集合/"},{"name":"map","slug":"map","permalink":"http://www.fufan.me/tags/map/"}]},{"title":"java常用集合源码分析之HashSet（三）","slug":"java常用集合源码分析之HashSet（三）","date":"2017-04-19T18:06:00.000Z","updated":"2018-11-02T18:34:30.164Z","comments":true,"path":"2017/04/20/java常用集合源码分析之HashSet（三）/","link":"","permalink":"http://www.fufan.me/2017/04/20/java常用集合源码分析之HashSet（三）/","excerpt":"","text":"HashSetHashSet 是一个不允许存储重复元素的集合，它的实现比较简单，只要理解了 HashMap，HashSet 就水到渠成了。成员变量首先了解下 HashSet 的成员变量:1234private transient HashMap&lt;E,Object&gt; map;// Dummy value to associate with an Object in the backing Mapprivate static final Object PRESENT = new Object();发现主要就两个变量:map ：用于存放最终数据的。PRESENT ：是所有写入 map 的 value 值。构造函数1234567public HashSet() &#123; map = new HashMap&lt;&gt;();&#125;public HashSet(int initialCapacity, float loadFactor) &#123; map = new HashMap&lt;&gt;(initialCapacity, loadFactor);&#125;构造函数很简单，利用了 HashMap 初始化了 map 。add123public boolean add(E e) &#123; return map.put(e, PRESENT)==null;&#125;比较关键的就是这个 add() 方法。可以看出它是将存放的对象当做了 HashMap 的健，value 都是相同的 PRESENT 。由于 HashMap 的 key 是不能重复的，所以每当有重复的值写入到 HashSet 时，value 会被覆盖，但 key 不会受到影响，这样就保证了 HashSet 中只能存放不重复的元素。遍历方式几种方式（iterator，for循环，lambda）jdk1.8 lambda123set.stream.foreach((v) -&gt; &#123; System.out.println(v);&#125;)总结HashSet 的原理比较简单，几乎全部借助于 HashMap 来实现的。所以 HashMap 会出现的问题 HashSet 依然不能避免。","categories":[],"tags":[{"name":"集合","slug":"集合","permalink":"http://www.fufan.me/tags/集合/"},{"name":"set","slug":"set","permalink":"http://www.fufan.me/tags/set/"}]},{"title":"java常用集合源码分析之List（一）","slug":"java常用集合源码分析之List（一）","date":"2017-04-09T16:44:00.000Z","updated":"2018-11-05T03:05:36.775Z","comments":true,"path":"2017/04/10/java常用集合源码分析之List（一）/","link":"","permalink":"http://www.fufan.me/2017/04/10/java常用集合源码分析之List（一）/","excerpt":"","text":"ArrayListArrayList 实现于 List、RandomAccess 接口。可以插入空数据，也支持随机访问。ArrayList默认初始化长度为10个1234/** * Default initial capacity. */private static final int DEFAULT_CAPACITY = 10;ArrayList相当于动态数据，其中最重要的两个属性分别是:elementData 数组，以及 size 大小。在调用 add() 方法的时候：12345public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;&#125;首先进行扩容校验。将插入的值放到尾部，并将 size + 1 。如果是调用 add(index,e) 在指定位置添加的话：12345678910public void add(int index, E element) &#123; rangeCheckForAdd(index); ensureCapacityInternal(size + 1); // Increments modCount!! //复制，向后移动 System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++;&#125;也是首先扩容校验。接着对数据进行复制，目的是把 index 位置空出来放本次插入的数据，并将后面的数据向后移动一个位置。其实扩容最终调用的代码:1234567891011private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity);&#125;也是一个数组复制的过程。由此可见 ArrayList 的主要消耗是数组扩容以及在指定位置添加数据，在日常使用时最好是指定大小，尽量减少扩容。更要减少在指定位置插入数据的操作。序列化由于 ArrayList 是基于动态数组实现的，所以并不是所有的空间都被使用。因此使用了 transient 修饰，可以防止被自动序列化。1transient Object[] elementData;因此 ArrayList 自定义了序列化与反序列化：1234567891011121314151617181920212223242526272829303132333435363738394041private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException&#123; // Write out element count, and any hidden stuff int expectedModCount = modCount; s.defaultWriteObject(); // Write out size as capacity for behavioural compatibility with clone() s.writeInt(size); // Write out all elements in the proper order. //只序列化了被使用的数据 for (int i=0; i&lt;size; i++) &#123; s.writeObject(elementData[i]); &#125; if (modCount != expectedModCount) &#123; throw new ConcurrentModificationException(); &#125;&#125;private void readObject(java.io.ObjectInputStream s) throws java.io.IOException, ClassNotFoundException &#123; elementData = EMPTY_ELEMENTDATA; // Read in size, and any hidden stuff s.defaultReadObject(); // Read in capacity s.readInt(); // ignored if (size &gt; 0) &#123; // be like clone(), allocate array based upon size not capacity ensureCapacityInternal(size); Object[] a = elementData; // Read in all elements in the proper order. for (int i=0; i&lt;size; i++) &#123; a[i] = s.readObject(); &#125; &#125;&#125;当对象中自定义了 writeObject 和 readObject 方法时，JVM 会调用这两个自定义方法来实现序列化与反序列化。从实现中可以看出 ArrayList 只序列化了被使用的数据。遍历方式ArrayList支持3种遍历方式通过迭代器遍历：12345Iterator iter = list.iterator();while (iter.hasNext())&#123; System.out.println(iter.next());&#125;随机访问，通过索引值去遍历，由于ArrayList实现了RandomAccess接口12345int size = list.size();for (int i=0; i&lt;size; i++) &#123; System.out.println(list.get(i)); &#125;for循环遍历1234for(String str:list) &#123; System.out.println(str); &#125;lambda123list.stream.foreach((v) -&gt; &#123; System.out.println(v);&#125;)VectorVector 也是实现于 List 接口，底层数据结构和 ArrayList 类似,也是一个动态数组存放数据。不过是在 add() 方法的时候使用 synchronized 进行同步写数据，但是开销较大，所以 Vector 是一个同步容器并不是一个并发容器。以下是 add() 方法：123456public synchronized boolean add(E e) &#123; modCount++; ensureCapacityHelper(elementCount + 1); elementData[elementCount++] = e; return true;&#125;以及指定位置插入数据:1234567891011121314public void add(int index, E element) &#123; insertElementAt(element, index);&#125;public synchronized void insertElementAt(E obj, int index) &#123; modCount++; if (index &gt; elementCount) &#123; throw new ArrayIndexOutOfBoundsException(index + \" &gt; \" + elementCount); &#125; ensureCapacityHelper(elementCount + 1); System.arraycopy(elementData, index, elementData, index + 1, elementCount - index); elementData[index] = obj; elementCount++;&#125;遍历方式和ArrayList相比, 多了一种Enumeration遍历12345Integer value = null;Enumeration enu = vec.elements();while (enu.hasMoreElements()) &#123; value = (Integer)enu.nextElement();&#125;LinkedList如图所示 LinkedList 底层是基于双向链表实现的，也是实现了 List 接口，所以也拥有 List 的一些特点(JDK1.7/8 之后取消了循环，修改为双向链表)。新增方法123456789101112131415161718public boolean add(E e) &#123; linkLast(e); return true;&#125; /** * Links e as last element. */void linkLast(E e) &#123; final Node&lt;E&gt; l = last; final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null); last = newNode; if (l == null) first = newNode; else l.next = newNode; size++; modCount++;&#125;可见每次插入都是移动指针，和 ArrayList 的拷贝数组来说效率要高上不少。查询方法1234567891011121314151617181920public E get(int index) &#123; checkElementIndex(index); return node(index).item;&#125;Node&lt;E&gt; node(int index) &#123; // assert isElementIndex(index); if (index &lt; (size &gt;&gt; 1)) &#123; Node&lt;E&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; &#125; else &#123; Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; &#125;&#125;上述代码，利用了双向链表的特性，如果index离链表头比较近，就从节点头部遍历。否则就从节点尾部开始遍历。使用空间（双向链表）来换取时间。node()会以O(n/2)的性能去获取一个结点如果索引值大于链表大小的一半，那么将从尾结点开始遍历这样的效率是非常低的，特别是当 index 越接近 size 的中间值时。总结：LinkedList 插入，删除都是移动指针效率很高。查找需要进行遍历查询，效率较低。遍历方式除了arraylist的四种遍历方式以外，还可以通过removeFirst()和removeLast()的方式：1234567891011//removeFirst()try &#123; while(list.removeFirst() != null) ; &#125; catch (NoSuchElementException e) &#123; &#125; //removeLast()try &#123; while(list.removeLast() != null) ; &#125; catch (NoSuchElementException e) &#123;这种方式是效率最高的，不过他会删除原始数据。随机遍历是效率最低的，推荐用for循环的方式。后面我也会拿出小节专门来分析集合的遍历效率","categories":[{"name":"集合","slug":"集合","permalink":"http://www.fufan.me/categories/集合/"}],"tags":[{"name":"集合","slug":"集合","permalink":"http://www.fufan.me/tags/集合/"},{"name":"list","slug":"list","permalink":"http://www.fufan.me/tags/list/"}]},{"title":"OSI和TCP/IP的模型简述","slug":"OSI和TCP-IP的7层模型简述","date":"2017-04-02T12:40:00.000Z","updated":"2018-11-02T13:37:34.149Z","comments":true,"path":"2017/04/02/OSI和TCP-IP的7层模型简述/","link":"","permalink":"http://www.fufan.me/2017/04/02/OSI和TCP-IP的7层模型简述/","excerpt":"","text":"体系结构概述先盗取盗图一张^_^, OSI的七层体系结构概念清楚，理论也很完整，但是它比较复杂而且不实用，总图如下：但是由于7层模型过于复杂，大部分公司和国家支持更简单的5层模型五层协议各层详述应用层（application layer）应用层的任务是通过应用进程间的交互来完成特定网络应用。应用层协议定义的是应用进程（进程：主机中正在运行的程序）间的通信和交互的规则。对于不同的网络应用需要不同的应用层协议。在互联网中应用层协议很多，如域名系统DNS，支持万维网应用的HTTP协议，支持电子邮件的SMTP协议等等。我们把应用层交互的数据单元称为报文。域名系统（Domain Name System缩写DNS，Domain Name被译为域名）域名系统是因特网的一项核心服务，它作为可以将域名和IP地址相互映射的一个分布式数据库，能够使人更方便的访问互联网，而不用去记住能够被机器直接读取的IP数串。（百度百科）例如：一个公司的Web网站可看作是它在网上的门户，而域名就相当于其门牌地址，通常域名都使用该公司的名称或简称。例如上面提到的微软公司的域名，类似的还有：IBM公司的域名是www.ibm.com、Oracle公司的域名是www.oracle.com、Cisco公司的域名是www.cisco.com等。HTTP协议超文本传输协议（HTTP，HyperText Transfer Protocol)是互联网上应用最为广泛的一种网络协议。所有的WWW文件都必须遵守这个标准。设计HTTP最初的目的是为了提供一种发布和接收HTML页面的方法。（百度百科）运输层（transport layer）运输层的主要任务就是负责向两台主机进程之间的通信提供通用的数据传输服务。应用进程利用该服务传送应用层报文。“通用的”是指并不针对某一个特定的网络应用，而是多种应用可以使用同一个运输层服务。由于一台主机可同时运行多个线程，因此运输层有复用和分用的功能。所谓复用就是指多个应用层进程可同时使用下面运输层的服务，分用和复用相反，是运输层把收到的信息分别交付上面应用层中的相应进程。运输层主要使用以下两种协议：传输控制协议TCP（Transmisson Control Protocol）–提供面向连接的，可靠的数据传输服务。用户数据协议UDP（User Datagram Protocol）–提供无连接的，尽最大努力的数据传输服务（不保证数据传输的可靠性）。UDP的主要特点：UDP是无连接的；UDP使用尽最大努力交付，即不保证可靠交付，因此主机不需要维持复杂的链接状态（这里面有许多参数）；UDP是面向报文的；UDP没有拥塞控制，因此网络出现拥塞不会使源主机的发送速率降低（对实时应用很有用，如IP电话，实时视频会议等）；UDP支持一对一、一对多、多对一和多对多的交互通信；UDP的首部开销小，只有8个字节，比TCP的20个字节的首部要短。TCP的主要特点：TCP是面向连接的。（就好像打电话一样，通话前需要先拨号建立连接，通话结束后要挂机释放连接）；每一条TCP连接只能有两个端点，每一条TCP连接只能是点对点的（一对一）；TCP提供可靠交付的服务。通过TCP连接传送的数据，无差错、不丢失、不重复、并且按序到达；TCP提供全双工通信。TCP允许通信双方的应用进程在任何时候都能发送数据。TCP连接的两端都设有发送缓存和接收缓存，用来临时存放双方通信的数据；面向字节流。TCP中的“流”（stream）指的是流入进程或从进程流出的字节序列。“面向字节流”的含义是：虽然应用程序和TCP的交互是一次一个数据块（大小不等），但TCP把应用程序交下来的数据仅仅看成是一连串的无结构的字节流。网络层（network layer）网络层负责为分组交换网上的不同主机提供通信服务。在发送数据时，网络层把运输层产生的报文段或用户数据报封装成分组和包进行传送。在TCP/IP体系结构中，由于网络层使用IP协议，因此分组也叫IP数据报，简称数据报。这里要注意：不要把运输层的“用户数据报UDP”和网络层的“IP数据报”弄混。另外，无论是哪一层的数据单元，都可笼统地用“分组”来表示。网络层的另一个任务就是选择合适的路由，使源主机运输层所传下来的分株，能通过网络层中的路由器找到目的主机。这里强调指出，网络层中的“网络”二字已经不是我们通常谈到的具体网络，而是指计算机网络体系结构模型中第三层的名称.互联网是由大量的异构（heterogeneous）网络通过路由器（router）相互连接起来的。互联网使用的网络层协议是无连接的网际协议（Intert Prococol）和许多路由选择协议，因此互联网的网络层也叫做网际层或IP层。数据链路层（data link layer）数据链路层通常简称为链路层。两台主机之间的数据传输，总是在一段一段的链路上传送的，这就需要使用专门的链路层的协议。 在两个相邻节点之间传送数据时，数据链路层将网络层交下来的IP数据报组装程帧，在两个相邻节点间的链路上传送帧。每一帧包括数据和必要的控制信息（如同步信息，地址信息，差错控制等）。在 接收数据时，控制信息使接收端能够知道一个帧从哪个比特开始和到哪个比特结束。这样，数据链路层在收到一个帧后，就可从中提出数据部分，上交给网络层。控制信息还使接收端能够检测到所收到的帧中有误差错。如果发现差错，数据链路层就简单地丢弃这个出了差错的帧，以避免继续在网络中传送下去白白浪费网络资源。如果需要改正数据在链路层传输时出现差错（这就是说，数据链路层不仅要检错，而且还要纠错），那么就要采用可靠性传输协议来纠正出现的差错。这种方法会使链路层的协议复杂些。物理层（physical layer）在物理层上所传送的数据单位是比特。物理层的作用是实现相邻计算机节点之间比特流的透明传送，尽可能屏蔽掉具体传输介质和物理设备的差异。使其上面的数据链路层不必考虑网络的具体传输介质是什么。“透明传送比特流”表示经实际电路传送后的比特流没有发生变化，对传送的比特流来说，这个电路好像是看不见的。","categories":[{"name":"网络","slug":"网络","permalink":"http://www.fufan.me/categories/网络/"}],"tags":[{"name":"网络","slug":"网络","permalink":"http://www.fufan.me/tags/网络/"}]},{"title":"java常用集合源码分析之绪论","slug":"java常用集合源码分析之绪论","date":"2017-04-01T02:52:00.000Z","updated":"2018-11-05T03:06:23.200Z","comments":true,"path":"2017/04/01/java常用集合源码分析之绪论/","link":"","permalink":"http://www.fufan.me/2017/04/01/java常用集合源码分析之绪论/","excerpt":"","text":"面向对象语言对事物的体现都是以对象的形式，所以为了方便对多个对象的操作，就对对象进行存储，集合就是存储对象最常用的一种方式。数组虽然也可以存储对象，但长度是固定的；集合长度是可变的，数组中可以存储基本数据类型，集合只能存储对象。集合类的特点：集合只用于存储对象，集合长度是可变的，集合可以存储不同类型的对象。java集合继承关系图先来看两张图：上述类图中，实线边框的是实现类，比如ArrayList，LinkedList，HashMap等，折线边框的是抽象类，比如AbstractCollection，AbstractList，AbstractMap等，而点线边框的是接口，比如Collection，Iterator，List等。1、Iterator接口Iterator接口，这是一个用于遍历集合中元素的接口，主要包含hashNext(),next(),remove()三种方法。它的一个子接口LinkedIterator在它的基础上又添加了三种方法，分别是add(),previous(),hasPrevious()。也就是说如果是先Iterator接口，那么在遍历集合中元素的时候，只能往后遍历，被遍历后的元素不会在遍历到，通常无序集合实现的都是这个接口，比如HashSet，HashMap；而那些元素有序的集合，实现的一般都是LinkedIterator接口，实现这个接口的集合可以双向遍历，既可以通过next()访问下一个元素，又可以通过previous()访问前一个元素，比如ArrayList。抽象类的使用。如果要自己实现一个集合类，去实现那些抽象的接口会非常麻烦，工作量很大。这个时候就可以使用抽象类，这些抽象类中给我们提供了许多现成的实现，我们只需要根据自己的需求重写一些方法或者添加一些方法就可以实现自己需要的集合类，工作流昂大大降低。2、Collection （集合的最大接口）继承关系——List 可以存放重复的内容——Set 不能存放重复的内容，所以的重复内容靠hashCode()和equals()两个方法区分——Queue 队列接口——SortedSet 可以对集合中的数据进行排序Collection定义了集合框架的共性功能。add方法的参数类型是Object。以便于接收任意类型对象。集合中存储的都是对象的引用(地址)。3、List的常用子类特有方法。凡是可以操作角标的方法都是该体系特有的方法。——ArrayList 线程不安全，查询速度快——Vector 线程安全，但速度慢，已被ArrayList替代——LinkedList 链表结果，增删速度快4、Set接口Set：元素是无序(存入和取出的顺序不一定一致)，元素不可以重复。——HashSet:底层数据结构是哈希表。是线程不安全的。不同步。HashSet是如何保证元素唯一性的呢？是通过元素的两个方法，hashCode和equals来完成。如果元素的HashCode值相同，才会判断equals是否为true。如果元素的hashcode值不同，不会调用equals。注意,对于判断元素是否存在，以及删除等操作，依赖的方法是元素的hashcode和equals方法。——TreeSet：有序的存放：TreeSet线程不安全，可以对Set集合中的元素进行排序; 通过compareTo或者compare方法来保证元素的唯一性，元素以二叉树的形式存放。5、Object类在实际开发中经常会碰到区分同一对象的问题，一个完整的类最好覆写Object类的hashCode()、equals()、toString()三个方法。6、集合的输出5种常见的输出方式——Iterator： 迭代输出，使用最多的输出方式——ListIterator： Iterator的子接口，专门用于输出List中的内容——Enumeration——foreach——lambda在迭代时，不可以通过集合对象的方法操作集合中的元素，因为会发生ConcurrentModificationException异常。所以，在迭代器时，只能用迭代器的放过操作元素，可是Iterator方法是有限的，只能对元素进行判断，取出，删除的操作，如果想要其他的操作如添加，修改等，就需要使用其子接口，ListIterator。该接口只能通过List集合的listIterator方法获取。7、Map接口Correction、Set、List接口都属于单值的操作，而Map中的每个元素都使用key——&gt;value的形式存储在集合中。Map集合：该集合存储键值对。一对一对往里存。而且要保证键的唯一性。8、Map接口的常用子类Map——HashMap：底层是哈希表数据结构，允许使用 null 值和 null 键，该集合是不同步的。将hashtable替代，jdk1.2.效率高。——TreeMap：底层是二叉树数据结构。线程不同步。可以用于给map集合中的键进行排序。9、集合工具类Collections:集合框架的工具类。里面定义的都是静态方法。Collections和Collection有什么区别？Collection是集合框架中的一个顶层接口，它里面定义了单列集合的共性方法。它有两个常用的子接口，——List：对元素都有定义索引。有序的。可以重复元素。——Set：不可以重复元素。无序。Collections是集合框架中的一个工具类。该类中的方法都是静态的。提供的方法中有可以对list集合进行排序，二分查找等方法。通常常用的集合都是线程不安全的。因为要提高效率。如果多线程操作这些集合时，可以通过该工具类中的同步方法，将线程不安全的集合，转换成安全的。10、比较11、总结List：add/remove/get/set。ArrayList：其实就是数组，容量一大，频繁增删就是噩梦，适合随机查找；LinkedList：增加了push/[pop|remove|pull]，其实都是removeFirst；Vector：历史遗留产物，同步版的ArrayList，代码和ArrayList太像；Stack：继承自Vector。Java里其实没有纯粹的Stack，可以自己实现，用组合的方式，封装一下LinkedList即可；Queue：本来是单独的一类，不过在SUN的JDK里就是用LinkedList来提供这个功能的，主要方法是offer/pull/peek，因此归到这里呢。Set：add/remove。可以用迭代器或者转换成list。HashSet：内部采用HashMap实现的；LinkedHashSet：采用LinkedHashMap实现；TreeSet：TreeMap。Map：put/get/remove。HashMap/HashTable：散列表，和ArrayList一样采用数组实现，超过初始容量会对性能有损耗；LinkedHashMap：继承自HashMap，但通过重写嵌套类HashMap.Entry实现了链表结构，同样有容量的问题；Properties：是继承的HashTable。","categories":[],"tags":[{"name":"集合","slug":"集合","permalink":"http://www.fufan.me/tags/集合/"}]},{"title":"Java面试总结积累（基础篇）之集合问题","slug":"Java面试总结积累（基础篇）之集合问题","date":"2017-01-12T16:35:00.000Z","updated":"2018-11-05T03:12:48.337Z","comments":true,"path":"2017/01/13/Java面试总结积累（基础篇）之集合问题/","link":"","permalink":"http://www.fufan.me/2017/01/13/Java面试总结积累（基础篇）之集合问题/","excerpt":"","text":"这里通过收集网上一些比较好的博客对集合的总结做一下记录和积累。List, Set, Map三者的区别及总结List：对付顺序的好帮手List接口存储一组不唯一（可以有多个元素引用相同的对象），有序的对象Set:注重独一无二的性质不允许重复的集合。不会有多个元素引用相同的对象。Map:用Key来搜索的专家使用键值对存储。Map会维护与Key有关联的值。两个Key可以引用相同的对象，但Key不能重复，典型的Key是String类型，但也可以是任何对象。Arraylist 与 LinkedList 区别Arraylist底层使用的是数组（存读数据效率高，插入删除特定位置效率低），LinkedList底层使用的是双向循环链表数据结构（插入，删除效率特别高）。学过数据结构这门课后我们就知道采用链表存储，插入，删除元素时间复杂度不受元素位置的影响，都是近似O（1）而数组为近似O（n），因此当数据特别多，而且经常需要插入删除元素时建议选用LinkedList.一般程序只用Arraylist就够用了，因为一般数据量都不会蛮大，Arraylist是使用最多的集合类。ArrayList 与 Vector 区别（为什么要用Arraylist取代Vector呢？）Vector类的所有方法都是同步的。可以由两个线程安全地访问一个Vector对象、但是一个线程访问Vector ，代码要在同步操作上耗费大量的时间。Arraylist不是同步的，所以在不需要同步时建议使用Arraylist。HashMap 和 Hashtable 的区别HashMap是非线程安全的，HashTable是线程安全的；HashTable内部的方法基本都经过synchronized修饰。因为线程安全的问题，HashMap要比HashTable效率高一点，HashTable基本被淘汰。HashMap允许有null值的存在，而在HashTable中put进的键值只要有一个null，直接抛出NullPointerException。TIPS: Hashtable和HashMap有几个主要的不同：线程安全以及速度。仅在你需要完全的线程安全的时候使用Hashtable，而如果你使用Java5或以上的话，请使用ConcurrentHashMap吧HashMap 和 ConcurrentHashMap 的区别ConcurrentHashMap对整个桶数组进行了分割分段(Segment)，然后在每一个分段上都用lock锁进行保护，相对于HashTable的synchronized锁的粒度更精细了一些，并发性能更好，而HashMap没有锁机制，不是线程安全的。（JDK1.8之后ConcurrentHashMap启用了一种全新的方式实现,利用CAS算法。）HashMap的键值对允许有null，但是ConCurrentHashMap都不允许。HashSet如何检查重复当你把对象加入HashSet时，HashSet会先计算对象的hashcode值来判断对象加入的位置，同时也会与其他加入的对象的hashcode值作比较，如果没有相符的hashcode，HashSet会假设对象没有重复出现。但是如果发现有相同hashcode值的对象，这时会调用equals（）方法来检查hashcode相等的对象是否真的相同。如果两者相同，HashSet就不会让加入操作成功。==与equals的区别如果两个对象相等，则hashcode一定也是相同的两个对象相等,对两个equals方法返回true两个对象有相同的hashcode值，它们也不一定是相等的综上，equals方法被覆盖过，则hashCode方法也必须被覆盖hashCode()的默认行为是对堆上的对象产生独特值。如果没有重写hashCode()，则该class的两个对象无论如何都不会相等（即使这两个对象指向相同的数据）。==是判断两个变量或实例是不是指向同一个内存空间 equals是判断两个变量或实例所指向的内存空间的值是不是相同==是指对内存地址进行比较 equals()是对字符串的内容进行比较3.==指引用是否相同 equals()指的是值是否相同comparable 和 comparator的区别？comparable接口实际上是出自java.lang包 它有一个 compareTo(Object obj)方法用来排序comparator接口实际上是出自 java.util 包它有一个compare(Object obj1, Object obj2)方法用来排序一般我们需要对一个集合使用自定义排序时，我们就要重写compareTo方法或compare方法，当我们需要对某一个集合实现两种排序方式，比如一个song对象中的歌名和歌手名分别采用一种排序方法的话，我们可以重写compareTo方法和使用自制的Comparator方法或者以两个Comparator来实现歌名排序和歌星名排序，第二种代表我们只能使用两个参数版的Collections.sort().如何对Object的list排序？对objects数组进行排序，我们可以用Arrays.sort()方法对objects的集合进行排序，需要使用Collections.sort()方法如何实现数组与List的相互转换？List转数组:toArray(arraylist.size()方法数组转List:Arrays的asList(a)方法1234567891011121314151617181920212223List&lt;String&gt; arrayList = new ArrayList&lt;String&gt;(); arrayList.add(&quot;s&quot;); arrayList.add(&quot;e&quot;); arrayList.add(&quot;n&quot;); /** * ArrayList转数组 */ int size=arrayList.size(); String[] a = arrayList.toArray(new String[size]); //输出第二个元素 System.out.println(a[1]);//结果：e //输出整个数组 System.out.println(Arrays.toString(a));//结果：[s, e, n] /** * 数组转list */ List&lt;String&gt; list=Arrays.asList(a); /** * list转Arraylist */ List&lt;String&gt; arrayList2 = new ArrayList&lt;String&gt;(); arrayList2.addAll(list); System.out.println(list);如何求ArrayList集合的交集 并集 差集 去重复并集需要用到List接口中定义的几个方法：addAll(Collection&lt;? extends E&gt; c) :按指定集合的Iterator返回的顺序将指定集合中的所有元素追加到此列表的末尾retainAll(Collection&lt;?&gt; c): 仅保留此列表中包含在指定集合中的元素。removeAll(Collection&lt;?&gt; c) :从此列表中删除指定集合中包含的所有元素。TIPS: JAVA8中提供了通过lambda方式处理集合，如Collections类中好多工具方法用stream流方式处理，方便了我们在处理集合时的各种情况，JAVA8集合这块再后面的blog中会单独拿出来总结。HashMap 的工作原理及代码实现集合框架源码学习之HashMap(JDK1.8)ConcurrentHashMap 的工作原理及代码实现ConcurrentHashMap实现原理及源码分析集合框架底层数据结构总结CollectionListArraylist：数组（查询快,增删慢 线程不安全,效率高 ）Vector：数组（查询快,增删慢 线程安全,效率低 ）LinkedList：链表（查询慢,增删快 线程不安全,效率高 ）SetHashSet（无序，唯一）:哈希表或者叫散列集(hash table)LinkedHashSet：链表和哈希表组成 。 由链表保证元素的排序 ， 由哈希表证元素的唯一性TreeSet（有序，唯一）：红黑树(自平衡的排序二叉树。)MapHashMap：基于哈希表的Map接口实现（哈希表对键进行散列，Map结构即映射表存放键值对）LinkedHashMap:HashMap 的基础上加上了链表数据结构HashTable:哈希表TreeMap:红黑树（自平衡的排序二叉树）集合的选用主要根据集合的特点来选用，比如我们需要根据键值获取到元素值时就选用Map接口下的集合，需要排序时选择TreeMap,不需要排序时就选择HashMap,需要保证线程安全就选用ConcurrentHashMap.当我们只需要存放元素值时，就选择实现Collection接口的集合，需要保证元素唯一时选择实现Set接口的集合比如TreeSet或HashSet，不需要就选择实现List接口的比如ArrayList或LinkedList，然后再根据实现这些接口的集合的特点来选用。","categories":[{"name":"面试","slug":"面试","permalink":"http://www.fufan.me/categories/面试/"}],"tags":[{"name":"面试","slug":"面试","permalink":"http://www.fufan.me/tags/面试/"},{"name":"java基础","slug":"java基础","permalink":"http://www.fufan.me/tags/java基础/"}]},{"title":"Java面试总结积累（基础篇）之语法问题","slug":"Java面试总结积累（基础篇）之语法问题","date":"2017-01-02T16:34:00.000Z","updated":"2018-11-05T03:12:29.706Z","comments":true,"path":"2017/01/03/Java面试总结积累（基础篇）之语法问题/","link":"","permalink":"http://www.fufan.me/2017/01/03/Java面试总结积累（基础篇）之语法问题/","excerpt":"","text":"java开发经验固然很重要，但是有很多面试当中会遇到一些基础问题，需要自己来进行总结归类，也算是扫盲和回归吧。 面向对象和面向过程的区别面向过程：优点：性能比面向对象高，因为类调用时需要实例化，开销比较大，比较消耗资源;比如单片机、嵌入式开发、Linux/Unix等一般采用面向过程开发，性能是最重要的因素。缺点：没有面向对象易维护、易复用、易扩展面向对象：优点：易维护、易复用、易扩展，由于面向对象有封装、继承、多态性的特性，可以设计出低耦合的系统，使系统更加灵活、更加易于维护缺点：性能比面向过程低Java语言有哪些特点？1，简单易学；2，面向对象（封装，继承，多态）；3，平台无关性（Java虚拟机实现平台无关性）；4，可靠性；5，安全性；6，支持多线程（C++语言没有内置的多线程机制，因此必须调用操作系统的多线程功能来进行多线程程序设计，而Java语言却提供了多线程支持）；7，支持网络编程并且很方便（Java语言诞生本身就是为简化网络编程设计的，因此Java语言不仅支持网络编程而且很方便）；8，编译与解释并存；什么是字节码？采用字节码的最大好处是什么？什么Java是虚拟机？先看下java中的编译器和解释器：Java中引入了虚拟机的概念，即在机器和编译程序之间加入了一层抽象的虚拟的机器。这台虚拟的机器在任何平台上都提供给编译程序一个的共同的接口。编译程序只需要面向虚拟机，生成虚拟机能够理解的代码，然后由解释器来将虚拟机代码转换为特定系统的机器码执行。在Java中，这种供虚拟机理解的代码叫做字节码（即扩展名为.class的文件），它不面向任何特定的处理器，只面向虚拟机。每一种平台的解释器是不同的，但是实现的虚拟机是相同的。Java源程序经过编译器编译后变成字节码，字节码由虚拟机解释执行，虚拟机将每一条要执行的字节码送给解释器，解释器将其翻译成特定机器上的机器码，然后在特定的机器上运行，这就是上面提到的Java的特点的编译与解释并存的解释。Java源代码—-&gt;编译器—-&gt;jvm可执行的Java字节码(即虚拟指令)—-&gt;jvm—-&gt;jvm中解释器—–&gt;机器可执行的二进制机器码—-&gt;程序运行。采用字节码的好处：Java语言通过字节码的方式，在一定程度上解决了传统解释型语言执行效率低的问题，同时又保留了解释型语言可移植的特点。所以Java程序运行时比较高效，而且，由于字节码并不专对一种特定的机器，因此，Java程序无须重新编译便可在多种不同的计算机上运行。什么是Java虚拟机任何一种可以运行Java字节码的软件均可看成是Java的虚拟机（JVM）字符型常量和字符串常量的区别形式上:字符常量是单引号引起的一个字符字符串常量是双引号引起的若干个字符含义上:字符常量相当于一个整形值(ASCII值),可以参加表达式运算字符串常量代表一个地址值(该字符串在内存中存放位置)占内存大小字符常量只占一个字节字符串常量占若干个字节(至少一个字符结束标志)Java语言采用何种编码方案？有何特点？Java语言采用Unicode编码标准，Unicode（标准码），它为每个字符制订了一个唯一的数值，因此在任何的语言，平台，程序都可以放心的使用。构造器Constructor是否可被override在讲继承的时候我们就知道父类的私有属性和构造方法并不能被继承，所以Constructor也就不能被override,但是可以overload,所以你可以看到一个类中有多个构造函数的情况。重载和重写的区别重载：发生在同一个类中，方法名必须相同，参数类型不同、个数不同、顺序不同，方法返回值和访问修饰符可以不同，发生在编译时。重写：发生在父子类中，方法名、参数列表必须相同，返回值小于等于父类，抛出的异常小于等于父类，访问修饰符大于等于父类；如果父类方法访问修饰符为private则子类中就不是重写。java 面向对象编程三大特性封装、继承、多态String和StringBuffer、StringBuilder的区别是什么？String为什么是不可变的可变性String类中使用字符数组保存字符串，private final char value[]，所以string对象是不可变的。StringBuilder与StringBuffer都继承自AbstractStringBuilder类，在AbstractStringBuilder中也是使用字符数组保存字符串，char[]value，这两种对象都是可变的。线程安全性String中的对象是不可变的，也就可以理解为常量，线程安全。AbstractStringBuilder是StringBuilder与StringBuffer的公共父类，定义了一些字符串的基本操作，如expandCapacity、append、insert、indexOf等公共方法。StringBuffer对方法加了同步锁或者对调用的方法加了同步锁，所以是线程安全的。StringBuilder并没有对方法进行加同步锁，所以是非线程安全的。性能每次对String 类型进行改变的时候，都会生成一个新的String对象，然后将指针指向新的String 对象。StringBuffer每次都会对StringBuffer对象本身进行操作，而不是生成新的对象并改变对象引用。相同情况下使用StirngBuilder 相比使用StringBuffer 仅能获得10%~15% 左右的性能提升，但却要冒多线程不安全的风险。对于三者使用的总结：如果要操作少量的数据用 = String单线程操作字符串缓冲区 下操作大量数据 = StringBuilder多线程操作字符串缓冲区 下操作大量数据 = StringBuffer在一个静态方法内调用一个非静态成员为什么是非法的？由于静态方法可以不通过对象进行调用，因此在静态方法里，不能调用其他非静态变量，也不可以访问非静态变量成员。在Java中定义一个不做事且没有参数的构造方法的作用Java程序在执行子类的构造方法之前，如果没有用super()来调用父类特定的构造方法，则会调用父类中“没有参数的构造方法”。因此，如果父类中只定义了有参数的构造方法，而在子类的构造方法中又没有用super()来调用父类中特定的构造方法，则编译时将发生错误，因为Java程序在父类中找不到没有参数的构造方法可供执行。解决办法是在父类里加上一个不做事且没有参数的构造方法。接口和抽象类的区别是什么？接口的方法默认是public，所有方法在接口中不能有实现，抽象类可以有非抽象的方法接口中的实例变量默认是final类型的，而抽象类中则不一定一个类可以实现多个接口，但最多只能实现一个抽象类一个类实现接口的话要实现接口的所有方法，而抽象类不一定接口不能用new实例化，但可以声明，但是必须引用一个实现该接口的对象从设计层面来说，抽象是对类的抽象，是一种模板设计，接口是行为的抽象，是一种行为的规范。成员变量与局部变量的区别有那些？从语法形式上，看成员变量是属于类的，而局部变量是在方法中定义的变量或是方法的参数；成员变量可以被public,private,static等修饰符所修饰，而局部变量不能被访问控制修饰符及static所修饰；成员变量和局部变量都能被final所修饰；从变量在内存中的存储方式来看，成员变量是对象的一部分，而对象存在于堆内存，局部变量存在于栈内存从变量在内存中的生存时间上看，成员变量是对象的一部分，它随着对象的创建而存在，而局部变量随着方法的调用而自动消失。成员变量如果没有被赋初值，则会自动以类型的默认值而赋值（一种情况例外被final修饰但没有被static修饰的成员变量必须显示地赋值）；而局部变量则不会自动赋值。创建一个对象用什么运算符？对象实体与对象引用有何不同？new运算符，new创建对象实例（对象实例在堆内存中），对象引用指向对象实例（对象引用存放在栈内存中）。一个对象引用可以指向0个或1个对象（一根绳子可以不系气球，也可以系一个气球）;一个对象可以有n个引用指向它（可以用n条绳子系住一个气球）什么是方法的返回值？返回值在类的方法里的作用是什么方法的返回值是指我们获取到的某个方法体中的代码执行后产生的结果！（前提是该方法可能产生结果）。返回值的作用:接收出结果，使得它可以用于其他的操作！一个类的构造方法的作用是什么？若一个类没有声明构造方法，改程序能正确执行吗？为什么？主要作用是完成对类对象的初始化工作。可以执行。因为一个类即使没有声明构造方法也会有默认的不带参数的构造方法。构造方法有哪些特性？名字与类名相同；没有返回值，但不能用void声明构造函数；生成类的对象时自动执行，无需调用。静态方法和实例方法有何不同？静态方法和实例方法的区别主要体现在两个方面：在外部调用静态方法时，可以使用”类名.方法名”的方式，也可以使用”对象名.方法名”的方式。而实例方法只有后面这种方式。也就是说，调用静态方法可以无需创建对象。静态方法在访问本类的成员时，只允许访问静态成员（即静态成员变量和静态方法），而不允许访问实例成员变量和实例方法；实例方法则无此限制对象的相等与指向他们的引用相等，两者有什么不同？对象的相等 比的是内存中存放的内容是否相等而 引用相等 比较的是他们指向的内存地址是否相等。在调用子类构造方法之前会先调用父类没有参数的构造方法，其目的是？帮助子类做初始化工作。equals 和 == 的区别？通俗点讲：==是看看左右是不是一个东西。equals是看看左右是不是长得一样。如何记住嘛。如果单纯是想记住，==：等于。equals：相同。两个长得一样的人，只能说长的相同(equals)，但是不等于他们俩是一个人。你只要记住equals，==就不用记了。术语来讲的区别：==是判断两个变量或实例是不是指向同一个内存空间 equals是判断两个变量或实例所指向的内存空间的值是不是相同==是指对内存地址进行比较 equals()是对字符串的内容进行比较3.==指引用是否相同 equals()指的是值是否相同","categories":[{"name":"面试","slug":"面试","permalink":"http://www.fufan.me/categories/面试/"}],"tags":[{"name":"面试","slug":"面试","permalink":"http://www.fufan.me/tags/面试/"},{"name":"java base","slug":"java-base","permalink":"http://www.fufan.me/tags/java-base/"}]},{"title":"（绪）设计模式之架构中的设计原则","slug":"（绪）设计模式之架构中的设计原则","date":"2016-10-11T10:51:00.000Z","updated":"2018-11-05T08:13:53.199Z","comments":true,"path":"2016/10/11/（绪）设计模式之架构中的设计原则/","link":"","permalink":"http://www.fufan.me/2016/10/11/（绪）设计模式之架构中的设计原则/","excerpt":"","text":"在进行软件架构工作时，需要遵循面向对象原则，这些原则同样在各类设计模式、架构模式之中，在学习过程中可以通过类图、时序图、示例代码等形式，不断体会这些原则在解决“依赖”和变化中的效果。当然，这些“原则”的队伍也在变化。不断有新的“原则加入，也有被淘汰掉的，真正沉淀下来的通用的”原则“其实并不多在使用面向对象的思想进行系统设计时，前任总结出了7条原则，分别是单一职责原则、开闭原则、里氏替换原则、依赖注入原则、接口分离原则、迪米特原则和有限使用组合而不是继承原则。下面来介绍下这几种原则的含义，也为后面学习设计模式打下基础。原则一：单一职责原则但以职责原则的核心思想就是：系统中的每一个对象都应该只有一个单独的职责，而所有对象所关注的就是自身职责的完成，全称即Single Responsibility Principle。其实单一职责的意思就是开发人员经常说的”高内聚、低耦合“。也就是说，每一个类应该只有一个职责，对外只能提供一种功能，而引起类变化的原因应该只有一个。在设计模式中，所有的设计模式都要遵守这个原则。”单一职责“也就是”单一变化原因“。通常一个类的职责越多，导致的变化因素也与阿朵，我们在设计的时候可能会把该类的有关的操作都组合在这个类中，这样做的后果就有可能将多个职责“耦合”到一起。解决这个问题的方法就是“分耦”，将不同的职责分别进行封装，不要将组合在一个类中。例如将用户的属性和用户的行为放在一个接口中声明，如下：12345678910interface User&#123; //身高 public double getHeight(); //体重 public double getWeight(); //吃饭 public void eat(); //玩游戏 public void gaming();&#125;上面的例子就存在这个问题，身高和体重属于业务对象，与之对应的方法主要负责用户的属性，而吃饭和玩游戏是相应的业务逻辑，主要负责用户的行为，这会给人一种不知道这个接口到底是做什么的感觉，职责不清晰，后期维护的时候会造成各种各样的问题。可以将这个接口分为两个。123456interface UserPro&#123; //身高 public double getHeight(); //体重 public double getWeight();&#125;123456interface UserAct&#123; //吃饭 public void eat(); //玩游戏 public void gaming();&#125;然后分别实现这两个接口，这里的实现我就不详细写了，主要通过这种方式可以做到当需要修改用户属性的时候，只需要对UserPro这个接口进行修改，而不会影响到其他类。另外，SRP原则的好处是可以消除耦合，减小因需求变化引起代码僵化的难堪局面。需要注意：一个合理的类，应该仅有一个引起他变化的原因，即单一职责。在没有变化征兆的情况下，应用SRP或其他原则是不明智的。在需求实际发生变化时就应该应用SRP等原则来重构代码。使用测试驱动开发会迫使我们在设计出现劣质趋势之前分离不合理代码如果测试不能迫使职责分离，僵化性和脆弱性的腐朽味会变得很浓烈，那就应该用Facade或者Proxy模式对代码重构原则二：里氏替换原则（LSP）里氏替换原则的核心思想就是：在任何父类出现的地方都可以用他的自雷来替代。它的英文缩写为LSP，全称是Liskov Subsitituition Principle。通俗点讲，就是同一个继承体系中的对象应该有果农共同的行为特征。里氏替换原则关注的是怎样良好地使用继承，也就是说不要滥用继承，它是继承复用的基石。在里氏替换原则中，所有引用基类的地方必须能够透明地使用其子类对象，也就是说，只要父类出现的地方，子类就能出现，而且替换为子类不会产生任何错误或者异常。但是反过来，子类出现的地方，替换为父类就可能出现问题了。主要抓住以下四层含义（子类的范围大于等于父类的范围）：子类必须完全实现父类的方法子类可以有自己的特性覆盖或者实现父类的方法时输入参数可以被放大覆写或者实现父类的方法时输出结果可以被缩小原则三：依赖注入原则（DIP）依赖注入原则的核心思想就是：要依赖于抽象，不要依赖于具体的实现。英文全称就是Dependence Inversion Principle。通俗的讲：在应用程序中，所有的类如果使用或者依赖于其他的类，则都应该依赖于这些其他类的抽象类，而不是这些其他类的具体实现。即要求开发人员在编程时针对接口编程，而不是针对实现编程。依赖注入原则有三点要注意的：高层模块不应该依赖低层模块，两者都应该依赖于抽象（抽象类或接口）。抽象（抽象类或接口）不应该依赖于细节（具体实现类）。细节（具体实现类）应该依赖抽象这里的抽象指的是不能被实例化的抽象类或接口，具体的实现则是可以通过new直接实例化的。这个原则是开闭原则的基础（对扩展开放，对修改关闭）。三种实现方式：通过构造函数传递依赖对象通过setter方法传递依赖对象接口声明实现依赖对象原则四：接口分离原则（ISP）接口分离原则的核心思想就是：不应该强迫客户程序依赖它们不需要使用的方法。它的全称是Interface Segregation Principle。其实接口分离原则的意思就是一个接口不需要提供太多的行为，一个接口应该只提供一种对外的功能，不应该吧所有的操作都封装到一个接口中。这里的接口不仅是interface关键字的实例，接口分为以下两种：对象接口（object Interface）java中声明一个类，通过new出一个实例，它是对一个类型的事务的描述，这也是一种接口。例如：1Phone phone = new Phone(); //这里的类Phone就是实例Phone的一个接口类接口（Class Interface）这种接口就是通过关键字Interface定义的接口。接口分离原则要求的是在一个模块中应该只依赖它需要的接口，以保证接口的纯洁。切勿定义太臃肿的接口。接口分离原则与但以职责原则有点类似，都是说如何设计接口，不过不同在于单一职责原则要求的是类和接口职责单一，注重的是职责，是业务逻辑上的划分。而接口分离原则要求的是接口的方法尽量少，针对一个模块尽量有用。如何做到该原则：接口尽量小：小的概念是保证一个接口之服务于一个子模块或者业务逻辑接口高内聚：指的是对内高度依赖，对外尽可能隔离。即一个接口内部声明的方法相互之间都与某一个子模块相关，且这个子模块必需的。接口设计是有限度的：话说回来，如果过度地遵循该原则，会使得接口数量剧增，复杂度正价，这并不是我们想要的结果。原则五：迪米特原则（LOD）全称是Law of Demeter。核心思想就是：一个对象应当对其他对象尽可能少地了解。意思就是降低各个对象之间的耦合，提高系统的可维护性。在模块之间，应该只通过接口来通信，而不理会模块的内部工作原理，它可以使各个模块偶和程度降到最低，促进软件的复用。它的核心观念还是类间解耦，弱耦合。举个例子，监狱的犯人是不能随便和外面的人打交道，除非探亲，所以狱警就是这个迪米特法则的执行者，监狱就是类，犯人就是类的内部信息。总结下这个原则要注意的地方：在类的划分上，因可更改创建有弱耦合的类。在类的结构设计上，每一个类都应当尽量降低成员的访问权限。在类的设计上，只要有可能，一个类应当设计成不变类在对其他类的引用上，一个对象对其他对象的引用应当降到最低尽量降低类的访问权限谨慎使用序列化功能不要暴露类成员，而应该提供相应的访问方法（属性getter）原则六：开闭原则（OCP）开闭原则的核心思想：一个对象对扩展开放，对修改关闭。其实开闭原则就是：对类的改动是通过增加代码进行的，而不是改动现有的代码。也就是说，软件开发人员一旦写出了可以运行的代码，就不应该去改变它，而是要保证他一直能运行下去，这就需要借助java的抽象和多态，即把可能变化的内容抽象出来，从而使抽象的部分是相对稳定的，而具体的实现层则是可以改变和扩展的。注意：这些设计原则并不是绝对的，而是应根据项目的实际需求来定夺。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://www.fufan.me/categories/设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://www.fufan.me/tags/设计模式/"}]},{"title":"JAVA基本数据结构","slug":"JAVA基本数据结构","date":"2016-09-12T15:18:00.000Z","updated":"2018-11-05T03:13:08.387Z","comments":true,"path":"2016/09/12/JAVA基本数据结构/","link":"","permalink":"http://www.fufan.me/2016/09/12/JAVA基本数据结构/","excerpt":"","text":"java基本数据结构其实学一门语言，基础很重要，现在很多java程序员只是对jdk和各种框架特别熟悉，能熟练地使用各种包和api组件，包括现在很多培训都是灌输这些所谓的实际应用。这会导致学到最后只会照葫芦画瓢。java数据结构的只是体系包括线性表、树、数组、集合、矩阵、排序、查找、哈希表，并将java的设计思想、方法及一些常见的算法、设计模式贯穿其中。其中线性表、链表、哈希表是最为常用的数据结构，在进行java开发时，jdk已经为我们提供了一系列相应的类，如下图。来实现基本的数据结构。这些类均在java.util包中。CollectionListLinkedListArrayListVector(Stack)SetQueueMapHashtableHashMapWeakHashMapCollection接口接口Collection是最基本的集合接口，一个Collection代表一组Object，即Collection的元素（Elements）。主要分为两类，LIst和Set，它们是以是否允许有相同元素来区分。当然其结构也不同。所有实现Collection接口的类都必须提供两个标准的构造函数。无参为空，有参则可复制一个传入的Collection。如何遍历？可以通过迭代器iterator()方法，注意访问Collection中的每一个元素，这种方式也是所有继承于它的类都可以使用的遍历方式。12345Collection collection = new ArrayList&lt;String&gt;();Iterator i = collection.iterator();while(i.hasNext())&#123; Object s = i.next();&#125;它的派生类包括List和Set，以下是他接口中的主要方法：boolean add(Object o)：用于添加对象到集合boolean remove(Object o)：用于删除指定的对象int size()：用于返回当前集合中元素的个数boolean isEmpty()：用于判断集合是否为空。Iterator iterator()：返回一个迭代器boolean contains(Object o)：用于查找集合中是否有指定的对象boolean containsAll(Collection c)：用于查找集合中是否有集合c中的元素boolean addAll（Collection c）：用于将集合c中的元素全部添加到该集合中void clear()：用于清空该集合void removeAll(Collection c)：用于从集合中删除从集合中所有的元素void retainAll(Collection c)：从集合中删除集合c中不包含的元素List接口List是有序的Collection，用户能够使用索引来访问List中的元素，类似数组。LIst包括以下几种子类ArrayList:：是一个数组队列，相当于动态数组。它由数组实现，随机访问效率高，随机插入、随机删除效率低。LinkedList：是一个双向链表。它也可以被当作堆栈、队列或双端队列进行操作。LinkedList随机访问效率高，但随机插入、随机删除效率低。Vector 是矢量队列，和ArrayList一样，它也是一个动态数组，由数组实现。但是ArrayList是非线程安全的，而Vector是线程安全的。Stack 是栈，它继承于Vector。它的特性是：先进后出(FILO, First In Last Out)。如果涉及到“栈”、“队列”、“链表”等操作，应该考虑用List，具体的选择哪个List，根据下面的标准来取舍。(01) 对于需要快速插入，删除元素，应该使用LinkedList。(02) 对于需要快速随机访问元素，应该使用ArrayList。(03)对于“单线程环境” 或者 “多线程环境，但List仅仅只会被单个线程操作”，此时应该使用非同步的类(如ArrayList)。对于“多线程环境，且List可能同时被多个线程操作”，此时，应该使用同步的类(如Vector)。通过下面的测试程序，我们来验证上面的(01)和(02)结论。参考代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081import java.util.*;import java.lang.Class;/* * @desc 对比ArrayList和LinkedList的插入、随机读取效率、删除的效率 * * @author skywang */public class ListCompareTest &#123; private static final int COUNT = 100000; private static LinkedList linkedList = new LinkedList(); private static ArrayList arrayList = new ArrayList(); private static Vector vector = new Vector(); private static Stack stack = new Stack(); public static void main(String[] args) &#123; // 换行符 System.out.println(&quot;插入元素&quot;); // 插入 insertByPosition(stack) ; insertByPosition(vector) ; insertByPosition(linkedList) ; insertByPosition(arrayList) ; // 换行符 System.out.println(&quot;随机读取&quot;); // 随机读取 readByPosition(stack); readByPosition(vector); readByPosition(linkedList); readByPosition(arrayList); // 换行符 System.out.println(&quot;删除元素&quot;); // 删除 deleteByPosition(stack); deleteByPosition(vector); deleteByPosition(linkedList); deleteByPosition(arrayList); &#125; // 获取list的名称 private static String getListName(List list) &#123; if (list instanceof LinkedList) &#123; return &quot;LinkedList&quot;; &#125; else if (list instanceof ArrayList) &#123; return &quot;ArrayList&quot;; &#125; else if (list instanceof Stack) &#123; return &quot;Stack&quot;; &#125; else if (list instanceof Vector) &#123; return &quot;Vector&quot;; &#125; else &#123; return &quot;List&quot;; &#125; &#125; // 向list的指定位置插入COUNT个元素，并统计时间 private static void insertByPosition(List list) &#123; long startTime = System.currentTimeMillis(); // 向list的位置0插入COUNT个数 for (int i=0; i&lt;COUNT; i++) list.add(0, i); long endTime = System.currentTimeMillis(); long interval = endTime - startTime; System.out.println(getListName(list) + &quot; : insert &quot;+COUNT+&quot; elements into the 1st position use time：&quot; + interval+&quot; ms&quot;); &#125; // 从list的指定位置删除COUNT个元素，并统计时间 private static void deleteByPosition(List list) &#123; long startTime = System.currentTimeMillis(); // 删除list第一个位置元素 for (int i=0; i&lt;COUNT; i++) list.remove(0); long endTime = System.currentTimeMillis(); long interval = endTime - startTime; System.out.println(getListName(list) + &quot; : delete &quot;+COUNT+&quot; elements from the 1st position use time：&quot; + interval+&quot; ms&quot;); &#125; // 根据position，不断从list中读取元素，并统计时间 private static void readByPosition(List list) &#123; long startTime = System.currentTimeMillis(); // 读取list元素 for (int i=0; i&lt;COUNT; i++) list.get(i); long endTime = System.currentTimeMillis(); long interval = endTime - startTime; System.out.println(getListName(list) + &quot; : read &quot;+COUNT+&quot; elements by position use time：&quot; + interval+&quot; ms&quot;); &#125;&#125;运行结果如下：插入元素Stack : insert 100000 elements into the 1st position use time：1544 msVector : insert 100000 elements into the 1st position use time：1520 msLinkedList : insert 100000 elements into the 1st position use time：17 msArrayList : insert 100000 elements into the 1st position use time：1519 ms随机读取Stack : read 100000 elements by position use time：7 msVector : read 100000 elements by position use time：7 msLinkedList : read 100000 elements by position use time：8023 msArrayList : read 100000 elements by position use time：2 ms删除元素Stack : delete 100000 elements from the 1st position use time：1553 msVector : delete 100000 elements from the 1st position use time：1525 msLinkedList : delete 100000 elements from the 1st position use time：9 msArrayList : delete 100000 elements from the 1st position use time：1547 ms这里只是对性能做了大致的测试，如果需要研究为何产生如此差异，需要看下数据结构的相关资料。Set接口Set接口是一种不包含重复元素的Collection，也就是说任何两个在Set里面的元素都存在以下e1.equals(e2) == false关系，且Set最多只有一个NULL元素。很明显，Set的构造函数有一个约束条件，就是传入的Collection参数不能包含重复的元素。Queue接口Queue接口与List、Set同一级别，都是继承了Collection接口。LinkedList实现了Queue接 口。Queue接口窄化了对LinkedList的方法的访问权限（即在方法中的参数类型如果是Queue时，就完全只能访问Queue接口所定义的方法 了，而不能直接访问 LinkedList的非Queue的方法），以使得只有恰当的方法才可以使用。BlockingQueue 继承了Queue接口。队列是一种数据结构．它有两个基本操作：在队列尾部加人一个元素，和从队列头部移除一个元素就是说，队列以一种先进先出的方式管理数据，如果你试图向一个 已经满了的阻塞队列中添加一个元素或者是从一个空的阻塞队列中移除一个元索，将导致线程阻塞．在多线程进行合作时，阻塞队列是很有用的工具。工作者线程可 以定期地把中间结果存到阻塞队列中而其他工作者线线程把中间结果取出并在将来修改它们。队列会自动平衡负载。如果第一个线程集运行得比第二个慢，则第二个 线程集在等待结果时就会阻塞。如果第一个线程集运行得快，那么它将等待第二个线程集赶上来。下表显示了jdk1.5中的阻塞队列的操作：add 增加一个元索 如果队列已满，则抛出一个IIIegaISlabEepeplian异常remove 移除并返回队列头部的元素 如果队列为空，则抛出一个NoSuchElementException异常element 返回队列头部的元素 如果队列为空，则抛出一个NoSuchElementException异常offer 添加一个元素并返回true 如果队列已满，则返回falsepoll 移除并返问队列头部的元素 如果队列为空，则返回nullpeek 返回队列头部的元素 如果队列为空，则返回nullput 添加一个元素 如果队列满，则阻塞take 移除并返回队列头部的元素 如果队列为空，则阻塞remove、element、offer 、poll、peek 其实是属于Queue接口。Map接口Map接口没有继承于接口Collection，Map提供key到value的映射。键值对key-value，主要方法如下：boolean equals(Object o)：用于比较对象boolean remove(Object o)：用于删除一个对象void put(Object key, Object value)：用于添加key和valueMap可分为HashMap、HashTable、WeakHashMap、ConcurrentHashMap等。但是我们常用的主要是HashMap和HashTable，下面比较下两者区别：HashMap是非线程安全的，HashTable是线程安全的。HashMap的键和值都允许有null值存在，而HashTable则不行。因为线程安全的问题，HashMap效率比HashTable的要高。能答出上面的三点，简单的面试，算是过了，但是如果再问：Java中的另一个线程安全的与HashMap及其类似的类是什么？(ConcurrentHashMap)同样是线程安全，它与HashTable在线程同步上有什么不同？(synchronized关键字加锁的原理，其实是对对象加锁，不论你是在方法前加synchronized还是语句块前加，锁住的都是对象整体，但是ConcurrentHashMap的同步机制和这个不同，它不是加synchronized关键字，而是基于lock操作的，这样的目的是保证同步的时候，锁住的不是整个对象。事实上，ConcurrentHashMap可以满足concurrentLevel个线程并发无阻塞的操作集合对象)能把第二个问题完整的答出来，说明你的基础算是不错的了。下面浅析更多区别。HashMap1) hashmap的数据结构Hashmap是一个数组和链表的结合体（在数据结构称“链表散列“），如下图示：当我们往hashmap中put元素的时候，先根据key的hash值得到这个元素在数组中的位置（即下标），然后就可以把这个元素放到对应的位置中了。如果这个元素所在的位子上已经存放有其他元素了，那么在同一个位子上的元素将以链表的形式存放，新加入的放在链头，最先加入的放在链尾。2)使用和遍历1234567891011Map map = new HashMap();map.put(&quot;Rajib Sarma&quot;,&quot;100&quot;);map.put(&quot;Rajib Sarma&quot;,&quot;200&quot;);//The value &quot;100&quot; is replaced by &quot;200&quot;.map.put(&quot;Sazid Ahmed&quot;,&quot;200&quot;);Iterator iter = map.entrySet().iterator();while (iter.hasNext()) &#123; Map.Entry entry = (Map.Entry) iter.next(); Object key = entry.getKey(); Object val = entry.getValue();&#125;HashTable和HashMap区别继承不同。public class Hashtable extends Dictionary implements Mappublic class HashMap extends AbstractMap implements MapHashtable 中的方法是同步的，而HashMap中的方法在缺省情况下是非同步的。在多线程并发的环境下，可以直接使用Hashtable，但是要使用HashMap的话就要自己增加同步处理了。Hashtable中，key和value都不允许出现null值。在HashMap中，null可以作为键，这样的键只有一个；可以有一个或多个键所对应的值为null。当get()方法返回null值时，即可以表示 HashMap中没有该键，也可以表示该键所对应的值为null。因此，在HashMap中不能由get()方法来判断HashMap中是否存在某个键， 而应该用containsKey()方法来判断。两个遍历方式的内部实现上不同。Hashtable、HashMap都使用了 Iterator。而由于历史原因，Hashtable还使用了Enumeration的方式 。哈希值的使用不同，HashTable直接使用对象的hashCode。而HashMap重新计算hash值。Hashtable和HashMap它们两个内部实现方式的数组的初始大小和扩容的方式。HashTable中hash数组默认大小是11，增加的方式是 old*2+1。HashMap中hash数组的默认大小是16，而且一定是2的指数。","categories":[{"name":"java base","slug":"java-base","permalink":"http://www.fufan.me/categories/java-base/"}],"tags":[{"name":"java 数据结构","slug":"java-数据结构","permalink":"http://www.fufan.me/tags/java-数据结构/"}]},{"title":"JAVA的Reflection反射机制","slug":"JAVA的Reflection反射机制","date":"2016-09-11T06:55:00.000Z","updated":"2018-11-05T03:13:21.648Z","comments":true,"path":"2016/09/11/JAVA的Reflection反射机制/","link":"","permalink":"http://www.fufan.me/2016/09/11/JAVA的Reflection反射机制/","excerpt":"","text":"反射即reflectionjava反射运用了代理模式，代理模式在之后学习的设计模式中可以了解反射主要用了以下几点：在运行时判断任意一个对象所属的类。在运行时构造任意一个类的对象。在运行时判断任意一个类所具有的成员变量和方法。在运行时调用任意一个对象的方法Class首先要搞清楚Class这个类，每个类在创建的时候都会有Class这个类伴随产生，这个Class是JVM产生的，由于是JVM产生的，所以我们一般获取Class的方法是：object.getClass()Class.forName(“java.lang.String”)Class.getSuperClass()运用.class语法,如java.lang.String.classprimitive wrapper classes的TYPE语法,如：Boolean.TYPE，类似Boolean.class12345Class&lt;?&gt; clazz = s.getClass();Class&lt;?&gt; clazz2 = Class.forName(&quot;com.fufan.reflection.Son&quot;);Class&lt;?&gt; clazz3 = clazz2.getSuperclass();Class&lt;?&gt; clazz4 = com.fufan.reflection.Son.class;Class&lt;?&gt; clazz5 = Boolean.class;Class是Reflection起源。针对任何您想探勘的class，唯有先为它产生一个Class object，接下来才能经由后者唤起为数十多个的Reflection APIs接下来就可以通过Class调用衍生出的一系列API：getName()：获得类的完整名字。getFields()：获得类的public类型的属性。getDeclaredFields()：获得类的所有属性。getMethods()：获得类的public类型的方法。getDeclaredMethods()：获得类的所有方法。getConstructors()：获得类的public类型的构造方法。getMethod(String name, Class[] parameterTypes)：获得类的特定方法，name参数指定方法的名字，parameterTypes 参数指定方法的参数类型。getConstructors()：获得类的public类型的构造方法。getConstructor(Class[] parameterTypes)：获得类的特定构造方法，parameterTypes 参数指定构造方法的参数类型。FieldFiled类：代表类的成员变量（成员变量也称为类的属性）。1String fieldName = field.getName();MethodMethod类：代表类的方法，invoke1Object value = getMethod.invoke(object, new Object[] &#123;&#125;);ConstructorConstructor类：代表类的构造方法，调用有参和无参newInstance()：通过类的不带参数的构造方法创建这个类的一个对象。newInstance(new Object[]{value})：当调用有参构造函数时使用。12345//无参构造方法Constructor constructor1 = classType.getConstructor();//有参构造方法Constructor constructor2 = classType.getConstructor(new Class[] &#123;int.class, String.class&#125;);代码示例11234567891011121314151617181920212223242526272829// 获得对象的类型 Class&lt;?&gt; classType = object.getClass(); System.out.println(&quot;Class:&quot; + classType.getName()); // 通过默认构造方法创建一个新的对象 Object objectCopy = classType.getConstructor(new Class[] &#123;&#125;).newInstance(new Object[] &#123;&#125;); // 获得对象的所有属性 Field fields[] = classType.getDeclaredFields(); for (int i = 0; i &lt; fields.length; i++) &#123; Field field = fields[i]; String fieldName = field.getName(); String firstLetter = fieldName.substring(0, 1).toUpperCase(); // 获得和属性对应的getXXX()方法的名字 String getMethodName = &quot;get&quot; + firstLetter + fieldName.substring(1); // 获得和属性对应的setXXX()方法的名字 String setMethodName = &quot;set&quot; + firstLetter + fieldName.substring(1); // 获得和属性对应的getXXX()方法 Method getMethod = classType.getMethod(getMethodName, new Class[] &#123;&#125;); // 获得和属性对应的setXXX()方法 Method setMethod = classType.getMethod(setMethodName, new Class[] &#123; field.getType() &#125;); // 调用原对象的getXXX()方法 Object value = getMethod.invoke(object, new Object[] &#123;&#125;); System.out.println(fieldName + &quot;:&quot; + value); // 调用拷贝对象的setXXX()方法 setMethod.invoke(objectCopy, new Object[] &#123; value &#125;);&#125;ArrayArray类：提供了动态创建数组，以及访问数组的元素的静态方法代码示例2123456789101112131415161718//一维数组的使用Object array = Array.newInstance(Integer.TYPE, 10);System.out.println(Integer.TYPE);for(int index=1; index&lt;10; index ++)&#123; Array.set(array, index, index);&#125;System.out.println(Array.get(array, 4));//多维数组的使用Object arrays = Array.newInstance(String.class, 3,5);Object array1 = Array.get(arrays, 2);Array.set(array1, 3, &quot;fufan&quot;);String[][] arrayInt = (String[][]) arrays; System.out.println(arrayInt[1][3]);","categories":[{"name":"java base","slug":"java-base","permalink":"http://www.fufan.me/categories/java-base/"}],"tags":[{"name":"java","slug":"java","permalink":"http://www.fufan.me/tags/java/"}]}]}