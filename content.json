{"pages":[{"title":"About Me","date":"2017-11-26T03:42:51.000Z","updated":"2018-10-02T12:59:31.889Z","comments":true,"path":"about/index.html","permalink":"http://www.fufan.me/about/index.html","excerpt":"","text":"I AM WHAT I AM ……"},{"title":"Categories","date":"2017-11-14T05:59:36.000Z","updated":"2018-10-02T07:50:02.801Z","comments":true,"path":"categories/index.html","permalink":"http://www.fufan.me/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2017-11-16T09:08:20.000Z","updated":"2018-10-02T07:52:02.919Z","comments":true,"path":"tags/index.html","permalink":"http://www.fufan.me/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"分布式锁的实现方式（一）——数据库和Redis锁","slug":"分布式锁的实现方式（一）——数据库和Redis锁","date":"2018-04-07T02:29:00.000Z","updated":"2018-11-07T06:07:36.483Z","comments":true,"path":"2018/04/07/分布式锁的实现方式（一）——数据库和Redis锁/","link":"","permalink":"http://www.fufan.me/2018/04/07/分布式锁的实现方式（一）——数据库和Redis锁/","excerpt":"","text":"分布式锁一般有三种实现方式：1. 数据库乐观锁；2. 基于Redis的分布式锁；3. 基于ZooKeeper的分布式锁。4.基于consul的分布式锁。在之前的java多线程系列中（java多线程系列（三）——锁），我已经学习到了各种各样的锁在jdk中的使用。如今大部分互联网系统都是分布式系统，所以实现支持具体业务的高可用分布式锁是我们经常要做的事情。问题什么是锁在单进程的系统中，当存在多个线程可以同时改变某个变量（可变共享变量）时，就需要对变量或代码块做同步，使其在修改这种变量时能够线性执行消除并发修改变量。而同步的本质是通过锁来实现的。为了实现多个线程在一个时刻同一个代码块只能有一个线程可执行，那么需要在某个地方做个标记，这个标记必须每个线程都能看到，当标记不存在时可以设置该标记，其余后续线程发现已经有标记了则等待拥有标记的线程结束同步代码块取消标记后再去尝试设置标记。这个标记可以理解为锁。不同地方实现锁的方式也不一样，只要能满足所有线程都能看得到标记即可。如 Java 中 synchronize 是在对象头设置标记，Lock 接口的实现类基本上都只是某一个 volitile 修饰的 int 型变量其保证每个线程都能拥有对该 int 的可见性和原子修改，linux 内核中也是利用互斥量或信号量等内存数据做标记。除了利用内存数据做锁其实任何互斥的都能做锁（只考虑互斥情况），如流水表中流水号与时间结合做幂等校验可以看作是一个不会释放的锁，或者使用某个文件是否存在作为锁等。只需要满足在对标记进行修改能保证原子性和内存可见性即可。什么是分布式？分布式的 CAP 理论告诉我们:1任何一个分布式系统都无法同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance），最多只能同时满足两项。目前很多大型网站及应用都是分布式部署的，分布式场景中的数据一致性问题一直是一个比较重要的话题。基于 CAP理论，很多系统在设计之初就要对这三者做出取舍。在互联网领域的绝大多数的场景中，都需要牺牲强一致性来换取系统的高可用性，系统往往只需要保证最终一致性。在许多的场景中，我们为了保证数据的最终一致性，需要很多的技术方案来支持，比如分布式事务、分布式锁等。很多时候我们需要保证一个方法在同一时间内只能被同一个线程执行。在单机环境中，通过 Java 提供的并发 API 我们可以解决，但是在分布式环境下，就没有那么简单啦。分布式与单机情况下最大的不同在于其不是多线程而是多进程。多线程由于可以共享堆内存，因此可以简单的采取内存作为标记存储位置。而进程之间甚至可能都不在同一台物理机上，因此需要将标记存储在一个所有进程都能看到的地方。什么是分布式锁？当在分布式模型下，数据只有一份（或有限制），此时需要利用锁的技术控制某一时刻修改数据的进程数。与单机模式下的锁不仅需要保证进程可见，还需要考虑进程与锁之间的网络问题。（我觉得分布式情况下之所以问题变得复杂，主要就是需要考虑到网络的延时和不可靠。。。一个大坑）分布式锁还是可以将标记存在内存，只是该内存不是某个进程分配的内存而是公共内存如 Redis、Memcache。至于利用数据库、文件等做锁与单机的实现是一样的，只要保证标记能互斥就行。我们需要怎样的分布式锁？可以保证在分布式部署的应用集群中，同一个方法在同一时间只能被一台机器-上的一个线程执行。这把锁要是一把可重入锁（避免死锁）这把锁最好是一把阻塞锁（根据业务需求考虑要不要这条）这把锁最好是一把公平锁（根据业务需求考虑要不要这条）有高可用的获取锁和释放锁功能获取锁和释放锁的性能要好基于数据库做分布式锁乐观锁基于表主键唯一做分布式锁思路： 利用主键唯一的特性，如果有多个请求同时提交到数据库的话，数据库会保证只有一个操作可以成功，那么我们就可以认为操作成功的那个线程获得了该方法的锁，当方法执行完毕之后，想要释放锁的话，删除这条数据库记录即可。上面这种简单的实现有以下几个问题：这把锁强依赖数据库的可用性，数据库是一个单点，一旦数据库挂掉，会导致业务系统不可用。这把锁没有失效时间，一旦解锁操作失败，就会导致锁记录一直在数据库中，其他线程无法再获得到锁。这把锁只能是非阻塞的，因为数据的 insert操作，一旦插入失败就会直接报错。没有获得锁的线程并不会进入排队队列，要想再次获得锁就要再次触发获得锁操作。这把锁是非重入的，同一个线程在没有释放锁之前无法再次获得该锁。因为数据中数据已经存在了。这把锁是非公平锁，所有等待锁的线程凭运气去争夺锁。在 MySQL 数据库中采用主键冲突防重，在大并发情况下有可能会造成锁表现象。当然，我们也可以有其他方式解决上面的问题。数据库是单点？搞两个数据库，数据之前双向同步，一旦挂掉快速切换到备库上。没有失效时间？只要做一个定时任务，每隔一定时间把数据库中的超时数据清理一遍。非阻塞的？搞一个 while 循环，直到 insert 成功再返回成功。非重入的？在数据库表中加个字段，记录当前获得锁的机器的主机信息和线程信息，那么下次再获取锁的时候先查询数据库，如果当前机器的主机信息和线程信息在数据库可以查到的话，直接把锁分配给他就可以了。非公平的？再建一张中间表，将等待锁的线程全记录下来，并根据创建时间排序，只有最先创建的允许获取锁。比较好的办法是在程序中生产主键进行防重。悲观锁基于表字段版本号做分布式锁这个策略源于 mysql 的 mvcc 机制，使用这个策略其实本身没有什么问题，唯一的问题就是对数据表侵入较大，我们要为每个表设计一个版本号字段，然后写一条判断 sql 每次进行判断，增加了数据库操作的次数，在高并发的要求下，对数据库连接的开销也是无法忍受的。基于数据库排他锁做分布式锁在查询语句后面增加for update，数据库会在查询过程中给数据库表增加排他锁 (注意： InnoDB 引擎在加锁的时候，只有通过索引进行检索的时候才会使用行级锁，否则会使用表级锁。这里我们希望使用行级锁，就要给要执行的方法字段名添加索引，值得注意的是，这个索引一定要创建成唯一索引，否则会出现多个重载方法之间无法同时被访问的问题。重载方法的话建议把参数类型也加上。)。当某条记录被加上排他锁之后，其他线程无法再在该行记录上增加排他锁。我们可以认为获得排他锁的线程即可获得分布式锁，当获取到锁之后，可以执行方法的业务逻辑，执行完方法之后，通过connection.commit()操作来释放锁。这种方法可以有效的解决上面提到的无法释放锁和阻塞锁的问题。阻塞锁？ for update语句会在执行成功后立即返回，在执行失败时一直处于阻塞状态，直到成功。锁定之后服务宕机，无法释放？使用这种方式，服务宕机之后数据库会自己把锁释放掉。但是还是无法直接解决数据库单点和可重入问题。这里还可能存在另外一个问题，虽然我们对方法字段名使用了唯一索引，并且显示使用 for update 来使用行级锁。但是，MySQL 会对查询进行优化，即便在条件中使用了索引字段，但是否使用索引来检索数据是由 MySQL 通过判断不同执行计划的代价来决定的，如果 MySQL 认为全表扫效率更高，比如对一些很小的表，它就不会使用索引，这种情况下 InnoDB 将使用表锁，而不是行锁。如果发生这种情况就悲剧了。。。还有一个问题，就是我们要使用排他锁来进行分布式锁的 lock，那么一个排他锁长时间不提交，就会占用数据库连接。一旦类似的连接变得多了，就可能把数据库连接池撑爆。优缺点优点：简单，易于理解缺点：会有各种各样的问题（操作数据库需要一定的开销，使用数据库的行级锁并不一定靠谱，性能不靠谱）基于 Redis 做分布式锁使用redis的setNX命令实现分布式锁实现的原理Redis为单进程单线程模式，采用队列模式将并发访问变成串行访问，且多客户端对Redis的连接并不存在竞争关系。redis的SETNX命令可以方便的实现分布式锁。基本命令解析1）setNX（SET if Not eXists）语法：1SETNX key value将 key 的值设为 value ，当且仅当 key 不存在。若给定的 key 已经存在，则 SETNX 不做任何动作。SETNX 是『SET if Not eXists』(如果不存在，则 SET)的简写返回值：设置成功，返回 1 。设置失败，返回 0 。所以我们使用执行下面的命令1SETNX lock.foo &lt;current Unix time + lock timeout + 1&gt;如返回1，则该客户端获得锁，把lock.foo的键值设置为时间值表示该键已被锁定，该客户端最后可以通过DEL lock.foo来释放该锁。如返回0，表明该锁已被其他客户端取得，这时我们可以先返回或进行重试等对方完成或等待锁超时。2）getSET语法：1GETSET key value将给定 key 的值设为 value ，并返回 key 的旧值(old value)。当 key 存在但不是字符串类型时，返回一个错误。返回值：返回给定 key 的旧值。当 key 没有旧值时，也即是， key 不存在时，返回 nil 。3）get语法：1GET key返回值：当 key 不存在时，返回 nil ，否则，返回 key 的值。如果 key 不是字符串类型，那么返回一个错误解决死锁问题如果一个持有锁的客户端失败或崩溃了不能释放锁，该怎么解决？设置超时时间来解决参考博文Java分布式锁看这篇就够了","categories":[{"name":"分布式","slug":"分布式","permalink":"http://www.fufan.me/categories/分布式/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"http://www.fufan.me/tags/分布式/"},{"name":"锁","slug":"锁","permalink":"http://www.fufan.me/tags/锁/"}]},{"title":"Kafka学习笔记（三）——kafka设计的要点","slug":"Kafka学习笔记（三）——kafka设计的要点","date":"2018-04-02T10:36:00.000Z","updated":"2018-11-08T10:37:39.328Z","comments":true,"path":"2018/04/02/Kafka学习笔记（三）——kafka设计的要点/","link":"","permalink":"http://www.fufan.me/2018/04/02/Kafka学习笔记（三）——kafka设计的要点/","excerpt":"","text":"kafka的特点1、吞吐量高吞吐是kafka需要实现的核心目标之一，为此kafka做了以下一些设计：数据磁盘持久化：消息不在内存中cache，直接写入到磁盘，充分利用磁盘的顺序读写性能zero-copy：减少IO操作步骤数据批量发送数据压缩Topic划分为多个partition，提高parallelism2、负载均衡producer根据用户指定的算法，将消息发送到指定的partition存在多个partiiton，每个partition有自己的replica，每个replica分布在不同的Broker节点上多个partition需要选取出lead partition，lead partition负责读写，并由zookeeper负责fail over通过zookeeper管理broker与consumer的动态加入与离开3、拉取系统由于kafka broker会持久化数据，broker没有内存压力，因此，consumer非常适合采取pull的方式消费数据，具有以下几点好处：简化kafka设计consumer根据消费能力自主控制消息拉取速度consumer根据自身情况自主选择消费模式，例如批量，重复消费，从尾端开始消费等4、可扩展性当需要增加broker结点时，新增的broker会向zookeeper注册，而producer及consumer会根据注册在zookeeper上的watcher感知这些变化，并及时作出调整。kafka的使用场景1、消息队列比起大多数的消息系统来说，Kafka有更好的吞吐量，内置的分区，冗余及容错性，这让Kafka成为了一个很好的大规模消息处理应用的解决方案。消息系统一般吞吐量相对较低，但是需要更小的端到端延时，并尝尝依赖于Kafka提供的强大的持久性保障。在这个领域，Kafka足以媲美传统消息系统，如ActiveMR或RabbitMQ。2、行为跟踪Kafka的另一个应用场景是跟踪用户浏览页面、搜索及其他行为，以发布-订阅的模式实时记录到对应的topic里。那么这些结果被订阅者拿到后，就可以做进一步的实时处理，或实时监控，或放到hadoop/离线数据仓库里处理。3、元信息监控作为操作记录的监控模块来使用，即汇集记录一些操作信息，可以理解为运维性质的数据监控吧。4、日志收集日志收集方面，其实开源产品有很多，包括Scribe、Apache Flume。很多人使用Kafka代替日志聚合（log aggregation）。日志聚合一般来说是从服务器上收集日志文件，然后放到一个集中的位置（文件服务器或HDFS）进行处理。然而Kafka忽略掉文件的细节，将其更清晰地抽象成一个个日志或事件的消息流。这就让Kafka处理过程延迟更低，更容易支持多数据源和分布式数据处理。比起以日志为中心的系统比如Scribe或者Flume来说，Kafka提供同样高效的性能和因为复制导致的更高的耐用性保证，以及更低的端到端延迟。kafka性能为了使得Kafka的吞吐率可以线性提高，物理上把Topic分成一个或多个Partition，每个Partition在物理上对应一个文件夹，该文件夹下存储这个Partition的所有消息和索引文件.每个分区都是有序的，不可变的记录序列，不断追加到结构化的提交日志中。分区中的记录每个分配一个连续的id号，称为offset(偏移量)，用于唯一标识分区内的每条记录。实际上，保留在每个消费者基础上的唯一元数据是该消费者在日志中的抵消或位置。这个偏移量是由消费者控制的：消费者通常会在读取记录时线性地推进其偏移量，但实际上，由于位置由消费者控制，因此它可以按任何喜欢的顺序消费记录。例如，消费者可以重置为较旧的offset(偏移量)以重新处理来自过去的数据，或者跳至最近的记录并从“now”开始消费。随你喜欢爱怎么读怎么读,而且这些操作对集群或其他消费者没有太大影响。这样的操作也就说kafka不用考虑加锁的问题,不存在消费完就要删除信息的问题,有效的保证了高吞吐率,这样没有锁竞争，充分发挥了横向的扩展性，吞吐量极高。这也就形成了分布式消费的概念。这里要注意，因为Kafka读取特定消息的时间复杂度为O(1)，即与文件大小无关，所以这里删除过期文件与提高Kafka性能无关,当然kafka也提供了删除旧数据的策略:时间,可以自己设置一个储存的最大时间.partition大小,可以给分区设置最大储存值.consumerp0=&gt;p3三个partition,而partition中的每个message只能被组（Consumer group）中的一个consumer（consumer 线程)消费.也就说一个partition只能被一个消费者消费（一个消费者可以同时消费多个partition）如果consumer从多个partition读到数据，不保证数据间的顺序性，kafka只保证在一个partition上数据是有序的，但多个partition，根据你读的顺序会有不同producerKakfa Broker Leader选举kafka集群是受zookeeper来管理的,这里需要将所有的kafka broker节点一起注册到zookeeper上,而这个过程中只有一个kafka broker能注册成功,在zookeeper上注册一个临时节点,这个kafka broker叫kafka broker Controller,其他的叫kafka broker follower,一旦这个kafka broker Controller发生宕机,临时节点会消失,其他的kafka broker follower会在竞争去zookeeper上注册,产生一个新Leader.(注:Kafka集群中broker之间的关系,不是主从关系，各个broker在集群中地位一样，我们可以随意的增加或删除任何一个broker节点。),还有一种情况是有Controller下的一个follower宕机了,这时Controller会去读取这个follower在zookeeper上所有的partition leader信息(host:port),并且找到这些partition的备份们,让他们选一个成为这个partition的leader.如果该partition的所有的备份都宕机了，则将新的leader设置为-1，等待恢复，等待任一个备份“活”过来，并且选它作为Leader.在Producer向kafka broker推送messagekafka在所有broker中产生一个controller，所有Partition的Leader选举都由controller决定。controller会将Leader的改变直接通过RPC的方式（比Zookeeper Queue的方式更高效）通知需为此作出响应的Broker。每个partition(分区)都有一台服务器充当“leader”，零个或多个服务器充当“follower”。leader处理分区的所有读取和写入请求，而follower被动地复制leader。如果leader失败，其中一个follower将自动成为新leader。每个服务器都充当其中一些分区的leader和其他人的follower，因此负载在集群内平衡良好。举个栗子消息生产者,就是向 kafka broker发消息的客户端。 Producer 采用异步 push 方式, 极大提高 Kafka 系统的吞吐率(可以通过参数控制是采用同步还是异步方式)。 producer 端 , 可以将消息 buffer 起来 , 当消息的条数达到一定阀值时 , 批量发送给 broker 。小数据 IO 太多,会拖慢整体的网络延迟,批量延迟发送事实上提升了网络效率。不过 这也有一定的隐患,比如说当 producer 失效时,那些尚未发送的消息将会丢失。producer将会和Topic下所有partition leader保持 socket 连接 ; 消息由 producer 直接 通过 socket 发送到 broker, 中间不会经过任何 ” 路由层 “. 事实上 , 消息被路由到哪个 partition 上 , 由 producer 客户端决定。 partition leader的位置 (host:port)注册在 zookeeper 中 ,producer 作为 zookeeper client,已经注册了 watch 用来监听 partition leader的变更事件。如上图kafka集群有四个broker,一个topic有四个partition,并且每一个partition都有一个follower(其实就是备份);一个消息流输入之后会先储存一个topic在不同的partition leader中(并行写入),然后在由partition leader同步到各自的备份中.我们加两个broker5,6,这个时候partition的变化partition(分区)机制的优势:当Producer发送消息到broker时，会根据Paritition机制选择将其存储到哪一个Partition。也就是我们上面说的机制，所有消息可以均匀分布到不同的Partition里，这样就实现了负载均衡。如果一个Topic对应一个文件，那这个文件所在的机器I/O将会成为这个Topic的性能瓶颈，而有了Partition后，不同的消息可以并行写入不同broker的不同Partition里，极大的提高了吞吐率。所以说kafka可以水平扩展，也就是扩展partition。segment一个partition可以实现跨服务器,可以一个分区占有一个服务器.参考博文日志收集为什么用kafka","categories":[{"name":"kafka","slug":"kafka","permalink":"http://www.fufan.me/categories/kafka/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://www.fufan.me/tags/kafka/"}]},{"title":"Kafka学习笔记（二）——Kafka shell命令","slug":"Kafka学习笔记（二）——Kafka-shell命令","date":"2018-03-26T10:17:00.000Z","updated":"2018-11-08T10:18:12.049Z","comments":true,"path":"2018/03/26/Kafka学习笔记（二）——Kafka-shell命令/","link":"","permalink":"http://www.fufan.me/2018/03/26/Kafka学习笔记（二）——Kafka-shell命令/","excerpt":"","text":"排查问题的时候可能会用到终端的一些命令，下面列举一下常用的一些命令启动zookeeper1bin/zookeeper-server-start.sh config/zookeeper.properties &amp;启动kafka1bin/kafka-server-start.sh config/server.properties &amp;停止kafka1bin/kafka-server-stop.sh停止zookeeper1bin/zookeeper-server-stop.sh创建topic1bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test展示topic1bin/kafka-topics.sh --list --zookeeper localhost:2181描述topic1bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic生产者1bin/kafka-console-producer.sh --broker-list 130.51.23.95:9092 --topic my-replicated-topic消费者1bin/kafka-console-consumer.sh --zookeeper 130.51.23.95:2181 --topic test --from-beginning","categories":[],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://www.fufan.me/tags/kafka/"}]},{"title":"Kafka学习笔记（一）——Kafka入门","slug":"Kafka学习笔记（一）——Kafka入门","date":"2018-03-18T10:32:00.000Z","updated":"2018-11-08T10:17:19.016Z","comments":true,"path":"2018/03/18/Kafka学习笔记（一）——Kafka入门/","link":"","permalink":"http://www.fufan.me/2018/03/18/Kafka学习笔记（一）——Kafka入门/","excerpt":"","text":"Apache Kafka：一个分布式流处理平台| 对比指标 | kafka | activemq | rabbitmq | rocketmq || — | — | — | — | — || 背景 | Kafka 是LinkedIn 开发的一个高性能、分布式的消息系统，广泛用于日志收集、流式数据处理、在线和离线消息分发等场景 | ActiveMQ是一种开源的，实现了JMS1.1规范的，面向消息(MOM)的中间件， 为应用程序提供高效的、可扩展的、稳定的和安全的企业级消息通信。 | RabbitMQ是一个由erlang开发的AMQP协议（Advanced Message Queue ）的开源实现。 | RocketMQ是阿里巴巴在2012年开源的分布式消息中间件，目前已经捐赠给Apache基金会，已经于2016年11月成为 Apache 孵化项目 ||开发语言 | Java、Scala | Java | Erlang | Java ||协议支持 | 自己实现的一套 | JMS协议 | AMQP | JMS、MQTT ||持久化 | 支持 | 支持 | 支持 | 支持 || producer容错 | 在kafka中提供了acks配置选项, acks=0 生产者在成功写入悄息之前不会等待任何来自服务器的响应 acks=1 只要集群的首领节点收到消息，生产者就会收到一个来自服务器的成功响应 acks=all 只有当所有参与复制的节点全部收到消息时，生产者才会收到一个来自服务器的成功响应,这种模式最安全 | 发送失败后即可重试 | 有ack模型。 ack模型可能重复消息 ，事务模型保证完全一致 | 和kafka类似 || 吞吐量 | kafka具有高的吞吐量，内部采用消息的批量处理，zero-copy机制，数据的存储和获取是本地磁盘顺序批量操作，具有O(1)的复杂度，消息处理的效率很高 | | rabbitMQ在吞吐量方面稍逊于kafka，他们的出发点不一样，rabbitMQ支持对消息的可靠的传递，支持事务，不支持批量的操作；基于存储的可靠性的要求存储可以采用内存或者硬盘。 | kafka在topic数量不多的情况下吞吐量比rocketMq高，在topic数量多的情况下rocketMq比kafka高 || 负载均衡 | kafka采用zookeeper对集群中的broker、consumer进行管理，可以注册topic到zookeeper上；通过zookeeper的协调机制，producer保存对应topic的broker信息，可以随机或者轮询发送到broker上；并且producer可以基于语义指定分片，消息发送到broker的某分片上 | | rabbitMQ的负载均衡需要单独的loadbalancer进行支持 | NamerServer进行负载均衡|相关名词Producer :消息生产者，向Broker发送消息的客户端Consumer :消息消费者，从Broker读取消息的客户端,消费者&lt;=消息的分区数量broker :消息中间件处理节点，一个Kafka节点就是一个broker，一个或者多个Broker可以组成一个Kafka集群topic : 主题，Kafka根据topic对消息进行归类，发布到Kafka集群的每条消息都需要指定一个topicPartition : 分区，物理上的概念，一个topic可以分为多个partition，每个partition内部是有序的，kafka默认根据key%partithon确定消息发送到具体的partitionConsumerGroup : 每个Consumer属于一个特定的Consumer Group，一条消息可以发送到多个不同的Consumer Group，但是一个Consumer Group中只能有一个Consumer能够消费该消息Topic 和 Partition一个Topic中的消息会按照指定的规则(默认是key的hash值%分区的数量，当然你也可以自定义)，发送到某一个分区上面；每一个分区都是一个顺序的、不可变的消息队列，并且可以持续的添加。分区中的消息都被分了一个序列号，称之为偏移量(offset)，在每个分区中此偏移量都是唯一的消费者所持有的元数据就是这个偏移量，也就是消费者在这个log（分区）中的位置。这个偏移量由消费者控制：正常情况当消费者消费消息的时候，偏移量也线性的的增加Consumer 和 Partition通常来讲，消息模型可以分为两种， 队列和发布-订阅式。队列的处理方式 是一个消费者组从队列的一端拉取数据，这个数据消费完就没了。在发布-订阅模型中，消息被广播给所有的消费者，接受到消息的消费者都能处理此消息。在Kafka模型中抽象出来了：消费者组（consumer group）消费者组（consumer group）：每个组中有若干个消费者，如果所有的消费者都在一个组中，那么这个就变成了队列模型；如果笑消费者在不同的组中，这就成了发布-订阅模型一个分区里面的数据只会由一个分组中的消费者处理，同分组的其他消费者不会重复处理消费者组中的消费者数量&lt;=分区数量，如果大于分区数量，多出来的消费者会处于收不到消息的状态，造成不必要的浪费。","categories":[{"name":"kafka","slug":"kafka","permalink":"http://www.fufan.me/categories/kafka/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://www.fufan.me/tags/kafka/"}]},{"title":"Consul学习笔记（一）——安装和命令使用","slug":"Consul学习笔记（一）——安装和命令使用","date":"2018-02-28T10:14:00.000Z","updated":"2018-11-08T10:14:33.306Z","comments":true,"path":"2018/02/28/Consul学习笔记（一）——安装和命令使用/","link":"","permalink":"http://www.fufan.me/2018/02/28/Consul学习笔记（一）——安装和命令使用/","excerpt":"","text":"由于公司目前工作当中微服务用到了consul集群来作为分布式系统中间件，用到了他内置了服务注册与发现框 架、分布一致性协议实现、健康检查、Key/Value存储、多数据中心方案。不再需要依赖其他工具（比如ZooKeeper等）。使用起来也较 为简单。简介Consul 是 HashiCorp 公司推出的开源工具，用于实现分布式系统的服务发现与配置。与其他分布式服务注册与发现的方案，Consul的方案更“一站式”，内置了服务注册与发现框 架、分布一致性协议实现、健康检查、Key/Value存储、多数据中心方案，不再需要依赖其他工具（比如ZooKeeper等）。使用起来也较 为简单。Consul使用Go语言编写，因此具有天然可移植性(支持Linux、windows和Mac OS X)；安装包仅包含一个可执行文件，方便部署，与Docker等轻量级容器可无缝配合 。Service Discovery (服务发现)Health Check (健康检查)Multi Datacenter (多数据中心)Key/Value Storage安装mac：64bit（查看mac位数：打开终端–&gt;”uname -a”）consul_0.6.4_darwin_amd64.zip和consul_0.6.4_web_ui.zip，从consul官网https://www.consul.io/downloads.html进行下载就好（选择好OS和位数）1、解压consul_0.6.4_darwin_amd64.zip2、将解压后的二进制文件consul（上边画红框的部分拷贝到/usr/local/bin下）1sudo scp consul /usr/local/bin/说明：使用sudo是因为权限问题。3、查看是否安装成功调用其命令12345678910111213141516171819202122232425262728 fufan@fufans-MacBook-Pro-2  ~  consul helpUsage: consul [--version] [--help] &lt;command&gt; [&lt;args&gt;]Available commands are: agent Runs a Consul agent catalog Interact with the catalog connect Interact with Consul Connect event Fire a new event exec Executes a command on Consul nodes force-leave Forces a member of the cluster to enter the &quot;left&quot; state info Provides debugging information for operators. intention Interact with Connect service intentions join Tell Consul agent to join cluster keygen Generates a new encryption key keyring Manages gossip layer encryption keys kv Interact with the key-value store leave Gracefully leaves the Consul cluster and shuts down lock Execute a command holding a lock maint Controls node or service maintenance mode members Lists the members of a Consul cluster monitor Stream logs from a Consul agent operator Provides cluster-level tools for Consul operators reload Triggers the agent to reload configuration files rtt Estimates network round trip time between nodes snapshot Saves, restores and inspects snapshots of Consul server state validate Validate config files/directories version Prints the Consul version watch Watch for changes in Consulconsul相关知识点AgentAgent 是一个守护进程运行在Consul集群的每个成员上有Client 和 Server 两种模式所有Agent都可以被调用DNS或者HTTP API,并负责检查和维护同步ClientClient 将所有RPC请求转发至ServerClient 是相对无状态的Client 唯一做的就是参与LAN Gossip PoolClient 只消耗少量的资源和少量的网络带宽Server参与 Raft quorum(一致性判断)响应RPC查询请求维护集群的状态转发查询到Leader 或 远程数据中心Datacenter数据中心私有的低延迟高带宽Consensus (一致性)Consul 使用consensus protocol 来提供CAP(一致性,高可用,分区容错性)Gossip一种协议: 用来保证 最终一致性 , 即: 无法保证在某个时刻, 所有节点状态一致, 但可以保证”最终”一致注册服务服务可以通过提供服务定义或通过对HTTP API进行适当的调用来注册。服务定义是注册服务最常用的方式，所以我们将在这一步中使用这种方法。 我们将建立在上一步中介绍的代理配置。首先，为Consul配置创建一个目录。 Consul将所有配置文件加载到配置目录中，因此Unix系统上的一个通用约定是将目录命名为/etc/consul.d（.d后缀意味着“该目录包含一组配置文件”）。建立服务配置目录:mkdir /etc/consul.d添加文件:echo ‘{“service”: {“name”: “web”, “tags”: [“rails”], “port”: 80}}’ | sudo tee /etc/consul.d/web.json以开发模式启动:consul agent -dev -config-dir=/etc/consul.d以服务方式启动:consul agent -server -bootstrap-expect 2 -data-dir ./tmp/consul -node=n1 -bind=192.168.109.241 -ui-dir ./dist -dc=dc1以客户端方式启动:consul agent -data-dir ./tmp/consul -ui-dir ./dist -bind=192.168.109.204 -dc=dc1加入集群将新节点添加到集群:consul join 192.168.100.101(其中101这个节点是master)显示成员:consul members查看UI管理页面http://192.168.0.70:8500/ui常用命令参数consul agent 命令的常用选项，如下：-data-dir作用：指定agent储存状态的数据目录这是所有agent都必须的对于server尤其重要，因为他们必须持久化集群的状态-config-dir作用：指定service的配置文件和检查定义所在的位置通常会指定为”某一个路径/consul.d”（通常情况下，.d表示一系列配置文件存放的目录）-config-file作用：指定一个要装载的配置文件该选项可以配置多次，进而配置多个配置文件（后边的会合并前边的，相同的值覆盖）-dev作用：创建一个开发环境下的server节点该参数配置下，不会有任何持久化操作，即不会有任何数据写入到磁盘这种模式不能用于生产环境（因为第二条）-bootstrap-expect作用：该命令通知consul server我们现在准备加入的server节点个数，该参数是为了延迟日志复制的启动直到我们指定数量的server节点成功的加入后启动。-node作用：指定节点在集群中的名称该名称在集群中必须是唯一的（默认采用机器的host）推荐：直接采用机器的IP-bind作用：指明节点的IP地址有时候不指定绑定IP，会报Failed to get advertise address: Multiple private IPs found. Please configure one. 的异常-server作用：指定节点为server每个数据中心（DC）的server数推荐至少为1，至多为5所有的server都采用raft一致性算法来确保事务的一致性和线性化，事务修改了集群的状态，且集群的状态保存在每一台server上保证可用性server也是与其他DC交互的门面（gateway）-client作用：指定节点为client，指定客户端接口的绑定地址，包括：HTTP、DNS、RPC默认是127.0.0.1，只允许回环接口访问若不指定为-server，其实就是-client-join作用：将节点加入到集群-datacenter（老版本叫-dc，-dc已经失效）作用：指定机器加入到哪一个数据中心中搭建集群此部分过程同redis集群、zookeeper集群类似问题踩坑googlegithub issues参考博文：Consul官方文档：https://www.consul.io/intro/getting-started/install.htmlConsul 系列博文：http://www.cnblogs.com/java-zhao/archive/2016/04/13/5387105.html使用consul实现分布式服务注册和发现：http://www.tuicool.com/articles/M3QFven","categories":[{"name":"consul","slug":"consul","permalink":"http://www.fufan.me/categories/consul/"}],"tags":[{"name":"consul","slug":"consul","permalink":"http://www.fufan.me/tags/consul/"}]},{"title":"Redis学习笔记（二）——redis安装","slug":"Redis学习笔记（二）——redis安装","date":"2018-02-15T03:05:00.000Z","updated":"2018-11-08T10:13:22.293Z","comments":true,"path":"2018/02/15/Redis学习笔记（二）——redis安装/","link":"","permalink":"http://www.fufan.me/2018/02/15/Redis学习笔记（二）——redis安装/","excerpt":"","text":"由于公司开发笔记本用的是mac，所以我这里介绍一下在mac和linux环境下的redis安装Linux1、下载源码，解压缩后编译源码。1234 $ wget http://download.redis.io/releases/redis-2.8.3.tar.gz$ tar xzf redis-2.8.3.tar.gz$ cd redis-2.8.3$ make2、编译完成后，在Src目录下，有四个可执行文件redis-server、redis-benchmark、redis-cli和redis.conf。然后拷贝到一个目录下。123456mkdir /usr/rediscp redis-server /usr/rediscp redis-benchmark /usr/rediscp redis-cli /usr/rediscp redis.conf /usr/rediscd /usr/redis3、启动Redis服务。1$ redis-server redis.conf4、然后用客户端测试一下是否启动成功。12345$ redis-cliredis&gt; set foo barOKredis&gt; get foo&quot;bar&quot;Macbrew 安装1、使用brew命令安装redis1brew install redis2、启动redis后台方式启动，brew services start redis。这样启动的好处是把控制台关掉后，redis仍然是启动的。当然，如果没有这样的需求，也可以这样启动1redis-server /usr/local/etc/redis.conf3、关闭redis1brew services stop redis4、使用控制台连接redis123redis-cliredis-cli -h 127.0.0.1 -predis.conf配置说明123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426# Redis配置文件样例# Note on units: when memory size is needed, it is possible to specifiy# it in the usual form of 1k 5GB 4M and so forth:## 1k =&gt; 1000 bytes# 1kb =&gt; 1024 bytes# 1m =&gt; 1000000 bytes# 1mb =&gt; 1024*1024 bytes# 1g =&gt; 1000000000 bytes# 1gb =&gt; 1024*1024*1024 bytes## units are case insensitive so 1GB 1Gb 1gB are all the same.# Redis默认不是以守护进程的方式运行，可以通过该配置项修改，使用yes启用守护进程# 启用守护进程后，Redis会把pid写到一个pidfile中，在/var/run/redis.piddaemonize no# 当Redis以守护进程方式运行时，Redis默认会把pid写入/var/run/redis.pid文件，可以通过pidfile指定pidfile /var/run/redis.pid# 指定Redis监听端口，默认端口为6379# 如果指定0端口，表示Redis不监听TCP连接port 6379# 绑定的主机地址# 你可以绑定单一接口，如果没有绑定，所有接口都会监听到来的连接# bind 127.0.0.1# Specify the path for the unix socket that will be used to listen for# incoming connections. There is no default, so Redis will not listen# on a unix socket when not specified.## unixsocket /tmp/redis.sock# unixsocketperm 755# 当客户端闲置多长时间后关闭连接，如果指定为0，表示关闭该功能timeout 0# 指定日志记录级别，Redis总共支持四个级别：debug、verbose、notice、warning，默认为verbose# debug (很多信息, 对开发／测试比较有用)# verbose (many rarely useful info, but not a mess like the debug level)# notice (moderately verbose, what you want in production probably)# warning (only very important / critical messages are logged)loglevel verbose# 日志记录方式，默认为标准输出，如果配置为redis为守护进程方式运行，而这里又配置为标准输出，则日志将会发送给/dev/nulllogfile stdout# To enable logging to the system logger, just set &apos;syslog-enabled&apos; to yes,# and optionally update the other syslog parameters to suit your needs.# syslog-enabled no# Specify the syslog identity.# syslog-ident redis# Specify the syslog facility. Must be USER or between LOCAL0-LOCAL7.# syslog-facility local0# 设置数据库的数量，默认数据库为0，可以使用select &lt;dbid&gt;命令在连接上指定数据库id# dbid是从0到‘databases’-1的数目databases 16################################ SNAPSHOTTING ################################## 指定在多长时间内，有多少次更新操作，就将数据同步到数据文件，可以多个条件配合# Save the DB on disk:## save &lt;seconds&gt; &lt;changes&gt;## Will save the DB if both the given number of seconds and the given# number of write operations against the DB occurred.## 满足以下条件将会同步数据:# 900秒（15分钟）内有1个更改# 300秒（5分钟）内有10个更改# 60秒内有10000个更改# Note: 可以把所有“save”行注释掉，这样就取消同步操作了save 900 1save 300 10save 60 10000# 指定存储至本地数据库时是否压缩数据，默认为yes，Redis采用LZF压缩，如果为了节省CPU时间，可以关闭该选项，但会导致数据库文件变的巨大rdbcompression yes# 指定本地数据库文件名，默认值为dump.rdbdbfilename dump.rdb# 工作目录.# 指定本地数据库存放目录，文件名由上一个dbfilename配置项指定# # Also the Append Only File will be created inside this directory.# # 注意，这里只能指定一个目录，不能指定文件名dir ./################################# REPLICATION ################################## 主从复制。使用slaveof从 Redis服务器复制一个Redis实例。注意，该配置仅限于当前slave有效# so for example it is possible to configure the slave to save the DB with a# different interval, or to listen to another port, and so on.# 设置当本机为slav服务时，设置master服务的ip地址及端口，在Redis启动时，它会自动从master进行数据同步# slaveof &lt;masterip&gt; &lt;masterport&gt;# 当master服务设置了密码保护时，slav服务连接master的密码# 下文的“requirepass”配置项可以指定密码# masterauth &lt;master-password&gt;# When a slave lost the connection with the master, or when the replication# is still in progress, the slave can act in two different ways:## 1) if slave-serve-stale-data is set to &apos;yes&apos; (the default) the slave will# still reply to client requests, possibly with out of data data, or the# data set may just be empty if this is the first synchronization.## 2) if slave-serve-stale data is set to &apos;no&apos; the slave will reply with# an error &quot;SYNC with master in progress&quot; to all the kind of commands# but to INFO and SLAVEOF.#slave-serve-stale-data yes# Slaves send PINGs to server in a predefined interval. It&apos;s possible to change# this interval with the repl_ping_slave_period option. The default value is 10# seconds.## repl-ping-slave-period 10# The following option sets a timeout for both Bulk transfer I/O timeout and# master data or ping response timeout. The default value is 60 seconds.## It is important to make sure that this value is greater than the value# specified for repl-ping-slave-period otherwise a timeout will be detected# every time there is low traffic between the master and the slave.## repl-timeout 60################################## SECURITY #################################### Warning: since Redis is pretty fast an outside user can try up to# 150k passwords per second against a good box. This means that you should# use a very strong password otherwise it will be very easy to break.# 设置Redis连接密码，如果配置了连接密码，客户端在连接Redis时需要通过auth &lt;password&gt;命令提供密码，默认关闭# requirepass foobared# Command renaming.## It is possilbe to change the name of dangerous commands in a shared# environment. For instance the CONFIG command may be renamed into something# of hard to guess so that it will be still available for internal-use# tools but not available for general clients.## Example:## rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52## It is also possilbe to completely kill a command renaming it into# an empty string:## rename-command CONFIG &quot;&quot;################################### LIMITS ##################################### 设置同一时间最大客户端连接数，默认无限制，Redis可以同时打开的客户端连接数为Redis进程可以打开的最大文件描述符数，# 如果设置maxclients 0，表示不作限制。当客户端连接数到达限制时，Redis会关闭新的连接并向客户端返回max Number of clients reached错误信息# maxclients 128# Don&apos;t use more memory than the specified amount of bytes.# When the memory limit is reached Redis will try to remove keys with an# EXPIRE set. It will try to start freeing keys that are going to expire# in little time and preserve keys with a longer time to live.# Redis will also try to remove objects from free lists if possible.## If all this fails, Redis will start to reply with errors to commands# that will use more memory, like SET, LPUSH, and so on, and will continue# to reply to most read-only commands like GET.## WARNING: maxmemory can be a good idea mainly if you want to use Redis as a# &apos;state&apos; server or cache, not as a real DB. When Redis is used as a real# database the memory usage will grow over the weeks, it will be obvious if# it is going to use too much memory in the long run, and you&apos;ll have the time# to upgrade. With maxmemory after the limit is reached you&apos;ll start to get# errors for write operations, and this may even lead to DB inconsistency.# 指定Redis最大内存限制，Redis在启动时会把数据加载到内存中，达到最大内存后，Redis会先尝试清除已到期或即将到期的Key，# 当此方法处理后，仍然到达最大内存设置，将无法再进行写入操作，但仍然可以进行读取操作。# Redis新的vm机制，会把Key存放内存，Value会存放在swap区# maxmemory &lt;bytes&gt;# MAXMEMORY POLICY: how Redis will select what to remove when maxmemory# is reached? You can select among five behavior:# # volatile-lru -&gt; remove the key with an expire set using an LRU algorithm# allkeys-lru -&gt; remove any key accordingly to the LRU algorithm# volatile-random -&gt; remove a random key with an expire set# allkeys-&gt;random -&gt; remove a random key, any key# volatile-ttl -&gt; remove the key with the nearest expire time (minor TTL)# noeviction -&gt; don&apos;t expire at all, just return an error on write operations# # Note: with all the kind of policies, Redis will return an error on write# operations, when there are not suitable keys for eviction.## At the date of writing this commands are: set setnx setex append# incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd# sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby# zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby# getset mset msetnx exec sort## The default is:## maxmemory-policy volatile-lru# LRU and minimal TTL algorithms are not precise algorithms but approximated# algorithms (in order to save memory), so you can select as well the sample# size to check. For instance for default Redis will check three keys and# pick the one that was used less recently, you can change the sample size# using the following configuration directive.## maxmemory-samples 3############################## APPEND ONLY MODE ################################ # Note that you can have both the async dumps and the append only file if you# like (you have to comment the &quot;save&quot; statements above to disable the dumps).# Still if append only mode is enabled Redis will load the data from the# log file at startup ignoring the dump.rdb file.# 指定是否在每次更新操作后进行日志记录，Redis在默认情况下是异步的把数据写入磁盘，如果不开启，可能会在断电时导致一段时间内的数据丢失。# 因为redis本身同步数据文件是按上面save条件来同步的，所以有的数据会在一段时间内只存在于内存中。默认为no# IMPORTANT: Check the BGREWRITEAOF to check how to rewrite the append# log file in background when it gets too big.appendonly no# 指定更新日志文件名，默认为appendonly.aof# appendfilename appendonly.aof# The fsync() call tells the Operating System to actually write data on disk# instead to wait for more data in the output buffer. Some OS will really flush # data on disk, some other OS will just try to do it ASAP.# 指定更新日志条件，共有3个可选值：# no:表示等操作系统进行数据缓存同步到磁盘（快）# always:表示每次更新操作后手动调用fsync()将数据写到磁盘（慢，安全）# everysec:表示每秒同步一次（折衷，默认值）appendfsync everysec# appendfsync no# When the AOF fsync policy is set to always or everysec, and a background# saving process (a background save or AOF log background rewriting) is# performing a lot of I/O against the disk, in some Linux configurations# Redis may block too long on the fsync() call. Note that there is no fix for# this currently, as even performing fsync in a different thread will block# our synchronous write(2) call.## In order to mitigate this problem it&apos;s possible to use the following option# that will prevent fsync() from being called in the main process while a# BGSAVE or BGREWRITEAOF is in progress.## This means that while another child is saving the durability of Redis is# the same as &quot;appendfsync none&quot;, that in pratical terms means that it is# possible to lost up to 30 seconds of log in the worst scenario (with the# default Linux settings).# # If you have latency problems turn this to &quot;yes&quot;. Otherwise leave it as# &quot;no&quot; that is the safest pick from the point of view of durability.no-appendfsync-on-rewrite no# Automatic rewrite of the append only file.# Redis is able to automatically rewrite the log file implicitly calling# BGREWRITEAOF when the AOF log size will growth by the specified percentage.# # This is how it works: Redis remembers the size of the AOF file after the# latest rewrite (or if no rewrite happened since the restart, the size of# the AOF at startup is used).## This base size is compared to the current size. If the current size is# bigger than the specified percentage, the rewrite is triggered. Also# you need to specify a minimal size for the AOF file to be rewritten, this# is useful to avoid rewriting the AOF file even if the percentage increase# is reached but it is still pretty small.## Specify a precentage of zero in order to disable the automatic AOF# rewrite feature.auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb################################## SLOW LOG #################################### The Redis Slow Log is a system to log queries that exceeded a specified# execution time. The execution time does not include the I/O operations# like talking with the client, sending the reply and so forth,# but just the time needed to actually execute the command (this is the only# stage of command execution where the thread is blocked and can not serve# other requests in the meantime).# # You can configure the slow log with two parameters: one tells Redis# what is the execution time, in microseconds, to exceed in order for the# command to get logged, and the other parameter is the length of the# slow log. When a new command is logged the oldest one is removed from the# queue of logged commands.# The following time is expressed in microseconds, so 1000000 is equivalent# to one second. Note that a negative number disables the slow log, while# a value of zero forces the logging of every command.slowlog-log-slower-than 10000# There is no limit to this length. Just be aware that it will consume memory.# You can reclaim memory used by the slow log with SLOWLOG RESET.slowlog-max-len 1024################################ VIRTUAL MEMORY ################################## WARNING! Virtual Memory is deprecated in Redis 2.4### The use of Virtual Memory is strongly discouraged.### WARNING! Virtual Memory is deprecated in Redis 2.4### The use of Virtual Memory is strongly discouraged.# Virtual Memory allows Redis to work with datasets bigger than the actual# amount of RAM needed to hold the whole dataset in memory.# In order to do so very used keys are taken in memory while the other keys# are swapped into a swap file, similarly to what operating systems do# with memory pages.# 指定是否启用虚拟内存机制，默认值为no，# VM机制将数据分页存放，由Redis将访问量较少的页即冷数据swap到磁盘上，访问多的页面由磁盘自动换出到内存中# 把vm-enabled设置为yes，根据需要设置好接下来的三个VM参数，就可以启动VM了vm-enabled no# vm-enabled yes# This is the path of the Redis swap file. As you can guess, swap files# can&apos;t be shared by different Redis instances, so make sure to use a swap# file for every redis process you are running. Redis will complain if the# swap file is already in use.## Redis交换文件最好的存储是SSD（固态硬盘）# 虚拟内存文件路径，默认值为/tmp/redis.swap，不可多个Redis实例共享# *** WARNING *** if you are using a shared hosting the default of putting# the swap file under /tmp is not secure. Create a dir with access granted# only to Redis user and configure Redis to create the swap file there.vm-swap-file /tmp/redis.swap# With vm-max-memory 0 the system will swap everything it can. Not a good# default, just specify the max amount of RAM you can in bytes, but it&apos;s# better to leave some margin. For instance specify an amount of RAM# that&apos;s more or less between 60 and 80% of your free RAM.# 将所有大于vm-max-memory的数据存入虚拟内存，无论vm-max-memory设置多少，所有索引数据都是内存存储的（Redis的索引数据就是keys）# 也就是说当vm-max-memory设置为0的时候，其实是所有value都存在于磁盘。默认值为0vm-max-memory 0# Redis swap文件分成了很多的page，一个对象可以保存在多个page上面，但一个page上不能被多个对象共享，vm-page-size是要根据存储的数据大小来设定的。# 建议如果存储很多小对象，page大小最后设置为32或64bytes；如果存储很大的对象，则可以使用更大的page，如果不确定，就使用默认值vm-page-size 32# 设置swap文件中的page数量由于页表（一种表示页面空闲或使用的bitmap）是存放在内存中的，在磁盘上每8个pages将消耗1byte的内存# swap空间总容量为 vm-page-size * vm-pages## With the default of 32-bytes memory pages and 134217728 pages Redis will# use a 4 GB swap file, that will use 16 MB of RAM for the page table.## It&apos;s better to use the smallest acceptable value for your application,# but the default is large in order to work in most conditions.vm-pages 134217728# Max number of VM I/O threads running at the same time.# This threads are used to read/write data from/to swap file, since they# also encode and decode objects from disk to memory or the reverse, a bigger# number of threads can help with big objects even if they can&apos;t help with# I/O itself as the physical device may not be able to couple with many# reads/writes operations at the same time.# 设置访问swap文件的I/O线程数，最后不要超过机器的核数，如果设置为0，那么所有对swap文件的操作都是串行的，可能会造成比较长时间的延迟，默认值为4vm-max-threads 4############################### ADVANCED CONFIG ################################ Hashes are encoded in a special way (much more memory efficient) when they# have at max a given numer of elements, and the biggest element does not# exceed a given threshold. You can configure this limits with the following# configuration directives.# 指定在超过一定的数量或者最大的元素超过某一临界值时，采用一种特殊的哈希算法hash-max-zipmap-entries 512hash-max-zipmap-value 64# Similarly to hashes, small lists are also encoded in a special way in order# to save a lot of space. The special representation is only used when# you are under the following limits:list-max-ziplist-entries 512list-max-ziplist-value 64# Sets have a special encoding in just one case: when a set is composed# of just strings that happens to be integers in radix 10 in the range# of 64 bit signed integers.# The following configuration setting sets the limit in the size of the# set in order to use this special memory saving encoding.set-max-intset-entries 512# Similarly to hashes and lists, sorted sets are also specially encoded in# order to save a lot of space. This encoding is only used when the length and# elements of a sorted set are below the following limits:zset-max-ziplist-entries 128zset-max-ziplist-value 64# Active rehashing uses 1 millisecond every 100 milliseconds of CPU time in# order to help rehashing the main Redis hash table (the one mapping top-level# keys to values). The hash table implementation redis uses (see dict.c)# performs a lazy rehashing: the more operation you run into an hash table# that is rhashing, the more rehashing &quot;steps&quot; are performed, so if the# server is idle the rehashing is never complete and some more memory is used# by the hash table.# # The default is to use this millisecond 10 times every second in order to# active rehashing the main dictionaries, freeing memory when possible.## If unsure:# use &quot;activerehashing no&quot; if you have hard latency requirements and it is# not a good thing in your environment that Redis can reply form time to time# to queries with 2 milliseconds delay.# 指定是否激活重置哈希，默认为开启activerehashing yes################################## INCLUDES #################################### 指定包含其他的配置文件，可以在同一主机上多个Redis实例之间使用同一份配置文件，而同时各实例又拥有自己的特定配置文件# include /path/to/local.conf# include /path/to/other.conf","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://www.fufan.me/tags/redis/"}]},{"title":"Redis学习笔记（一）——初识redis","slug":"Redis学习笔记（一）——初识redis","date":"2018-02-07T13:02:00.000Z","updated":"2018-11-08T10:11:15.679Z","comments":true,"path":"2018/02/07/Redis学习笔记（一）——初识redis/","link":"","permalink":"http://www.fufan.me/2018/02/07/Redis学习笔记（一）——初识redis/","excerpt":"","text":"由于加入公司一段时间了，公司的爬虫项目太过于依赖redis这个中间件，包括线上的几次重大异常都是由redis来引起的（redis分布式锁问题，阿里云redis主从和分布式服务异常问题等）。所以找时间专门学习一下关于redis的一些使用。Redis学习链接：菜鸟redis教程官网教程Redis简介Redis是一个速度极快的非关系数据库，也就是我们所说的NoSQL数据库(non-relational database)，它可以存储键(key)与5种不同类型的值(value)之间的映射(mapping)，可以将存储在内存的键值对数据持久化到硬盘，可以使用复制特性来扩展读性能，还可以使用客户端分片来扩展性能，并且它还提供了多种语言的API。Redis 是完全开源免费的，遵守BSD协议，是一个高性能的key-value数据库。Redis支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用。Redis支持数据的备份，即master-slave模式的数据备份。优势性能极高： Redis能读的速度是110000次/s,写的速度是81000次/s丰富的数据类型：Redis支持二进制案例的 Strings, Lists, Hashes, Sets 及 Ordered Sets 数据类型操作。原子性：Redis的所有操作都是原子性的，同时Redis还支持对几个操作全并后的原子性执行。丰富的特性：支持 publish/subscribe, 通知, key 过期等等特性。分布式锁：很多分布式系统中可以用redis的setnx和getset来做分布式锁数据结构字符串 - Stringstring类型是二进制安全的。意思是redis的string可以包含任何数据。比如jpg图片或者序列化的对象 。string类型是Redis最基本的数据类型，一个键最大能存储512MB。api示例：12345127.0.0.1:6379&gt; set username fufanOK127.0.0.1:6379&gt; get username&quot;fufan&quot;127.0.0.1:6379&gt;哈希 - hashRedis hash 是一个键值对集合。Redis hash是一个string类型的field和value的映射表，hash特别适合用于存储对象。每个 hash 可以存储 232 - 1 键值对（40多亿）。api示例：1234567127.0.0.1:6379&gt; HSET user name fufan password 121314(integer) 2127.0.0.1:6379&gt; HGETALL user1) &quot;name&quot;2) &quot;fufan&quot;3) &quot;password&quot;4) &quot;121314&quot;列表 - listRedis 列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素导列表的头部（左边）或者尾部（右边）。列表最多可存储 2^32 - 1 元素 (4294967295, 每个列表可存储40多亿)。api示例：1234567891011127.0.0.1:6379&gt; LPUSH userlist fufan(integer) 1127.0.0.1:6379&gt; LPUSH userlist yajun(integer) 2127.0.0.1:6379&gt; LPUSH userlist luwei(integer) 3127.0.0.1:6379&gt; LPOP userlist&quot;luwei&quot;127.0.0.1:6379&gt; RPOP userlist&quot;fufan&quot;127.0.0.1:6379&gt;集合 - setRedis的Set是string类型的无序集合。集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是O(1)根据集合内元素的唯一性，第二次插入的元素将被忽略。集合中最大的成员数为 2^32 -1(4294967295,每个集合可存储40多亿个成员)12345678910127.0.0.1:6379&gt; sadd product phone(integer) 1127.0.0.1:6379&gt; sadd product pad(integer) 1127.0.0.1:6379&gt; sadd product tv(integer) 1127.0.0.1:6379&gt; SMEMBERS product1) &quot;tv&quot;2) &quot;phone&quot;3) &quot;pad&quot;有序集合 - sorted setRedis zset 和 set一样也是string类型元素的集合,且不允许重复的成员。不同的是每个元素都会关联一个double类型的分数。redis正是通过分数来为集合中的成员进行从小到大的排序。zset的成员是唯一的,但分数(score)却可以重复。1234567891011121314151617127.0.0.1:6379&gt; ZADD fufan 100 chinese(integer) 1127.0.0.1:6379&gt; ZADD fufan 100 math(integer) 1127.0.0.1:6379&gt; ZADD fufan 135 english(integer) 1127.0.0.1:6379&gt; ZRANGE fufan 0 10 withscores1) &quot;chinese&quot;2) &quot;100&quot;3) &quot;math&quot;4) &quot;100&quot;5) &quot;english&quot;6) &quot;135&quot;127.0.0.1:6379&gt; ZRANGEBYSCORE fufan 100 1001) &quot;chinese&quot;2) &quot;math&quot;127.0.0.1:6379&gt;","categories":[{"name":"redis","slug":"redis","permalink":"http://www.fufan.me/categories/redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://www.fufan.me/tags/redis/"}]},{"title":"JVM专题（三）——GC算法 垃圾收集器","slug":"JVM专题（三）——GC算法-垃圾收集器","date":"2017-11-20T16:30:00.000Z","updated":"2018-11-08T10:38:47.127Z","comments":true,"path":"2017/11/21/JVM专题（三）——GC算法-垃圾收集器/","link":"","permalink":"http://www.fufan.me/2017/11/21/JVM专题（三）——GC算法-垃圾收集器/","excerpt":"","text":"这篇文件将给大家介绍GC都有哪几种算法，以及JVM都有那些垃圾回收器，它们的工作原理。概述垃圾收集 Garbage Collection 通常被称为“GC”，它诞生于1960年 MIT 的 Lisp 语言，经过半个多世纪，目前已经十分成熟了。 jvm 中，程序计数器、虚拟机栈、本地方法栈都是随线程而生随线程而灭，栈帧随着方法的进入和退出做入栈和出栈操作，实现了自动的内存清理，因此，我们的内存垃圾回收主要集中于 java 堆和方法区中，在程序运行期间，这部分内存的分配和使用都是动态的.对象存活判断判断对象是否存活一般有两种方式：引用计数：每个对象有一个引用计数属性，新增一个引用时计数加1，引用释放时计数减1，计数为0时可以回收。此方法简单，无法解决对象相互循环引用的问题。可达性分析（Reachability Analysis）：从GC Roots开始向下搜索，搜索所走过的路径称为引用链。当一个对象到GC Roots没有任何引用链相连时，则证明此对象是不可用的。不可达对象。在Java语言中，GC Roots包括：虚拟机栈中引用的对象。方法区中类静态属性实体引用的对象。方法区中常量引用的对象。本地方法栈中JNI引用的对象。垃圾收集算法标记 -清除算法“标记-清除”（Mark-Sweep）算法，如它的名字一样，算法分为“标记”和“清除”两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收掉所有被标记的对象。之所以说它是最基础的收集算法，是因为后续的收集算法都是基于这种思路并对其缺点进行改进而得到的。它的主要缺点有两个：一个是效率问题，标记和清除过程的效率都不高；另外一个是空间问题，标记清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致，当程序在以后的运行过程中需要分配较大对象时无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。复制算法“复制”（Copying）的收集算法，它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。这样使得每次都是对其中的一块进行内存回收，内存分配时也就不用考虑内存碎片等复杂情况，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效。只是这种算法的代价是将内存缩小为原来的一半，持续复制长生存期的对象则导致效率降低。标记-压缩算法复制收集算法在对象存活率较高时就要执行较多的复制操作，效率将会变低。更关键的是，如果不想浪费50%的空间，就需要有额外的空间进行分配担保，以应对被使用的内存中所有对象都100%存活的极端情况，所以在老年代一般不能直接选用这种算法。根据老年代的特点，有人提出了另外一种“标记-整理”（Mark-Compact）算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存分代收集算法GC分代的基本假设：绝大部分对象的生命周期都非常短暂，存活时间短。“分代收集”（Generational Collection）算法，把Java堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。在新生代中，每次垃圾收集时都发现有大批对象死去，只有少量存活，那就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集。而老年代中因为对象存活率高、没有额外空间对它进行分配担保，就必须使用“标记-清理”或“标记-整理”算法来进行回收。垃圾收集器1如果说收集算法是内存回收的方法论，垃圾收集器就是内存回收的具体实现Serial收集器串行收集器是最古老，最稳定以及效率高的收集器，可能会产生较长的停顿，只使用一个线程去回收。新生代、老年代使用串行回收；新生代复制算法、老年代标记-压缩；垃圾收集的过程中会Stop The World（服务暂停）参数控制：-XX:+UseSerialGC 串行收集器ParNew收集器ParNew收集器其实就是Serial收集器的多线程版本。新生代并行，老年代串行；新生代复制算法、老年代标记-压缩参数控制：-XX:+UseParNewGC ParNew收集器-XX:ParallelGCThreads 限制线程数量Parallel收集器Parallel Scavenge收集器类似ParNew收集器，Parallel收集器更关注系统的吞吐量。可以通过参数来打开自适应调节策略，虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或最大的吞吐量；也可以通过参数控制GC的时间不大于多少毫秒或者比例；新生代复制算法、老年代标记-压缩参数控制：-XX:+UseParallelGC 使用Parallel收集器+ 老年代串行Parallel Old 收集器Parallel Old是Parallel Scavenge收集器的老年代版本，使用多线程和“标记－整理”算法。这个收集器是在JDK 1.6中才开始提供参数控制： -XX:+UseParallelOldGC 使用Parallel收集器+ 老年代并行CMS收集器CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。目前很大一部分的Java应用都集中在互联网站或B/S系统的服务端上，这类应用尤其重视服务的响应速度，希望系统停顿时间最短，以给用户带来较好的体验。从名字（包含“Mark Sweep”）上就可以看出CMS收集器是基于“标记-清除”算法实现的，它的运作过程相对于前面几种收集器来说要更复杂一些，整个过程分为4个步骤，包括：初始标记（CMS initial mark）并发标记（CMS concurrent mark）重新标记（CMS remark）并发清除（CMS concurrent sweep）其中初始标记、重新标记这两个步骤仍然需要“Stop The World”。初始标记仅仅只是标记一下GC Roots能直接关联到的对象，速度很快，并发标记阶段就是进行GC Roots Tracing的过程，而重新标记阶段则是为了修正并发标记期间，因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段稍长一些，但远比并发标记的时间短。由于整个过程中耗时最长的并发标记和并发清除过程中，收集器线程都可以与用户线程一起工作，所以总体上来说，CMS收集器的内存回收过程是与用户线程一起并发地执行。老年代收集器（新生代使用ParNew）优点: 并发收集、低停顿缺点: 产生大量空间碎片、并发阶段会降低吞吐量参数控制：-XX:+UseConcMarkSweepGC 使用CMS收集器-XX:+ UseCMSCompactAtFullCollection Full GC后，进行一次碎片整理；整理过程是独占的，会引起停顿时间变长-XX:+CMSFullGCsBeforeCompaction 设置进行几次Full GC后，进行一次碎片整理-XX:ParallelCMSThreads 设定CMS的线程数量（一般情况约等于可用CPU数量）G1收集器G1是目前技术发展的最前沿成果之一，HotSpot开发团队赋予它的使命是未来可以替换掉JDK1.5中发布的CMS收集器。与CMS收集器相比G1收集器有以下特点：空间整合，G1收集器采用标记整理算法，不会产生内存空间碎片。分配大对象时不会因为无法找到连续空间而提前触发下一次GC。可预测停顿，这是G1的另一大优势，降低停顿时间是G1和CMS的共同关注点，但G1除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为N毫秒的时间片段内，消耗在垃圾收集上的时间不得超过N毫秒，这几乎已经是实时Java（RTSJ）的垃圾收集器的特征了。上面提到的垃圾收集器，收集的范围都是整个新生代或者老年代，而G1不再是这样。使用G1收集器时，Java堆的内存布局与其他收集器有很大差别，它将整个Java堆划分为多个大小相等的独立区域（Region），虽然还保留有新生代和老年代的概念，但新生代和老年代不再是物理隔阂了，它们都是一部分（可以不连续）Region的集合。G1的新生代收集跟ParNew类似，当新生代占用达到一定比例的时候，开始出发收集。和CMS类似，G1收集器收集老年代对象会有短暂停顿。收集步骤：1、标记阶段，首先初始标记(Initial-Mark),这个阶段是停顿的(Stop the World Event)，并且会触发一次普通Mintor GC。对应GC log:GC pause (young) (inital-mark)2、Root Region Scanning，程序运行过程中会回收survivor区(存活到老年代)，这一过程必须在young GC之前完成。3、Concurrent Marking，在整个堆中进行并发标记(和应用程序并发执行)，此过程可能被young GC中断。在并发标记阶段，若发现区域对象中的所有对象都是垃圾，那个这个区域会被立即回收(图中打X)。同时，并发标记过程中，会计算每个区域的对象活性(区域中存活对象的比例)。4、Remark, 再标记，会有短暂停顿(STW)。再标记阶段是用来收集 并发标记阶段 产生新的垃圾(并发阶段和应用程序一同运行)；G1中采用了比CMS更快的初始快照算法:snapshot-at-the-beginning (SATB)。5、Copy/Clean up，多线程清除失活对象，会有STW。G1将回收区域的存活对象拷贝到新区域，清除Remember Sets，并发清空回收区域并把它返回到空闲区域链表中。6、复制/清除过程后。回收区域的活性对象已经被集中回收到深蓝色和深绿色区域。常用的收集器组合服务器新生代GC策略老年老代GC策略说明组合1SerialSerial OldSerial和Serial Old都是单线程进行GC，特点就是GC时暂停所有应用线程。组合2SerialCMS+Serial OldCMS（Concurrent Mark Sweep）是并发GC，实现GC线程和应用线程并发工作，不需要暂停所有应用线程。另外，当CMS进行GC失败时，会自动使用Serial Old策略进行GC。组合3ParNewCMS使用-XX:+UseParNewGC选项来开启。ParNew是Serial的并行版本，可以指定GC线程数，默认GC线程数为CPU的数量。可以使用-XX:ParallelGCThreads选项指定GC的线程数。如果指定了选项-XX:+UseConcMarkSweepGC选项，则新生代默认使用ParNew GC策略。组合4ParNewSerial Old使用-XX:+UseParNewGC选项来开启。新生代使用ParNew GC策略，年老代默认使用Serial Old GC策略。组合5Parallel ScavengeSerial OldParallel Scavenge策略主要是关注一个可控的吞吐量：应用程序运行时间 / (应用程序运行时间 + GC时间)，可见这会使得CPU的利用率尽可能的高，适用于后台持久运行的应用程序，而不适用于交互较多的应用程序。组合6Parallel ScavengeParallel OldParallel Old是Serial Old的并行版本组合7G1GCG1GC-XX:+UnlockExperimentalVMOptions -XX:+UseG1GC #开启；-XX:MaxGCPauseMillis =50 #暂停时间目标；-XX:GCPauseIntervalMillis =200 #暂停间隔目标；-XX:+G1YoungGenSize=512m #年轻代大小；-XX:SurvivorRatio=6 #幸存区比例系统吞吐量和系统并发数以及响时间的关系理解","categories":[{"name":"java虚拟机","slug":"java虚拟机","permalink":"http://www.fufan.me/categories/java虚拟机/"}],"tags":[{"name":"java虚拟机","slug":"java虚拟机","permalink":"http://www.fufan.me/tags/java虚拟机/"}]},{"title":"JVM专题（二）——JVM内存模型","slug":"JVM专题（二）——JVM内存模型","date":"2017-11-18T19:30:00.000Z","updated":"2018-11-08T10:38:30.873Z","comments":true,"path":"2017/11/19/JVM专题（二）——JVM内存模型/","link":"","permalink":"http://www.fufan.me/2017/11/19/JVM专题（二）——JVM内存模型/","excerpt":"","text":"所有的Java开发人员可能会遇到这样的困惑？我该为堆内存设置多大空间呢？OutOfMemoryError的异常到底涉及到运行时数据的哪块区域？该怎么解决呢？其实如果你经常解决服务器性能问题，那么这些问题就会变的非常常见，了解JVM内存也是为了服务器出现性能问题的时候可以快速的了解那块的内存区域出现问题，以便于快速的解决生产故障。先看一张图，这张图能很清晰的说明JVM内存结构布局。JVM内存结构主要有三大块：堆内存、方法区和栈。堆内存是JVM中最大的一块由年轻代和老年代组成，而年轻代内存又被分成三部分，Eden空间、From Survivor空间、To Survivor空间,默认情况下年轻代按照8:1:1的比例来分配；方法区存储类信息、常量、静态变量等数据，是线程共享的区域，为与Java堆区分，方法区还有一个别名Non-Heap(非堆)；栈又分为java虚拟机栈和本地方法栈主要用于方法的执行。在通过一张图来了解如何通过参数来控制各区域的内存大小控制参数-Xms设置堆的最小空间大小。-Xmx设置堆的最大空间大小。-XX:NewSize设置新生代最小空间大小。-XX:MaxNewSize设置新生代最大空间大小。-XX:PermSize设置永久代最小空间大小。-XX:MaxPermSize设置永久代最大空间大小。-Xss设置每个线程的堆栈大小。没有直接设置老年代的参数，但是可以设置堆空间大小和新生代空间大小两个参数来间接控制。老年代空间大小=堆空间大小-年轻代大空间大小从更高的一个维度再次来看JVM和系统调用之间的关系方法区和对是所有线程共享的内存区域；而java栈、本地方法栈和程序员计数器是运行是线程私有的内存区域。下面我们详细介绍每个区域的作用Java堆（Heap）对于大多数应用来说，Java堆（Java Heap）是Java虚拟机所管理的内存中最大的一块。Java堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配内存。Java堆是垃圾收集器管理的主要区域，因此很多时候也被称做“GC堆”。如果从内存回收的角度看，由于现在收集器基本都是采用的分代收集算法，所以Java堆中还可以细分为：新生代和老年代；再细致一点的有Eden空间、From Survivor空间、To Survivor空间等。根据Java虚拟机规范的规定，Java堆可以处于物理上不连续的内存空间中，只要逻辑上是连续的即可，就像我们的磁盘空间一样。在实现时，既可以实现成固定大小的，也可以是可扩展的，不过当前主流的虚拟机都是按照可扩展来实现的（通过-Xmx和-Xms控制）。如果在堆中没有内存完成实例分配，并且堆也无法再扩展时，将会抛出OutOfMemoryError异常。方法区（Method Area）方法区（Method Area）与Java堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然Java虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做Non-Heap（非堆），目的应该是与Java堆区分开来。对于习惯在HotSpot虚拟机上开发和部署程序的开发者来说，很多人愿意把方法区称为“永久代”（Permanent Generation），本质上两者并不等价，仅仅是因为HotSpot虚拟机的设计团队选择把GC分代收集扩展至方法区，或者说使用永久代来实现方法区而已。Java虚拟机规范对这个区域的限制非常宽松，除了和Java堆一样不需要连续的内存和可以选择固定大小或者可扩展外，还可以选择不实现垃圾收集。相对而言，垃圾收集行为在这个区域是比较少出现的，但并非数据进入了方法区就如永久代的名字一样“永久”存在了。这个区域的内存回收目标主要是针对常量池的回收和对类型的卸载，一般来说这个区域的回收“成绩”比较难以令人满意，尤其是类型的卸载，条件相当苛刻，但是这部分区域的回收确实是有必要的。根据Java虚拟机规范的规定，当方法区无法满足内存分配需求时，将抛出OutOfMemoryError异常。方法区有时被称为持久代（PermGen）。所有的对象在实例化后的整个运行周期内，都被存放在堆内存中。堆内存又被划分成不同的部分：伊甸区(Eden)，幸存者区域(Survivor Sapce)，老年代（Old Generation Space）。方法的执行都是伴随着线程的。原始类型的本地变量以及引用都存放在线程栈中。而引用关联的对象比如String，都存在在堆中。程序计数器（Program Counter Register）程序计数器（Program Counter Register）是一块较小的内存空间，它的作用可以看做是当前线程所执行的字节码的行号指示器。在虚拟机的概念模型里（仅是概念模型，各种虚拟机可能会通过一些更高效的方式去实现），字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。由于Java虚拟机的多线程是通过线程轮流切换并分配处理器执行时间的方式来实现的，在任何一个确定的时刻，一个处理器（对于多核处理器来说是一个内核）只会执行一条线程中的指令。因此，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各条线程之间的计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。如果线程正在执行的是一个Java方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址；如果正在执行的是Natvie方法，这个计数器值则为空（Undefined）。JVM栈（JVM Stacks）与程序计数器一样，Java虚拟机栈（Java Virtual Machine Stacks）也是线程私有的，它的生命周期与线程相同。虚拟机栈描述的是Java方法执行的内存模型：每个方法被执行的时候都会同时创建一个栈帧（Stack Frame）用于存储局部变量表、操作栈、动态链接、方法出口等信息。每一个方法被调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。局部变量表存放了编译期可知的各种基本数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference类型，它不等同于对象本身，根据不同的虚拟机实现，它可能是一个指向对象起始地址的引用指针，也可能指向一个代表对象的句柄或者其他与此对象相关的位置）和returnAddress类型（指向了一条字节码指令的地址）。其中64位长度的long和double类型的数据会占用2个局部变量空间（Slot），其余的数据类型只占用1个。局部变量表所需的内存空间在编译期间完成分配，当进入一个方法时，这个方法需要在帧中分配多大的局部变量空间是完全确定的，在方法运行期间不会改变局部变量表的大小。在Java虚拟机规范中，对这个区域规定了两种异常状况：如果线程请求的栈深度大于虚拟机所允许的深度，将抛出StackOverflowError异常；如果虚拟机栈可以动态扩展（当前大部分的Java虚拟机都可动态扩展，只不过Java虚拟机规范中也允许固定长度的虚拟机栈），当扩展时无法申请到足够的内存时会抛出OutOfMemoryError异常。本地方法栈（Native Method Stacks）本地方法栈（Native Method Stacks）与虚拟机栈所发挥的作用是非常相似的，其区别不过是虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则是为虚拟机使用到的Native方法服务。虚拟机规范中对本地方法栈中的方法使用的语言、使用方式与数据结构并没有强制规定，因此具体的虚拟机可以自由实现它。甚至有的虚拟机（譬如Sun HotSpot虚拟机）直接就把本地方法栈和虚拟机栈合二为一。与虚拟机栈一样，本地方法栈区域也会抛出StackOverflowError和OutOfMemoryError异常。哪儿的OutOfMemoryError对内存结构清晰的认识同样可以帮助理解不同OutOfMemoryErrors：1Exception in thread “main”: java.lang.OutOfMemoryError: Java heap space原因：对象不能被分配到堆内存中1Exception in thread “main”: java.lang.OutOfMemoryError: PermGen space原因：类或者方法不能被加载到持久代。它可能出现在一个程序加载很多类的时候，比如引用了很多第三方的库；1Exception in thread “main”: java.lang.OutOfMemoryError: Requested array size exceeds VM limit原因：创建的数组大于堆内存的空间1Exception in thread “main”: java.lang.OutOfMemoryError: request &lt;size&gt; bytes for &lt;reason&gt;. Out of swap space?原因：分配本地分配失败。JNI、本地库或者Java虚拟机都会从本地堆中分配内存空间。1Exception in thread “main”: java.lang.OutOfMemoryError: &lt;reason&gt; &lt;stack trace&gt;（Native method）原因：同样是本地方法内存分配失败，只不过是JNI或者本地方法或者Java虚拟机发现","categories":[{"name":"java虚拟机","slug":"java虚拟机","permalink":"http://www.fufan.me/categories/java虚拟机/"}],"tags":[{"name":"java虚拟机","slug":"java虚拟机","permalink":"http://www.fufan.me/tags/java虚拟机/"}]},{"title":"Java面试总结积累（基础篇）之JVM问题(三)","slug":"Java面试总结积累（基础篇）之JVM问题-三","date":"2017-11-13T11:44:00.000Z","updated":"2018-11-07T02:01:43.507Z","comments":true,"path":"2017/11/13/Java面试总结积累（基础篇）之JVM问题-三/","link":"","permalink":"http://www.fufan.me/2017/11/13/Java面试总结积累（基础篇）之JVM问题-三/","excerpt":"","text":"一. 垃圾回收算法的实现原理垃圾回收算法有一下几种：1. 引用计数法对于一个A对象，只要有任何一个对象引用了A，则A的引用计算器就加1，当引用失效时，引用计数器减1.只要A的引用计数器值为0，则对象A就不可能再被使用。存在两个问题：1.无法处理循环引用的问题，因此在Java的垃圾回收器中，没有使用该算法；2.伴随一个加法操作和减法操作，对系统性能会有一定的影响。2. 标记清除法标记清除法是现代垃圾回收算法的思想基础。分为两个阶段：标记阶段和清除阶段。缺点是可能产生的最大的问题就是空间碎片。标记清除算法先通过根节点标记所有可达对象，然后清除所有不可达对象，完成垃圾回收。3. 复制算法将原有的内存空间分为两块相同的存储空间，每次只使用一块，在垃圾回收时，将正在使用的内存块中存活对象复制到未使用的那一块内存空间中，之后清除正在使用的内存块中的所有对象，完成垃圾回收。在java中的新生代串行垃圾回收器中，使用了复制算法的思想，新生代分为eden空间、from空间和to空间3个部分，其中from和to空间可以看做用于复制的两块大小相同、可互换角色的内存空间块（同一时间只能有一个被当做当前内存空间使用，另一个在垃圾回收时才发挥作用），from和to空间也称为survivor空间，用于存放未被回收的对象。在java中的新生代串行垃圾回收器中，使用了复制算法的思想，新生代分为eden空间、from空间和to空间3个部分，其中from和to空间可以看做用于复制的两块大小相同、可互换角色的内存空间块（同一时间只能有一个被当做当前内存空间使用，另一个在垃圾回收时才发挥作用），from和to空间也称为survivor空间，用于存放未被回收的对象。4. 标记压缩算法类似标记清除算法，也是先将可达对象标记，然后扫描的时候，在清除不可达对象之前，先做了一步压缩到内存空间的一端的操作，减少了内存空间中的碎片。这样做避免的碎片的产生，又不需要两块相同的内存空间，因此性价比高。5. 分代算法将内存空间根据对象的特点不同进行划分，选择合适的垃圾回收算法，以提高垃圾回收的效率。通常，java虚拟机会将所有的新建对象都放入称为新生代的内存空间。新生代的特点是：对象朝生夕灭，大约90%的对象会很快回收，因此，新生代比较适合使用复制算法。当一个对象经过几次垃圾回收后依然存活，对象就会放入老年代的内存空间，在老年代中，几乎所有的对象都是经过几次垃圾回收后依然得以存活的，因此，认为这些对象在一段时间内，甚至在程序的整个生命周期将是常驻内存的。老年代的存活率是很高的，如果依然使用复制算法回收老年代，将需要复制大量的对象。这种做法是不可取的，根据分代的思想，对老年代的回收使用标记清除或者标记压缩算法可以提高垃圾回收效率。分代的思想被现有的虚拟机广泛使用，几乎所有的垃圾回收器都区分新生代和老年代。6. 分区算法分区算法将整个堆空间划分为连续的不同小区间。每一个小区间都独立使用，独立回收。算法优点是：可以控制一次回收多少个小区间通常，相同的条件下，堆空间越大，一次GC所需的时间就越长，从而产生的停顿时间就越长。为了更好的控制GC产生的停顿时间，将一块大的内存区域分割成多个小块，根据目标的停顿时间，每次合理的回收若干个小区间，而不是整个堆空间，从而减少一个GC的停顿时间。如图所示：二. 当出现了内存溢出，你怎么排错。内存溢出可能存在的情况OOMStack Overflow运行时常量池溢出方法区溢出1. OOM原因:即堆区溢出，对象过多导致内存疯长，大于预设置的最大堆容量后报出java heap space解决方法:1)首先确认是内存泄露(Memory Leak)还是内存溢出(Memory Overflow);2)如果是内存泄漏引起的,查看GC Roots引用链,找出为什么无法被垃圾回收的原因;3)如果是内存溢出,检查虚拟机的堆参数(-Xmx最大值和-Xms最小值),对比物理内存看是否可以调大;4)由于本人在工作中负责过分布式爬虫的项目，发现内存疯长也可能是堆外内存的问题，和并发线程相关。严重的需要使用jheap dump下内存结构进行分析，利用工具jprofile、eclipse等，后续会有专门的专题理一下这一块的处理办法。2. Stack Overflow虚拟机栈和本地方法栈溢出原因:在单线程下,虚拟机栈容量太小或者定义了大量的本地变量,会抛出SO;解决方法:增大虚拟机栈容量，可以通过-Xss参数来设定栈容量;3. PermGen space原因:代码在运行时创建了大量的常量,超出了常量池上限;解决方法:通过修改-XX:PermSize和-XX:MaxPermSize参数来修改方法区大小,从而修改常量池大小;4. 方法区溢出原因:在运行时,ClassLoader动态加载了大量的Class信息,超出方法区上限;解决方法:通过修改-XX:PermSize和-XX:MaxPermSize参数来修改方法区大小;下面罗列一些经常用到的jvm参数:用到的JVM启动参数:-Xss2M 设置JVM栈内存大小-Xms20M 设置堆内存初始值-Xmx20M 设置堆内存最大值-Xmn10M 设置堆内存中新生代大小-XX:SurvivorRatio=8设置堆内存中新生代Eden 和 Survivor 比例-XX:PermSize=10M设置方法区内存初始值-XX:MaxPermSize=10M设置方法区内存最大值三. JVM内存模型的相关知识了解多少，比如重排序，内存屏障，happen-before，主内存，工作内存等。重排序通常是编译器或运行时环境为了优化程序性能而采取的对指令进行重新排序执行的一种手段。重排序分为两类：编译期重排序和运行期重排序，分别对应编译时和运行时环境。内存屏障（Memory Barrier，或有时叫做内存栅栏，Memory Fence）是一种CPU指令，用于控制特定条件下的重排序和内存可见性问题。Java编译器也会根据内存屏障的规则禁止重排序。主内存存在于主内存中的变量和对象，可以被所有线程共享工作内存只存在于各个线程中，被线程私有，线程要读取主内存变量时，必须拷贝一份主内存中的到工作内存，不能直接从主内存中读取。不同线程之间无法直接访问其他线程工作内存中的变量，线程间变量值的传递需要通过主内存来完成。四. 如何实现内存可见性要实现共享变量的可见性，必须保证两点线程修改后的共享变量值能够及时从工作内存中刷新到主内存中其他线程能够及时把共享变量的最新值从主内存更新到自己的工作内存中synchronized实现可见性线程解锁前，必须把共享变量的最新值刷新到主内存中线程加锁时，将清空工作内存中共享变量的值，从而使用共享变量时需要从主存中重新读取最新的值线程执行互斥代码的过程获得互斥锁清空工作内存从主内存拷贝变量的最新副本到工作内存执行代码将更改后的共享变量的值刷新到主内存中释放互斥锁volatile实现可见性能够保证volatile变量的可见性不能保证volatile变量复合操作的原子原理：通过加入内存屏障和禁止重排序优化来实现的（1.在每个volatile写操作前插入StoreStore屏障，在写操作后插入StoreLoad屏障；2.在每个volatile读操作前插入LoadLoad屏障，在读操作后插入LoadStore屏障）通俗的讲：volatile变量在每次被线程访问时，都强迫从主内存中重读该变量的值，而当该变量发生变化时，又会强迫将最新的值刷新到主内存。这样任何时刻，不同的线程总能看到该变量的最新值。线程写volatile变量的过程：改变线程工作内存中volatile变量副本的值将改变后的副本的值从工作内存刷新到主内存线程读volatile变量的过程：从主内存中读取volatile变量的最新值到线程的工作内存中从工作内存中读取volatile变量的副本synchronized vs volatilevolatile不需要加锁，比synchronized更轻量级，不会阻塞线程synchronized既能保证可见性，又能保证原子性，而volatile只能保证可见性，无法保证原子性五. 简单说说你了解的类加载器，可以打破双亲委派么，怎么打破。JDK 默认提供了如下几种ClassLoaderBootstrp loaderExtClassLoaderAppClassLoader为什么要有三个类加载器，一方面是分工，各自负责各自的区块，另一方面为了实现委托模型。java采用了委托模型机制，这个机制简单来讲，就是“类装载器有载入类的需求时，会先请示其Parent使用其搜索路径帮忙载入，如果Parent 找不到,那么才由自己依照自己的搜索路径搜索类”为什么要使用这种双亲委托模式呢？因为这样可以避免重复加载，当父亲已经加载了该类的时候，就没有必要子ClassLoader再加载一次。考虑到安全因素，我们试想一下，如果不使用这种委托模式，那我们就可以随时使用自定义的String来动态替代java核心api中定义类型，这样会存在非常大的安全隐患，而双亲委托的方式，就可以避免这种情况，因为String已经在启动时被加载，所以用户自定义类是无法加载一个自定义的ClassLoader。/思考：假如我们自己写了一个java.lang.String的类，我们是否可以替换调JDK本身的类？/答案是否定的。我们不能实现。为什么呢？我看很多网上解释是说双亲委托机制解决这个问题，其实不是非常的准确。因为双亲委托机制是可以打破的，你完全可以自己写一个classLoader来加载自己写的java.lang.String类，但是你会发现也不会加载成功，具体就是因为针对java.*开头的类，jvm的实现中已经保证了必须由bootstrp来加载。定义自已的ClassLoader既然JVM已经提供了默认的类加载器，为什么还要定义自已的类加载器呢？因为Java中提供的默认ClassLoader，只加载指定目录下的jar和class，如果我们想加载其它位置的类或jar时，比如：我要加载网络上的一个class文件，通过动态加载到内存之后，要调用这个类中的方法实现我的业务逻辑。在这样的情况下，默认的ClassLoader就不能满足我们的需求了，所以需要定义自己的ClassLoader。定义自已的类加载器分为两步：继承java.lang.ClassLoader重写父类的findClass方法读者可能在这里有疑问，父类有那么多方法，为什么偏偏只重写findClass方法？因为JDK已经在loadClass方法中帮我们实现了ClassLoader搜索类的算法，当在loadClass方法中搜索不到类时，loadClass方法就会调用findClass方法来搜索类，所以我们只需重写该方法即可。如没有特殊的要求，一般不建议重写loadClass搜索类的算法。六. 讲讲JAVA的反射机制。JAVA反射机制是在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法;对于任意一个对象，都能够调用它的任意方法和属性;这种动态获取信息以及动态调用对象方法的功能称为java语言的反射机制。在java中反射是很重要的，在现在的很多框架中也都运用了反射的概念，比如spring中的aop机制就是利用反射原理，动态代理，其实说起动态代理就必须要说反射。在写java我们使用对象的时候一般都是使用new的方式来创建对象，这些将要在程序中使用的对象在编译期间都已经知道了，但是编译期间和运行期间还不一样。假如有类Person，Student类extends了Person（都有空构造函数），Person person=new Stuednt();在编译的时候person是Person类型的，但是在运行的时候确实Student的，有时候我们在程序运行期间根据类去生成相应的对象然后进行一系列的操作，这就是反射，所谓的反射个人理解就是在JVM运行期间通过查找到相应的类，通过类获取其属性以及方法来创造对象。参考博文jvm参数设置大全浅谈CMS垃圾收集器与G1收集器JVM的GC策略java jvm内存管理/gc策略/参数设置","categories":[{"name":"面试","slug":"面试","permalink":"http://www.fufan.me/categories/面试/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://www.fufan.me/tags/jvm/"},{"name":" 面试","slug":"面试","permalink":"http://www.fufan.me/tags/面试/"}]},{"title":"Java面试总结积累（基础篇）之JVM问题(二)","slug":"Java面试总结积累（基础篇）之JVM问题-二","date":"2017-11-10T02:41:00.000Z","updated":"2018-11-06T11:44:21.407Z","comments":true,"path":"2017/11/10/Java面试总结积累（基础篇）之JVM问题-二/","link":"","permalink":"http://www.fufan.me/2017/11/10/Java面试总结积累（基础篇）之JVM问题-二/","excerpt":"","text":"一. JVM中一次完整的GC流程是怎样的，对象如何晋升到老年代，说说你知道的几种主要的JVM参数。GC流程当现在有一个新的对象产生，那么对象一定需要内存空间，于是现在就需要为该对象进行内存空间的申请；首先会判断eden是否有内存空间，如果此时有内存空间，则直接将新对象保存在eden；但是如果此时eden的内存空间不足，那么会自动执行一个MinorGC操作，将eden的无用内存空间进行清理，清理之后会继续判断eden的内存空间是否充足？如果内存空间充足，则将新的对象直接在eden进行空间分配；如果执行了MinorGC之后发现eden的内存依然不足，那么这个时候会进行survivor判断，如果survivor有剩余空间，则将eden的部分活跃对象保存在survivor，那么随后继续判断eden的内存空间是否充足，如果充足，则在eden进行新对象的空间分配；如果此时survivor也已经没有内存空间了，则继续判断老年区，如果此时老年区空间充足，则将survivor中的活跃对象保存到老年代，而后survivor就会存现有空余空间，随后eden将活跃对象保存在survivor之中，而后在eden里为新对象开辟空间；如果这个时候老年代也满了，那么这个时候将产生M ajor GC（FullGC），进行老年代的内存清理。如果老年代执行了Full GC之后发现依然无法进行对象的保存，就会产生OOM异常“OutOfMemoryError”jvm参数-Xmx3550m：设置JVM最大堆内存为3550M。-Xms3550m：设置JVM初始堆内存为3550M。此值可以设置与-Xmx相同，以避免每次垃圾回收完成后JVM重新分配内存。-Xss128k：设置每个线程的栈大小。JDK5.0以后每个线程栈大小为1M，之前每个线程栈大小为256K。应当根据应用的线程所需内存大小进行调整。在相同物理内存下，减小这个值能生成更多的线程。但是操作系统对一个进程内的线程数还是有限制的，不能无限生成，经验值在3000~5000左右。需要注意的是：当这个值被设置的较大（例如&gt;2MB）时将会在很大程度上降低系统的性能。-Xmn2g：设置年轻代大小为2G。在整个堆内存大小确定的情况下，增大年轻代将会减小年老代，反之亦然。此值关系到JVM垃圾回收，对系统性能影响较大，官方推荐配置为整个堆大小的3/8。-XX:NewSize=1024m：设置年轻代初始值为1024M。-XX:MaxNewSize=1024m：设置年轻代最大值为1024M。-XX:PermSize=256m：设置持久代初始值为256M。-XX:MaxPermSize=256m：设置持久代最大值为256M。-XX:NewRatio=4：设置年轻代（包括1个Eden和2个Survivor区）与年老代的比值。表示年轻代比年老代为1:4。-XX:SurvivorRatio=4：设置年轻代中Eden区与Survivor区的比值。表示2个Survivor区（JVM堆内存年轻代中默认有2个大小相等的Survivor区）与1个Eden区的比值为2:4，即1个Survivor区占整个年轻代大小的1/6。-XX:MaxTenuringThreshold=7：表示一个对象如果在Survivor区（救助空间）移动了7次还没有被垃圾回收就进入年老代。如果设置为0的话，则年轻代对象不经过Survivor区，直接进入年老代，对于需要大量常驻内存的应用，这样做可以提高效率。如果将此值设置为一个较大值，则年轻代对象会在Survivor区进行多次复制，这样可以增加对象在年轻代存活时间，增加对象在年轻代被垃圾回收的概率，减少Full GC的频率，这样做可以在某种程度上提高服务稳定性。疑问解答-Xmn，-XX:NewSize/-XX:MaxNewSize，-XX:NewRatio 3组参数都可以影响年轻代的大小，混合使用的情况下，优先级是什么？如下：高优先级：-XX:NewSize/-XX:MaxNewSize中优先级：-Xmn（默认等效 -Xmn=-XX:NewSize=-XX:MaxNewSize=?）低优先级：-XX:NewRatio推荐使用-Xmn参数，原因是这个参数简洁，相当于一次设定 NewSize/MaxNewSIze，而且两者相等，适用于生产环境。-Xmn 配合 -Xms/-Xmx，即可将堆内存布局完成。-Xmn参数是在JDK 1.4 开始支持。二. 你知道哪几种垃圾收集器，各自的优缺点，重点讲下cms和G1，包括原理，流程，优缺点。CMS收集器CMS收集器是一种以获取最短回收停顿时间为目标的收集器。基于“标记-清除”算法实现，它的运作过程如下：1）初始标记2）并发标记3）重新标记4）并发清除初始标记、从新标记这两个步骤仍然需要“stop the world”，初始标记仅仅只是标记一下GC Roots能直接关联到的对象，熟读很快，并发标记阶段就是进行GC Roots Tracing，而重新标记阶段则是为了修正并发标记期间因用户程序继续运作而导致标记产生表动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段稍长点，但远比并发标记的时间短。CMS是一款优秀的收集器，主要优点：并发收集、低停顿。缺点：1）CMS收集器对CPU资源非常敏感。在并发阶段，它虽然不会导致用户线程停顿，但是会因为占用了一部分线程而导致应用程序变慢，总吞吐量会降低。2）CMS收集器无法处理浮动垃圾，可能会出现“Concurrent Mode Failure（并发模式故障）”失败而导致Full GC产生。浮动垃圾：由于CMS并发清理阶段用户线程还在运行着，伴随着程序运行自然就会有新的垃圾不断产生，这部分垃圾出现的标记过程之后，CMS无法在当次收集中处理掉它们，只好留待下一次GC中再清理。这些垃圾就是“浮动垃圾”。3）CMS是一款“标记–清除”算法实现的收集器，容易出现大量空间碎片。当空间碎片过多，将会给大对象分配带来很大的麻烦，往往会出现老年代还有很大空间剩余，但是无法找到足够大的连续空间来分配当前对象，不得不提前触发一次Full GC。G1收集器G1是一款面向服务端应用的垃圾收集器。1、并行于并发：G1能充分利用CPU、多核环境下的硬件优势，使用多个CPU（CPU或者CPU核心）来缩短stop-The-World停顿时间。部分其他收集器原本需要停顿Java线程执行的GC动作，G1收集器仍然可以通过并发的方式让java程序继续执行。2、分代收集：虽然G1可以不需要其他收集器配合就能独立管理整个GC堆，但是还是保留了分代的概念。它能够采用不同的方式去处理新创建的对象和已经存活了一段时间，熬过多次GC的旧对象以获取更好的收集效果。3、空间整合：与CMS的“标记–清理”算法不同，G1从整体来看是基于“标记整理”算法实现的收集器；从局部上来看是基于“复制”算法实现的。4、可预测的停顿：这是G1相对于CMS的另一个大优势，降低停顿时间是G1和ＣＭＳ共同的关注点，但Ｇ１除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为M毫秒的时间片段内，5、G1运作步骤：1、初始标记； 2、并发标记； 3、最终标记； 4、筛选回收； 上面几个步骤的运作过程和CMS有很多相似之处。初始标记阶段仅仅只是标记一下GC Roots能直接关联到的对象，并且修改TAMS的值，让下一个阶段用户程序并发运行时，能在正确可用的Region中创建新对象，这一阶段需要停顿线程，但是耗时很短，并发标记阶段是从GC Root开始对堆中对象进行可达性分析，找出存活的对象，这阶段时耗时较长，但可与用户程序并发执行。而最终标记阶段则是为了修正在并发标记期间因用户程序继续运作而导致标记产生变动的那一部分标记记录，虚拟机将这段时间对象变化记录在线程Remenbered Set Logs里面，最终标记阶段需要把Remembered Set Logs的数据合并到Remembered Set Logs里面，最终标记阶段需要把Remembered Set Logs的数据合并到Remembered Set中，这一阶段需要停顿线程，但是可并行执行。最后在筛选回收阶段首先对各个Region的回收价值和成本进行排序，根据用户所期望的GC停顿时间来制定回收计划。三. GC策略是如何的，有哪些New Generation的GC策略Serial GC。采用单线程方式，用Copying算法。到这里我们再来说说为什么New Generation会再次被划分成Eden Space和S0、S1，相信聪明的你一定已经想到Copying算法所需要的额外内存空间了吧，S0和S1又称为From Space和To Space。具体细节自己好好想想。Parallel Scavenge。将内存空间分段来使用多线程，也是用Copying算法。ParNew。比Parallel Scavenge多做了与Old Generation使用CMS GC一起发生时的特殊处理。Old Generation的GC策略Serial GC。当然也是单线程方式，但是实现是将Mark-Sweep和Mark-Compact结合了下，做了点改进。Parallel Mark-Sweep、Parallel Mark-Compact。同样也是把Old Generation空间进行划分成regions，只是粒度更细了。为什么用这两个算法，不用我赘述了吧。CMS（Concurrent Mark-Sweep） GC。我承认这个GC我真的没怎么看懂，目的是为了实现并发，结果就造成具体实现太麻烦了。有兴趣的朋友去看书吧，文末我说了是哪本书。这里有个地方可以说一下，就是算法使用的还是Mark-Sweep，对于内存碎片的问题，CMS提供了一个内存碎片的整理功能，会在执行几次Full GC以后执行一次。4. JVM的垃圾收集器JVM给出了3种选择：串行收集器、并行收集器、并发收集器。串行收集器只适用于小数据量的情况，所以生产环境的选择主要是并行收集器和并发收集器。默认情况下JDK5.0以前都是使用串行收集器，如果想使用其他收集器需要在启动时加入相应参数。JDK5.0以后，JVM会根据当前系统配置进行智能判断。串行收集器-XX:+UseSerialGC：设置串行收集器。并行收集器（吞吐量优先）-XX:+UseParallelGC：设置为并行收集器。此配置仅对年轻代有效。即年轻代使用并行收集，而年老代仍使用串行收集。-XX:ParallelGCThreads=20：配置并行收集器的线程数，即：同时有多少个线程一起进行垃圾回收。此值建议配置与CPU数目相等。-XX:+UseParallelOldGC：配置年老代垃圾收集方式为并行收集。JDK6.0开始支持对年老代并行收集。-XX:MaxGCPauseMillis=100：设置每次年轻代垃圾回收的最长时间（单位毫秒）。如果无法满足此时间，JVM会自动调整年轻代大小，以满足此时间。-XX:+UseAdaptiveSizePolicy：设置此选项后，并行收集器会自动调整年轻代Eden区大小和Survivor区大小的比例，以达成目标系统规定的最低响应时间或者收集频率等指标。此参数建议在使用并行收集器时，一直打开。并发收集器（响应时间优先）XX:+UseConcMarkSweepGC：即CMS收集，设置年老代为并发收集。CMS收集是JDK1.4后期版本开始引入的新GC算法。它的主要适合场景是对响应时间的重要性需求大于对吞吐量的需求，能够承受垃圾回收线程和应用线程共享CPU资源，并且应用中存在比较多的长生命周期对象。CMS收集的目标是尽量减少应用的暂停时间，减少Full GC发生的几率，利用和应用程序线程并发的垃圾回收线程来标记清除年老代内存。-XX:+UseParNewGC：设置年轻代为并发收集。可与CMS收集同时使用。JDK5.0以上，JVM会根据系统配置自行设置，所以无需再设置此参数。-XX:CMSFullGCsBeforeCompaction=0：由于并发收集器不对内存空间进行压缩和整理，所以运行一段时间并行收集以后会产生内存碎片，内存使用效率降低。此参数设置运行0次Full GC后对内存空间进行压缩和整理，即每次Full GC后立刻开始压缩和整理内存。-XX:+UseCMSCompactAtFullCollection：打开内存空间的压缩和整理，在Full GC后执行。可能会影响性能，但可以消除内存碎片。-XX:+CMSIncrementalMode：设置为增量收集模式。一般适用于单CPU情况。-XX:CMSInitiatingOccupancyFraction=70：表示年老代内存空间使用到70%时就开始执行CMS收集，以确保年老代有足够的空间接纳来自年轻代的对象，避免Full GC的发生。其它垃圾回收参数-XX:+ScavengeBeforeFullGC：年轻代GC优于Full GC执行。-XX:-DisableExplicitGC：不响应 System.gc() 代码。-XX:+UseThreadPriorities：启用本地线程优先级API。即使 java.lang.Thread.setPriority() 生效，不启用则无效。-XX:SoftRefLRUPolicyMSPerMB=0：软引用对象在最后一次被访问后能存活0毫秒（JVM默认为1000毫秒）。-XX:TargetSurvivorRatio=90：允许90%的Survivor区被占用（JVM默认为50%）。提高对于Survivor区的使用率。辅助信息参数设置-XX:-CITime：打印消耗在JIT编译的时间。-XX:ErrorFile=./hs_err_pid.log：保存错误日志或数据到指定文件中。-XX:HeapDumpPath=./java_pid.hprof：指定Dump堆内存时的路径。-XX:-HeapDumpOnOutOfMemoryError：当首次遭遇内存溢出时Dump出此时的堆内存。-XX:OnError=”;”：出现致命ERROR后运行自定义命令。-XX:OnOutOfMemoryError=”;”：当首次遭遇内存溢出时执行自定义命令。-XX:-PrintClassHistogram：按下 Ctrl+Break 后打印堆内存中类实例的柱状信息，同JDK的 jmap -histo 命令。-XX:-PrintConcurrentLocks：按下 Ctrl+Break 后打印线程栈中并发锁的相关信息，同JDK的 jstack -l 命令。-XX:-PrintCompilation：当一个方法被编译时打印相关信息。-XX:-PrintGC：每次GC时打印相关信息。-XX:-PrintGCDetails：每次GC时打印详细信息。-XX:-PrintGCTimeStamps：打印每次GC的时间戳。-XX:-TraceClassLoading：跟踪类的加载信息。-XX:-TraceClassLoadingPreorder：跟踪被引用到的所有类的加载信息。-XX:-TraceClassResolution：跟踪常量池。-XX:-TraceClassUnloading：跟踪类的卸载信息。调优实战1. 大型网站服务器案例承受海量访问的动态Web应用服务器配置：8 CPU, 8G MEM, JDK 1.6.X参数方案：-server -Xmx3550m -Xms3550m -Xmn1256m -Xss128k -XX:SurvivorRatio=6 -XX:MaxPermSize=256m -XX:ParallelGCThreads=8 -XX:MaxTenuringThreshold=0 -XX:+UseConcMarkSweepGC调优说明：-Xmx 与 -Xms 相同以避免JVM反复重新申请内存。-Xmx 的大小约等于系统内存大小的一半，即充分利用系统资源，又给予系统安全运行的空间。-Xmn1256m 设置年轻代大小为1256MB。此值对系统性能影响较大，Sun官方推荐配置年轻代大小为整个堆的3/8。-Xss128k 设置较小的线程栈以支持创建更多的线程，支持海量访问，并提升系统性能。-XX:SurvivorRatio=6 设置年轻代中Eden区与Survivor区的比值。系统默认是8，根据经验设置为6，则2个Survivor区与1个Eden区的比值为2:6，一个Survivor区占整个年轻代的1/8。-XX:ParallelGCThreads=8 配置并行收集器的线程数，即同时8个线程一起进行垃圾回收。此值一般配置为与CPU数目相等。-XX:MaxTenuringThreshold=0 设置垃圾最大年龄（在年轻代的存活次数）。如果设置为0的话，则年轻代对象不经过Survivor区直接进入年老代。对于年老代比较多的应用，可以提高效率；如果将此值设置为一个较大值，则年轻代对象会在Survivor区进行多次复制，这样可以增加对象再年轻代的存活时间，增加在年轻代即被回收的概率。根据被海量访问的动态Web应用之特点，其内存要么被缓存起来以减少直接访问DB，要么被快速回收以支持高并发海量请求，因此其内存对象在年轻代存活多次意义不大，可以直接进入年老代，根据实际应用效果，在这里设置此值为0。-XX:+UseConcMarkSweepGC 设置年老代为并发收集。CMS（ConcMarkSweepGC）收集的目标是尽量减少应用的暂停时间，减少Full GC发生的几率，利用和应用程序线程并发的垃圾回收线程来标记清除年老代内存，适用于应用中存在比较多的长生命周期对象的情况。内部集成构建服务器案例高性能数据处理的工具应用服务器配置：1 CPU, 4G MEM, JDK 1.6.X参数方案：-server -XX:PermSize=196m -XX:MaxPermSize=196m -Xmn320m -Xms768m -Xmx1024m调优说明：-XX:PermSize=196m -XX:MaxPermSize=196m 根据集成构建的特点，大规模的系统编译可能需要加载大量的Java类到内存中，所以预先分配好大量的持久代内存是高效和必要的。-Xmn320m 遵循年轻代大小为整个堆的3/8原则。-Xms768m -Xmx1024m 根据系统大致能够承受的堆内存大小设置即可。参考博文jvm参数设置大全浅谈CMS垃圾收集器与G1收集器JVM的GC策略java jvm内存管理/gc策略/参数设置","categories":[{"name":"面试","slug":"面试","permalink":"http://www.fufan.me/categories/面试/"}],"tags":[{"name":"面试","slug":"面试","permalink":"http://www.fufan.me/tags/面试/"},{"name":"jvm","slug":"jvm","permalink":"http://www.fufan.me/tags/jvm/"}]},{"title":"Java面试总结积累（基础篇）之JVM问题(一)","slug":"Java面试总结积累（基础篇）之JVM问题-一","date":"2017-11-06T11:36:00.000Z","updated":"2018-11-06T11:41:23.861Z","comments":true,"path":"2017/11/06/Java面试总结积累（基础篇）之JVM问题-一/","link":"","permalink":"http://www.fufan.me/2017/11/06/Java面试总结积累（基础篇）之JVM问题-一/","excerpt":"","text":"一. 什么情况下会发生栈内存溢出。栈溢出(StackOverflowError)原因：12栈是每个线程私有的，他的生命周期与线程相同，每个方法在执行的时候都会创建一个栈帧，用来存储局部变量表，操作数栈，动态链接，方法出口灯信息。局部变量表又包含基本数据类型，对象引用类型（局部变量表编译器完成，运行期间不会变化）所以我们可以理解为栈溢出就是方法执行是创建的栈帧超过了栈的深度。解决方法1我们需要使用参数 -Xss 去调整JVM栈的大小二. 什么情况下会发生堆内存溢出。堆溢出(OutOfMemoryError:java heap space)原因1heap space表示堆空间，堆中主要存储的是对象。如果不断的new对象则会导致堆中的空间溢出解决1可以通过 -Xmx4096M 调整堆的总大小三. JVM的内存结构，Eden和Survivor比例。JVM的内存结构jvm将管理的内存中分几块，方法区、堆、虚拟机栈、本地方法栈、程序计数器、运行时常量池线程私有的块有：虚拟机栈、本地方法栈、程序计数器线程共享的块有：方法区、堆、运行时常量池方法区：它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据非堆数据堆：heap是java虚拟机所管理内存中最大的一块，我们创建的对象实例都是存储在这里，它也是垃圾回收的主要区域。现在垃圾回收的算法基本用的是分代收集，虚拟机将其分为新生代和老年代，新生代分为eden和survivor，survivor还可以分为from和to，我们可以通过设置jvm参数的方式来对其进行比例分配。我们也可以通过-Xmx和-Xms控制堆的最大和初始化值，一般将其设置为相同的值，避免其重新进行分配，提高性能。如果在堆中没有内存完成实例分配，并且堆也无法再扩展时，将会抛出OutOfMemoryError 异常。分配和回收内容请看jvm系列中的blog。运行时常量池：运行时常量池（Runtime Constant Pool）是方法区的一部分。Class 文件中除了有类的版本、字段、方法、接口等描述等信息外，还有一项信息是常量池（Constant PoolTable），用于存放编译期生成的各种字面量和符号引用，这部分内容将在类加载后存放到方法区的运行时常量池中。虚拟机栈：每个方法被执行的时候都会同时创建一个栈帧用于存储局部变量表、操作栈、动态链接、方法出口等信息。每一个方法被调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。线程可以通过访问对象的引用找到访问堆中的对象。有两种情况会抛出异常，一是单个线程的栈深度大于虚拟机所允许的深度，将抛出StackOverflowError 异常；如果虚拟机栈可以动态扩展，当扩展时无法申请到足够的内存时会抛出OutOfMemoryError 异常。本地方法栈：本地方法栈（Native Method Stacks）与虚拟机栈所发挥的作用是非常相似的，其区别不过是虚拟机栈为虚拟机执行Java 方法（也就是字节码）服务，而本地方法栈则是为虚拟机使用到的Native 方法服务。其他同虚拟机栈原理类似。程序计数器：程序计数器（Program Counter Register）是一块较小的内存空间，它的作用可以看做是当前线程所执行的字节码的行号指示器。在虚拟机的概念模型里（仅是概念模型，各种虚拟机可能会通过一些更高效的方式去实现），字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。由于Java 虚拟机的多线程是通过线程轮流切换并分配处理器执行时间的方式来实现的，在任何一个确定的时刻，一个处理器（对于多核处理器来说是一个内核）只会执行一条线程中的指令。因此，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各条线程之间的计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。Eden和Survivor比例-XX:NewSize和-XX:MaxNewSize：用于设置年轻代的大小，建议设为整个堆大小的1/3或者1/4,两个值设为一样大。-XX:SurvivorRatio：用于设置Eden和其中一个Survivor的比值，这个值也比较重要。XX:+PrintTenuringDistribution：这个参数用于显示每次Minor GC时Survivor区中各个年龄段的对象的大小。.-XX:InitialTenuringThreshol和-XX:MaxTenuringThresholdJVM内存为什么要分成新生代，老年代，持久代。新生代中为什么要分为Eden和Survivor。为什么需要把堆分代？不分代不能完成他所做的事情么？其实不分代完全可以，分代的唯一理由就是优化GC性能。你先想想，如果没有分代，那我们所有的对象都在一块，GC的时候我们要找到哪些对象没用，这样就会对堆的所有区域进行扫描。而我们的很多对象都是朝生夕死的，如果分代的话，我们把新创建的对象放到某一地方，当GC的时候先把这块存“朝生夕死”对象的区域进行回收，这样就会腾出很大的空间出来。新生代：大多数新生的对象在Eden区分配，当Eden区没有足够空间进行分配时，虚拟机就会进行一次MinorGC。在方法中new一个对象，方法调用完毕，对象就无用，这就是典型的新生代对象。（新生对象在Eden区经历过一次MinorGC并且被Survivor容纳的话，对象年龄为1，并且每熬过一次MinorGC，年龄就会加1，直到15，就会晋升到老年代）注意动态对象的判定：Survivor空间中相同年龄的对象大小总和大于Survivor空间的一半，大于或者等于该年龄的对象就可以直接进入老年代。老年代：在新生代中经历了N次垃圾回收后仍然存活的对象，就会被放到老年代中，而且大对象（占用大量连续内存空间的java对象如很长的字符串及数组）直接进入老年代。当survivor空间不够用时，需要依赖老年代进行分配担保。永久代方法区主要存放Class和Meta的信息，Class在被加载的时候被放入永久代。 它和存放对象的堆区域不同，GC(Garbage Collection)不会在主程序运行期对永久代进行清理，所以如果你的应用程序会加载很多Class的话,就很可能出现PermGen space错误。GC分类MinorGC：是指清理新生代MajorGC：是指清理老年代（很多MajorGC是由MinorGC触发的）FullGC：是指清理整个堆空间包括年轻代和永久代参考博文jvm参数设置大全浅谈CMS垃圾收集器与G1收集器JVM的GC策略","categories":[{"name":"面试","slug":"面试","permalink":"http://www.fufan.me/categories/面试/"}],"tags":[{"name":"面试","slug":"面试","permalink":"http://www.fufan.me/tags/面试/"},{"name":"jvm","slug":"jvm","permalink":"http://www.fufan.me/tags/jvm/"}]},{"title":"JVM专题之jvm知识点总览","slug":"JVM专题之jvm知识点总览","date":"2017-10-31T02:20:00.000Z","updated":"2018-11-07T02:20:37.232Z","comments":true,"path":"2017/10/31/JVM专题之jvm知识点总览/","link":"","permalink":"http://www.fufan.me/2017/10/31/JVM专题之jvm知识点总览/","excerpt":"","text":"jvm体系总体分四大块：类的加载机制jvm内存结构GC算法 垃圾回收GC分析 命令调优类加载机制主要关注点：什么是类的加载类的生命周期类加载器双亲委派模型什么是类的加载类的加载指的是将类的.class文件中的二进制数据读入到内存中，将其放在运行时数据区的方法区内，然后在堆区创建一个java.lang.Class对象，用来封装类在方法区内的数据结构。类的加载的最终产品是位于堆区中的Class对象，Class对象封装了类在方法区内的数据结构，并且向Java程序员提供了访问方法区内的数据结构的接口。类的生命周期类的生命周期包括这几个部分，加载、连接、初始化、使用和卸载，其中前三部是类的加载的过程,如下图；加载，查找并加载类的二进制数据，在Java堆中也创建一个java.lang.Class类的对象连接，连接又包含三块内容：验证、准备、初始化。1）验证，文件格式、元数据、字节码、符号引用验证；2）准备，为类的静态变量分配内存，并将其初始化为默认值；3）解析，把类中的符号引用转换为直接引用初始化，为类的静态变量赋予正确的初始值使用，new出对象程序中使用卸载，执行垃圾回收类加载器启动类加载器：Bootstrap ClassLoader，负责加载存放在JDK\\jre\\lib(JDK代表JDK的安装目录，下同)下，或被-Xbootclasspath参数指定的路径中的，并且能被虚拟机识别的类库扩展类加载器：Extension ClassLoader，该加载器由sun.misc.Launcher$ExtClassLoader实现，它负责加载DK\\jre\\lib\\ext目录中，或者由java.ext.dirs系统变量指定的路径中的所有类库（如javax.*开头的类），开发者可以直接使用扩展类加载器。应用程序类加载器：Application ClassLoader，该类加载器由sun.misc.Launcher$AppClassLoader来实现，它负责加载用户类路径（ClassPath）所指定的类，开发者可以直接使用该类加载器类加载机制全盘负责，当一个类加载器负责加载某个Class时，该Class所依赖的和引用的其他Class也将由该类加载器负责载入，除非显示使用另外一个类加载器来载入父类委托，先让父类加载器试图加载该类，只有在父类加载器无法加载该类时才尝试从自己的类路径中加载该类缓存机制，缓存机制将会保证所有加载过的Class都会被缓存，当程序中需要使用某个Class时，类加载器先从缓存区寻找该Class，只有缓存区不存在，系统才会读取该类对应的二进制数据，并将其转换成Class对象，存入缓存区。这就是为什么修改了Class后，必须重启JVM，程序的修改才会生效jvm内存结构主要关注点：jvm内存结构都是什么对象分配规则jvm内存结构方法区和堆是所有线程共享的内存区域；而java栈、本地方法栈和程序计数器是运行是线程私有的内存区域。Java堆（Heap）,是Java虚拟机所管理的内存中最大的一块。Java堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配内存。方法区（Method Area）,方法区（Method Area）与Java堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。程序计数器（Program Counter Register）,程序计数器（Program Counter Register）是一块较小的内存空间，它的作用可以看做是当前线程所执行的字节码的行号指示器。JVM栈（JVM Stacks）,与程序计数器一样，Java虚拟机栈（Java Virtual Machine Stacks）也是线程私有的，它的生命周期与线程相同。虚拟机栈描述的是Java方法执行的内存模型：每个方法被执行的时候都会同时创建一个栈帧（Stack Frame）用于存储局部变量表、操作栈、动态链接、方法出口等信息。每一个方法被调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。本地方法栈（Native Method Stacks）,本地方法栈（Native Method Stacks）与虚拟机栈所发挥的作用是非常相似的，其区别不过是虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则是为虚拟机使用到的Native方法服务。对象分配规则对象优先分配在Eden区，如果Eden区没有足够的空间时，虚拟机执行一次Minor GC。大对象直接进入老年代（大对象是指需要大量连续内存空间的对象）。这样做的目的是避免在Eden区和两个Survivor区之间发生大量的内存拷贝（新生代采用复制算法收集内存）。长期存活的对象进入老年代。虚拟机为每个对象定义了一个年龄计数器，如果对象经过了1次Minor GC那么对象会进入Survivor区，之后每经过一次Minor GC那么对象的年龄加1，知道达到阀值对象进入老年区。动态判断对象的年龄。如果Survivor区中相同年龄的所有对象大小的总和大于Survivor空间的一半，年龄大于或等于该年龄的对象可以直接进入老年代。空间分配担保。每次进行Minor GC时，JVM会计算Survivor区移至老年区的对象的平均大小，如果这个值大于老年区的剩余值大小则进行一次Full GC，如果小于检查HandlePromotionFailure设置，如果true则只进行Monitor GC,如果false则进行Full GC。GC算法 垃圾回收主要关注点：对象存活判断GC算法垃圾回收器对象存活判断判断对象是否存活一般有两种方式：引用计数：每个对象有一个引用计数属性，新增一个引用时计数加1，引用释放时计数减1，计数为0时可以回收。此方法简单，无法解决对象相互循环引用的问题。可达性分析（Reachability Analysis）：从GC Roots开始向下搜索，搜索所走过的路径称为引用链。当一个对象到GC Roots没有任何引用链相连时，则证明此对象是不可用的，不可达对象。GC算法GC最基础的算法有三种：标记 -清除算法、复制算法、标记-压缩算法，我们常用的垃圾回收器一般都采用分代收集算法。标记 -清除算法，“标记-清除”（Mark-Sweep）算法，如它的名字一样，算法分为“标记”和“清除”两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收掉所有被标记的对象。复制算法，“复制”（Copying）的收集算法，它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。标记-压缩算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存分代收集算法，“分代收集”（Generational Collection）算法，把Java堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。垃圾回收器Serial收集器，串行收集器是最古老，最稳定以及效率高的收集器，可能会产生较长的停顿，只使用一个线程去回收。ParNew收集器，ParNew收集器其实就是Serial收集器的多线程版本。Parallel收集器，Parallel Scavenge收集器类似ParNew收集器，Parallel收集器更关注系统的吞吐量。Parallel Old 收集器，Parallel Old是Parallel Scavenge收集器的老年代版本，使用多线程和“标记－整理”算法CMS收集器，CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。G1收集器，G1 (Garbage-First)是一款面向服务器的垃圾收集器,主要针对配备多颗处理器及大容量内存的机器. 以极高概率满足GC停顿时间要求的同时,还具备高吞吐量性能特征GC分析 命令调优主要关注点：GC日志分析调优命令调优工具GC日志分析Young GC日志:Full GC日志:调优命令Sun JDK监控和故障处理命令有jps jstat jmap jhat jstack jinfojps，JVM Process Status Tool,显示指定系统内所有的HotSpot虚拟机进程。jstat，JVM statistics Monitoring是用于监视虚拟机运行时状态信息的命令，它可以显示出虚拟机进程中的类装载、内存、垃圾收集、JIT编译等运行数据。jmap，JVM Memory Map命令用于生成heap dump文件jhat，JVM Heap Analysis Tool命令是与jmap搭配使用，用来分析jmap生成的dump，jhat内置了一个微型的HTTP/HTML服务器，生成dump的分析结果后，可以在浏览器中查看jstack，用于生成java虚拟机当前时刻的线程快照。jinfo，JVM Configuration info 这个命令作用是实时查看和调整虚拟机运行参数。调优工具常用调优工具分为两类,jdk自带监控工具：jconsole和jvisualvm，第三方有：MAT(Memory Analyzer Tool)、GChisto。jconsole，Java Monitoring and Management Console是从java5开始，在JDK中自带的java监控和管理控制台，用于对JVM中内存，线程和类等的监控jvisualvm，jdk自带全能工具，可以分析内存快照、线程快照；监控内存变化、GC变化等。MAT，Memory Analyzer Tool，一个基于Eclipse的内存分析工具，是一个快速、功能丰富的Java heap分析工具，它可以帮助我们查找内存泄漏和减少内存消耗GChisto，一款专业分析gc日志的工具jprofile","categories":[{"name":"java虚拟机","slug":"java虚拟机","permalink":"http://www.fufan.me/categories/java虚拟机/"}],"tags":[{"name":"java虚拟机","slug":"java虚拟机","permalink":"http://www.fufan.me/tags/java虚拟机/"}]},{"title":"JVM专题（绪论）","slug":"JVM系列（绪论）","date":"2017-10-26T02:17:00.000Z","updated":"2018-11-07T02:30:53.357Z","comments":true,"path":"2017/10/26/JVM系列（绪论）/","link":"","permalink":"http://www.fufan.me/2017/10/26/JVM系列（绪论）/","excerpt":"","text":"作为一个合格的java程序员，对于jvm的熟悉使用是非常有必要的，这个系列将会介绍我们常用jvm的一些原理和常用命令，以及常见的问题的排查及处理。由于最近工作需要，顺便整理和学习一下jvm的相关知识，自己工作中遇到的问题，同时借鉴网上一些资料和博客，理一下这部分知识。博文目录JVM专题之jvm知识点总览JVM专题（一）——类加载机制JVM专题（二）——JVM内存模型JVM专题（三）——GC算法 垃圾收集器JVM专题（四）——jvm调优-命令篇JVM专题（五）——Java GC 分析JVM专题（六）——Java服务GC参数调优案例JVM专题（七）——jvm调优-工具篇博文参考jvm知识点总览感谢博客作者的分享，受益匪浅，已吸收。","categories":[{"name":"java虚拟机","slug":"java虚拟机","permalink":"http://www.fufan.me/categories/java虚拟机/"}],"tags":[{"name":"java虚拟机","slug":"java虚拟机","permalink":"http://www.fufan.me/tags/java虚拟机/"}]},{"title":"常用的vim命令整理（持续更新）","slug":"常用的vim命令整理（持续更新）","date":"2017-10-02T12:58:00.000Z","updated":"2018-11-04T14:46:09.403Z","comments":true,"path":"2017/10/02/常用的vim命令整理（持续更新）/","link":"","permalink":"http://www.fufan.me/2017/10/02/常用的vim命令整理（持续更新）/","excerpt":"","text":"1. 插入模式 (command mode)命令模式切换至插入状态i光标前插入I行首插入a光标后插入A行尾插入o行上新行O行下新行2. 命令模式 (insert mode)ESC从插入状态切换至命令模式光标移动h左移j下移k上移l右移H到屏幕顶部M到屏幕中央L到屏幕底部0到行首$到行尾Ctrl+f向前翻屏Ctrl+b向后翻屏Ctrl+d向前翻半屏Ctrl+u向后翻半屏定位gg回到文件首行,G回到文件尾行:n和nG光标定位到文件第n行(:20或20G表示光标定位到第20行):set nu 或:set number显示行号,:set nonu 取消显示行号ctrl+g删除x删除光标所在字符(与Delete键相同的方向),X删除光标所在字符(与Backspace键相同的方向)nx删除光标后n个字符dd删除光标所在行ndd删除光标所在行以后的n行D删除光标到行尾的内容dG删除光标所在行到文件末尾的内容n1,n2d删除行n1到行n2的内容，包括第n1和n2行都被删除s删除一个字符来插入模式S删除当前行以插入模式复制、剪切、粘贴、替换yy或Y复制当前行nyy或nY从当前行开始赋值n行ggVG全选剪切使用dd和ndd，相当于删除p在光标所在行之后粘贴P在光标所在行之前粘贴r替换当前字符后回到命令模式R一直替换知道通过ESC回到命令模式查找、替换\\KeyWord回车，n查找下一处?KeyWord回车，n查找上一处n重复相同方向N重复反向方向·:s/old/new/g替换整个文件，不确认:s/old/new/gc替换整个文件，确认:n1,n2s/old/new/g替换n1-n2行中匹配内容，不确认撤销u保存及离开:w保存文件:w!强制保存:w file将修改另外保存到file:wq保存文件并退出:wq!强制保存文件并退出:q不保存退出:q!不保存强制退出:e!放弃所有修改，从上次保存文件开始再编辑","categories":[],"tags":[]},{"title":"ssh免密码登录步骤及别名设置","slug":"ssh免密码登录步骤及别名设置","date":"2017-10-02T05:23:00.000Z","updated":"2018-11-04T14:45:55.388Z","comments":true,"path":"2017/10/02/ssh免密码登录步骤及别名设置/","link":"","permalink":"http://www.fufan.me/2017/10/02/ssh免密码登录步骤及别名设置/","excerpt":"","text":"ssh免密码登录步骤及别名设置1. 生成本机的公私钥ssh-keygen -t rsa2. 将公钥复制到目标机器上ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.0.1003. 设置别名登录vim ~/.ssh/config添加如下内容123456Host 100HostName 192.168.0.100Port 22User rootIdentityFile ~/.ssh/id_rsa.pubIdentitiesOnly yes4. 登录ssh 100","categories":[{"name":"linux","slug":"linux","permalink":"http://www.fufan.me/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://www.fufan.me/tags/linux/"}]},{"title":"java加密算法基础","slug":"java加密算法基础","date":"2017-06-23T13:10:00.000Z","updated":"2018-11-06T15:54:55.766Z","comments":true,"path":"2017/06/23/java加密算法基础/","link":"","permalink":"http://www.fufan.me/2017/06/23/java加密算法基础/","excerpt":"","text":"机密算法分类，基础加密算法基本有四种：Base64Md5ShaHMAC复杂加密有两种：对称加密非对称加密基础加密Base64按 照RFC2045的定义，Base64被定义为：Base64内容传送编码被设计用来把任意序列的8位字节描述为一种不易被人直接识别的形式。（The Base64 Content-Transfer-Encoding is designed to represent arbitrary sequences of octets in a form that need not be humanly readable.）常见于邮件、http加密，截取http信息，你就会发现登录操作的用户名、密码字段通过BASE64加密的。java代码如下：123456789101112131415161718192021/** * BASE64解密 * * @param key * @return * @throws Exception */ public static byte[] decryptBASE64(String key) throws Exception &#123; return (new BASE64Decoder()).decodeBuffer(key); &#125; /** * BASE64加密 * * @param key * @return * @throws Exception */ public static String encryptBASE64(byte[] key) throws Exception &#123; return (new BASE64Encoder()).encodeBuffer(key); &#125;主要就是BASE64Encoder、BASE64Decoder两个类，我们只需要知道使用对应的方法即可。另，BASE加密后产生的字节位数是8的倍数，如果不够位数以=符号填充。MD5MD5 – message-digest algorithm 5 （信息-摘要算法）缩写，广泛用于加密和解密技术，常用于文件校验。校验？不管文件多大，经过MD5后都能生成唯一的MD5值。好比现在的ISO校验，都 是MD5校验。怎么用？当然是把ISO经过MD5后产生MD5的值。一般下载linux-ISO的朋友都见过下载链接旁边放着MD5的串。就是用来验证文 件是否一致的。通过java代码实现如下：123456789101112131415/** * MD5加密 * * @param data * @return * @throws Exception */ public static byte[] encryptMD5(byte[] data) throws Exception &#123; MessageDigest md5 = MessageDigest.getInstance(KEY_MD5); md5.update(data); return md5.digest(); &#125;通常我们不直接使用上述MD5加密。通常将MD5产生的字节数组交给BASE64再加密一把，得到相应的字符串。SHASHA(Secure Hash Algorithm，安全散列算法），数字签名等密码学应用中重要的工具，被广泛地应用于电子商务等信息安全领域。虽然，SHA与MD5通过碰撞法都被破解了， 但是SHA仍然是公认的安全加密算法，较之MD5更为安全。通过java代码实现如下：123456789101112131415161718192021222324252627282930/** * 初始化HMAC密钥 * * @return * @throws Exception */ public static String initMacKey() throws Exception &#123; KeyGenerator keyGenerator = KeyGenerator.getInstance(KEY_MAC); SecretKey secretKey = keyGenerator.generateKey(); return encryptBASE64(secretKey.getEncoded()); &#125; /** * HMAC加密 * * @param data * @param key * @return * @throws Exception */ public static byte[] encryptHMAC(byte[] data, String key) throws Exception &#123; SecretKey secretKey = new SecretKeySpec(decryptBASE64(key), KEY_MAC); Mac mac = Mac.getInstance(secretKey.getAlgorithm()); mac.init(secretKey); return mac.doFinal(data); &#125;BASE64的加密解密是双向的，可以求反解。MD5、SHA以及HMAC是单向加密，任何数据加密后只会产生唯一的一个加密串，通常用来校验数据在传输过程中是否被修改。其中HMAC算法有一个密钥，增强了数据传输过程中的安全性，强化了算法外的不可控因素。单向加密的用途主要是为了校验数据在传输过程中是否被修改。复杂加密对称加密DESDES-Data Encryption Standard,即数据加密算法。是IBM公司于1975年研究成功并公开发表的。DES算法的入口参数有三个:Key、Data、Mode。其中 Key为8个字节共64位,是DES算法的工作密钥;Data也为8个字节64位,是要被加密或被解密的数据;Mode为DES的工作方式,有两种:加密 或解密。DES算法把64位的明文输入块变为64位的密文输出块,它所使用的密钥也是64位。其实DES有很多同胞兄弟，如DESede(TripleDES)、AES、Blowfish、RC2、RC4(ARCFOUR)。这里就不过多阐述了，大同小异，只要换掉ALGORITHM换成对应的值，同时做一个代码替换SecretKey secretKey = new SecretKeySpec(key, ALGORITHM);就可以了，此外就是密钥长度不同了。PBEPBE——Password-based encryption（基于密码加密）。其特点在于口令由用户自己掌管，不借助任何物理媒体；采用随机数（这里我们叫做盐）杂凑多重加密等方法保证数据的安全性。是一种简便的加密方式。非对称加密RSA这种算法1978年就出现了，它是第一个既能用于数据加密也能用于数字签名的算法。它易于理解和操作，也很流行。算法的名字以发明者的名字命名：Ron Rivest, AdiShamir 和Leonard Adleman。这种加密算法的特点主要是密钥的变化，上文我们看到DES只有一个密钥。相当于只有一把钥匙，如果这把钥匙丢了，数据也就不安全了。RSA同时有两把钥 匙，公钥与私钥。同时支持数字签名。数字签名的意义在于，对传输过来的数据进行校验。确保数据在传输工程中不被修改。流程分析：甲方构建密钥对儿，将公钥公布给乙方，将私钥保留。甲方使用私钥加密数据，然后用私钥对加密后的数据签名，发送给乙方签名以及加密后的数据；乙方使用公钥、签名来验证待解密数据是否有效，如果有效使用公钥对数据解密。乙方使用公钥加密数据，向甲方发送经过加密后的数据；甲方获得加密数据，通过私钥解密。简要总结一下，使用公钥加密、私钥解密，完成了乙方到甲方的一次数据传递，通过私钥加密、公钥解密，同时通过私钥签名、公钥验证签名，完成了一次甲方到乙方的数据传递与验证，两次数据传递完成一整套的数据交互！类似数字签名，数字信封是这样描述的：数字信封数字信封用加密技术来保证只有特定的收信人才能阅读信的内容。 流程：信息发送方采用对称密钥来加密信息，然后再用接收方的公钥来加密此对称密钥（这部分称为数字信封），再将它和信息一起发送给接收方；接收方先用相应的私钥打开数字信封，得到对称密钥，然后使用对称密钥再解开信息。接下来我们分析DH加密算法，一种适基于密钥一致协议的加密算法。DHDiffie- Hellman算法(D-H算法)，密钥一致协议。是由公开密钥密码体制的奠基人Diffie和Hellman所提出的一种思想。简单的说就是允许两名用 户在公开媒体上交换信息以生成”一致”的、可以共享的密钥。换句话说，就是由甲方产出一对密钥（公钥、私钥），乙方依照甲方公钥产生乙方密钥对（公钥、私 钥）。以此为基线，作为数据传输保密基础，同时双方使用同一种对称加密算法构建本地密钥（SecretKey）对数据加密。这样，在互通了本地密钥 （SecretKey）算法后，甲乙双方公开自己的公钥，使用对方的公钥和刚才产生的私钥加密数据，同时可以使用对方的公钥和自己的私钥对数据解密。不单 单是甲乙双方两方，可以扩展为多方共享数据通讯，这样就完成了网络交互数据的安全通讯！该算法源于中国的同余定理——中国馀数定理。流程分析：甲方构建密钥对儿，将公钥公布给乙方，将私钥保留；双方约定数据加密算法；乙方通过甲方公钥构建密钥对儿，将公钥公布给甲方，将私钥保留。甲方使用私钥、乙方公钥、约定数据加密算法构建本地密钥，然后通过本地密钥加密数据，发送给乙方加密后的数据；乙方使用私钥、甲方公钥、约定数据加密算法构建本地密钥，然后通过本地密钥对数据解密。乙方使用私钥、甲方公钥、约定数据加密算法构建本地密钥，然后通过本地密钥加密数据，发送给甲方加密后的数据；甲方使用私钥、乙方公钥、约定数据加密算法构建本地密钥，然后通过本地密钥对数据解密。常见加密算法DES（Data Encryption Standard）：数据加密标准，速度较快，适用于加密大量数据的场合；3DES（Triple DES）：是基于DES，对一块数据用三个不同的密钥进行三次加密，强度更高；RC2和 RC4：用变长密钥对大量数据进行加密，比 DES 快；IDEA（International Data Encryption Algorithm）国际数据加密算法：使用 128 位密钥提供非常强的安全性；RSA：由 RSA 公司发明，是一个支持变长密钥的公共密钥算法，需要加密的文件块的长度也是可变的；DSA（Digital Signature Algorithm）：数字签名算法，是一种标准的 DSS（数字签名标准）；AES（Advanced Encryption Standard）：高级加密标准，是下一代的加密算法标准，速度快，安全级别高，目前 AES 标准的一个实现是 Rijndael 算法；BLOWFISH，它使用变长的密钥，长度可达448位，运行速度很快；其它算法，如ElGamal、Deffie-Hellman、新型椭圆曲线算法ECC等。 比如说，MD5，你在一些比较正式而严格的网站下的东西一般都会有MD5值给出，如安全焦点的软件工具，每个都有MD5。严格来说MD5并不能算是一种加密算法，只能说是一种摘要算法（数据摘要算法是密码学算法中非常重要的一个分支，它通过对所有数据提取指纹信息以实现数据签名、数据完整性校验等功能，由于其不可逆性，有时候会被用做敏感信息的加密。数据摘要算法也被称为哈希(Hash)算法、散列算法。）参考博文各种Java加密算法","categories":[{"name":"加密算法","slug":"加密算法","permalink":"http://www.fufan.me/categories/加密算法/"}],"tags":[{"name":"加密","slug":"加密","permalink":"http://www.fufan.me/tags/加密/"}]},{"title":"java多线程系列（七）——JUC锁","slug":"java多线程系列（七）——JUC锁","date":"2017-06-05T17:47:00.000Z","updated":"2018-11-05T17:55:14.987Z","comments":true,"path":"2017/06/06/java多线程系列（七）——JUC锁/","link":"","permalink":"http://www.fufan.me/2017/06/06/java多线程系列（七）——JUC锁/","excerpt":"","text":"下面介绍一下JUC包中可以让我们在多线程并发中使用的锁。UC包中的锁，包括：Lock接口ReadWriteLock接口LockSupport阻塞原语Condition条件AbstractOwnableSynchronizer/AbstractQueuedSynchronizer/AbstractQueuedLongSynchronizer三个抽象类ReentrantLock独占锁ReentrantReadWriteLock读写锁由于CountDownLatch，CyclicBarrier和Semaphore也是通过AQS来实现的；因此，我也将它们归纳到锁的框架中进行介绍。先看看锁的框架图，如下所示。下面简述一下每个类或接口Lock接口JUC包中的 Lock 接口支持那些语义不同(重入、公平等)的锁规则。所谓语义不同，是指锁可是有”公平机制的锁”、”非公平机制的锁”、”可重入的锁”等等。”公平机制”是指”不同线程获取锁的机制是公平的”，而”非公平机制”则是指”不同线程获取锁的机制是非公平的”，”可重入的锁”是指同一个锁能够被一个线程多次获取。ReadWriteLockReadWriteLock 接口以和Lock类似的方式定义了一些读取者可以共享而写入者独占的锁。JUC包只有一个类实现了该接口，即 ReentrantReadWriteLock，因为它适用于大部分的标准用法上下文。但程序员可以创建自己的、适用于非标准要求的实现。AbstractOwnableSynchronizer/AbstractQueuedSynchronizer/AbstractQueuedLongSynchronizerAbstractQueuedSynchronizer就是被称之为AQS的类，它是一个非常有用的超类，可用来定义锁以及依赖于排队阻塞线程的其他同步器；ReentrantLock，ReentrantReadWriteLock，CountDownLatch，CyclicBarrier和Semaphore等这些类都是基于AQS类实现的。AbstractQueuedLongSynchronizer 类提供相同的功能但扩展了对同步状态的 64 位的支持。两者都扩展了类 AbstractOwnableSynchronizer（一个帮助记录当前保持独占同步的线程的简单类）。LockSupportLockSupport提供“创建锁”和“其他同步类的基本线程阻塞原语”。LockSupport的功能和”Thread中的Thread.suspend()和Thread.resume()有点类似”，LockSupport中的park() 和 unpark() 的作用分别是阻塞线程和解除阻塞线程。但是park()和unpark()不会遇到“Thread.suspend 和 Thread.resume所可能引发的死锁”问题。ConditionCondition需要和Lock联合使用，它的作用是代替Object监视器方法，可以通过await(),signal()来休眠/唤醒线程。Condition 接口描述了可能会与锁有关联的条件变量。这些变量在用法上与使用 Object.wait 访问的隐式监视器类似，但提供了更强大的功能。需要特别指出的是，单个 Lock 可能与多个 Condition 对象关联。为了避免兼容性问题，Condition 方法的名称与对应的 Object 版本中的不同。ReentrantLockReentrantLock是独占锁。所谓独占锁，是指只能被独自占领，即同一个时间点只能被一个线程锁获取到的锁。ReentrantLock锁包括”公平的ReentrantLock”和”非公平的ReentrantLock”。”公平的ReentrantLock”是指”不同线程获取锁的机制是公平的”，而”非公平的 ReentrantLock”则是指”不同线程获取锁的机制是非公平的”，ReentrantLock是”可重入的锁”。ReentrantLock的UML类图如下：ReentrantLock实现了Lock接口。ReentrantLock中有一个成员变量sync，sync是Sync类型；Sync是一个抽象类，而且它继承于AQS。ReentrantLock中有”公平锁类”FairSync和”非公平锁类”NonfairSync，它们都是Sync的子类。ReentrantReadWriteLock中sync对象，是FairSync与NonfairSync中的一种，这也意味着ReentrantLock是”公平锁”或”非公平锁”中的一种，ReentrantLock默认是非公平锁。ReentrantReadWriteLockReentrantReadWriteLock是读写锁接口ReadWriteLock的实现类，它包括子类ReadLock和WriteLock。ReentrantLock是共享锁，而WriteLock是独占锁。ReentrantReadWriteLock的UML类图如下：ReentrantReadWriteLock实现了ReadWriteLock接口。ReentrantReadWriteLock中包含sync对象，读锁readerLock和写锁writerLock。读锁ReadLock和写锁WriteLock都实现了Lock接口。和”ReentrantLock”一样，sync是Sync类型；而且，Sync也是一个继承于AQS的抽象类。Sync也包括”公平锁”FairSync和”非公平锁”NonfairSync。CountDownLatchCountDownLatch是一个同步辅助类，在完成一组正在其他线程中执行的操作之前，它允许一个或多个线程一直等待。CountDownLatch的UML类图如下：CountDownLatch包含了sync对象，sync是Sync类型。CountDownLatch的Sync是实例类，它继承于AQS。CyclicBarrierCyclicBarrier是一个同步辅助类，允许一组线程互相等待，直到到达某个公共屏障点 (common barrier point)。因为该 barrier 在释放等待线程后可以重用，所以称它为循环 的 barrier。CyclicBarrier的UML类图如下：CyclicBarrier是包含了”ReentrantLock对象lock”和”Condition对象trip”，它是通过独占锁实现的。CyclicBarrier和CountDownLatch的区别是：1. CountDownLatch的作用是允许1或N个线程等待其他线程完成执行；而CyclicBarrier则是允许N个线程相互等待。2. CountDownLatch的计数器无法被重置；CyclicBarrier的计数器可以被重置后使用，因此它被称为是循环的barrier。SemaphoreSemaphore是一个计数信号量，它的本质是一个”共享锁”。信号量维护了一个信号量许可集。线程可以通过调用acquire()来获取信号量的许可；当信号量中有可用的许可时，线程能获取该许可；否则线程必须等待，直到有可用的许可为止。 线程可以通过release()来释放它所持有的信号量许可。Semaphore的UML类图如下：和”ReentrantLock”一样，Semaphore包含了sync对象，sync是Sync类型；而且，Sync也是一个继承于AQS的抽象类。Sync也包括”公平信号量”FairSync和”非公平信号量”NonfairSync。参考博文Java多线程系列目录(共43篇)","categories":[{"name":"多线程","slug":"多线程","permalink":"http://www.fufan.me/categories/多线程/"}],"tags":[{"name":"多线程","slug":"多线程","permalink":"http://www.fufan.me/tags/多线程/"}]},{"title":"java多线程系列（六）——JUC并发集合","slug":"java多线程系列（六）——JUC并发集合","date":"2017-05-30T17:29:00.000Z","updated":"2018-11-05T17:30:24.014Z","comments":true,"path":"2017/05/31/java多线程系列（六）——JUC并发集合/","link":"","permalink":"http://www.fufan.me/2017/05/31/java多线程系列（六）——JUC并发集合/","excerpt":"","text":"说到并发集合，还是先回到java的集合包里来。Java集合包集合包主要包括两大类：CollectionListSetMapList的实现类主要有: LinkedList, ArrayList, Vector, Stack。LinkedList是双向链表实现的双端队列；它不是线程安全的，只适用于单线程。ArrayList是数组实现的队列，它是一个动态数组；它也不是线程安全的，只适用于单线程。Vector是数组实现的矢量队列，它也一个动态数组；不过和ArrayList不同的是，Vector是线程安全的，它支持并发。Stack是Vector实现的栈；和Vector一样，它也是线程安全的。Set的实现类主要有: HastSet和TreeSet。HashSet是一个没有重复元素的集合，它通过HashMap实现的；HashSet不是线程安全的，只适用于单线程。TreeSet也是一个没有重复元素的集合，不过和HashSet不同的是，TreeSet中的元素是有序的；它是通过TreeMap实现的；TreeSet也不是线程安全的，只适用于单线程。Map的实现类主要有: HashMap，WeakHashMap, Hashtable和TreeMap。HashMap是存储“键-值对”的哈希表；它不是线程安全的，只适用于单线程。WeakHashMap是也是哈希表；和HashMap不同的是，HashMap的“键”是强引用类型，而WeakHashMap的“键”是弱引用类型，也就是说当WeakHashMap 中的某个键不再正常使用时，会被从WeakHashMap中被自动移除。WeakHashMap也不是线程安全的，只适用于单线程。Hashtable也是哈希表；和HashMap不同的是，Hashtable是线程安全的，支持并发。TreeMap也是哈希表，不过TreeMap中的“键-值对”是有序的，它是通过R-B Tree(红黑树)实现的；TreeMap不是线程安全的，只适用于单线程。为了方便，我们将前面介绍集合类统称为”java集合包“。java集合包大多是“非线程安全的”，虽然可以通过Collections工具类中的方法获取java集合包对应的同步类，但是这些同步类的并发效率并不是很高。回顾完这些集合以后，我发现工作中其实多线程中使用到的集合印象深刻点的就是LinkedBlockingQueue，就是在线程池那里涉及到过，不过其实还有很多类似的变种，并发大师Doug Lea在JUC(java.util.concurrent)包中添加了java集合包中单线程类的对应的支持高并发的类。例如，ArrayList对应的高并发类是CopyOnWriteArrayList，HashMap对应的高并发类是ConcurrentHashMap，等等。JUC包在添加”java集合包“对应的高并发类时，为了保持API接口的一致性，使用了”Java集合包“中的框架。例如，CopyOnWriteArrayList实现了“Java集合包”中的List接口，HashMap继承了“java集合包”中的AbstractMap类，等等。得益于“JUC包使用了Java集合包中的类”，如果我们了解了Java集合包中的类的思想之后，理解JUC包中的类也相对容易；理解时，最大的难点是，对JUC包是如何添加对“高并发”的支持的！JUC中的集合类List和SetJUC集合包中的List和Set实现类包括: CopyOnWriteArrayList, CopyOnWriteArraySet和ConcurrentSkipListSet。ConcurrentSkipListSet稍后在说明Map时再说明，CopyOnWriteArrayList 和 CopyOnWriteArraySet的框架如下图所示：CopyOnWriteArrayList相当于线程安全的ArrayList，它实现了List接口。CopyOnWriteArrayList是支持高并发的。CopyOnWriteArraySet相当于线程安全的HashSet，它继承于AbstractSet类。CopyOnWriteArraySet内部包含一个CopyOnWriteArrayList对象，它是通过CopyOnWriteArrayList实现的。MapJUC集合包中Map的实现类包括: ConcurrentHashMap和ConcurrentSkipListMap。它们的框架如下图所示：ConcurrentHashMap是线程安全的哈希表(相当于线程安全的HashMap)；它继承于AbstractMap类，并且实现ConcurrentMap接口。ConcurrentHashMap是通过“锁分段”来实现的，它支持并发。ConcurrentSkipListMap是线程安全的有序的哈希表(相当于线程安全的TreeMap); 它继承于AbstractMap类，并且实现ConcurrentNavigableMap接口。ConcurrentSkipListMap是通过“跳表”来实现的，它支持并发。ConcurrentSkipListSet是线程安全的有序的集合(相当于线程安全的TreeSet)；它继承于AbstractSet，并实现了NavigableSet接口。ConcurrentSkipListSet是通过ConcurrentSkipListMap实现的，它也支持并发。QueueJUC集合包中Queue的实现类包括: ArrayBlockingQueue, LinkedBlockingQueue, LinkedBlockingDeque, ConcurrentLinkedQueue和ConcurrentLinkedDeque。它们的框架如下图所示：ArrayBlockingQueue是数组实现的线程安全的有界的阻塞队列。LinkedBlockingQueue是单向链表实现的(指定大小)阻塞队列，该队列按 FIFO（先进先出）排序元素。LinkedBlockingDeque是双向链表实现的(指定大小)双向并发阻塞队列，该阻塞队列同时支持FIFO和FILO两种操作方式。ConcurrentLinkedQueue是单向链表实现的无界队列，该队列按 FIFO（先进先出）排序元素。ConcurrentLinkedDeque是双向链表实现的无界队列，该队列同时支持FIFO和FILO两种操作方式参考博文Java多线程系列目录(共43篇)","categories":[{"name":"多线程","slug":"多线程","permalink":"http://www.fufan.me/categories/多线程/"}],"tags":[{"name":"多线程","slug":"多线程","permalink":"http://www.fufan.me/tags/多线程/"}]},{"title":"java多线程系列（五）——JUC原子类","slug":"java多线程系列（五）——JUC原子类","date":"2017-05-25T17:09:00.000Z","updated":"2018-11-05T17:10:03.303Z","comments":true,"path":"2017/05/26/java多线程系列（五）——JUC原子类/","link":"","permalink":"http://www.fufan.me/2017/05/26/java多线程系列（五）——JUC原子类/","excerpt":"","text":"根据修改的数据类型，可以将JUC包中的原子操作类可以分为4类。基本类型: AtomicInteger, AtomicLong, AtomicBoolean ;数组类型: AtomicIntegerArray, AtomicLongArray, AtomicReferenceArray ;引用类型: AtomicReference, AtomicStampedRerence, AtomicMarkableReference ;对象的属性修改类型: AtomicIntegerFieldUpdater, AtomicLongFieldUpdater, AtomicReferenceFieldUpdater 。这些类存在的目的是对相应的数据进行原子操作。所谓原子操作，是指操作过程不会被中断，保证数据操作是以原子方式进行的。值得一提的是，Java的AtomXXX类并不是使用了锁的方式进行同步，而是采用了一种新的理念，叫做CAS.CAS是一组原语指令，用来实现多线程下的变量同步。在 x86 下的指令CMPXCHG实现了CAS，前置LOCK既可以达到原子性操作。由于CAS原语的直接操作与计算机底层的联系很大，CAS原语有三个参数，内存地址，期望值，新值。我们在Java中一般不去直接写CAS相关的代码，JDK为我们封装在AtomXXX中，因此，我们直接使用就可以了。基本类型用AtomicLong来举例主要函数AtomicLong是作用是对长整形进行原子操作。在32位操作系统中，64位的long 和 double 变量由于会被JVM当作两个分离的32位来进行操作，所以不具有原子性。而使用AtomicLong能让long的操作保持原子型。123456789101112131415161718192021222324252627282930313233343536// 构造函数AtomicLong()// 创建值为initialValue的AtomicLong对象AtomicLong(long initialValue)// 以原子方式设置当前值为newValue。final void set(long newValue) // 获取当前值final long get() // 以原子方式将当前值减 1，并返回减1后的值。等价于“--num”final long decrementAndGet() // 以原子方式将当前值减 1，并返回减1前的值。等价于“num--”final long getAndDecrement() // 以原子方式将当前值加 1，并返回加1后的值。等价于“++num”final long incrementAndGet() // 以原子方式将当前值加 1，并返回加1前的值。等价于“num++”final long getAndIncrement() // 以原子方式将delta与当前值相加，并返回相加后的值。final long addAndGet(long delta) // 以原子方式将delta添加到当前值，并返回相加前的值。final long getAndAdd(long delta) // 如果当前值 == expect，则以原子方式将该值设置为update。成功返回true，否则返回false，并且不修改原值。final boolean compareAndSet(long expect, long update)// 以原子方式设置当前值为newValue，并返回旧值。final long getAndSet(long newValue)// 返回当前值对应的int值int intValue() // 获取当前值对应的long值long longValue() // 以 float 形式返回当前值float floatValue() // 以 double 形式返回当前值double doubleValue() // 最后设置为给定值。延时设置变量值，这个等价于set()方法，但是由于字段是volatile类型的，因此次字段的修改会比普通字段（非volatile字段）有稍微的性能延时（尽管可以忽略），所以如果不是想立即读取设置的新值，允许在“后台”修改值，那么此方法就很有用。如果还是难以理解，这里就类似于启动一个后台线程如执行修改新值的任务，原线程就不等待修改结果立即返回（这种解释其实是不正确的，但是可以这么理解）。final void lazySet(long newValue)// 如果当前值 == 预期值，则以原子方式将该设置为给定的更新值。JSR规范中说：以原子方式读取和有条件地写入变量但不 创建任何 happen-before 排序，因此不提供与除 weakCompareAndSet 目标外任何变量以前或后续读取或写入操作有关的任何保证。大意就是说调用weakCompareAndSet时并不能保证不存在happen-before的发生（也就是可能存在指令重排序导致此操作失败）。但是从Java源码来看，其实此方法并没有实现JSR规范的要求，最后效果和compareAndSet是等效的，都调用了unsafe.compareAndSwapInt()完成操作。final boolean weakCompareAndSet(long expect, long update)源码分析AtomicLong的代码很简单，下面仅以incrementAndGet()为例，对AtomicLong的原理进行说明。incrementAndGet()源码如下：1234567891011public final long incrementAndGet() &#123; for (;;) &#123; // 获取AtomicLong当前对应的long值 long current = get(); // 将current加1 long next = current + 1; // 通过CAS函数，更新current的值 if (compareAndSet(current, next)) return next; &#125;&#125;说明：(01) incrementAndGet()首先会根据get()获取AtomicLong对应的long值。该值是volatile类型的变量，get()的源码如下：123456// value是AtomicLong对应的long值private volatile long value;// 返回AtomicLong对应的long值public final long get() &#123; return value;&#125;(02) incrementAndGet()接着将current加1,然后通过CAS函数，将新的值赋值给value。compareAndSet()的源码如下：123public final boolean compareAndSet(long expect, long update) &#123; return unsafe.compareAndSwapLong(this, valueOffset, expect, update);&#125;compareAndSet()的作用是更新AtomicLong对应的long值。它会比较AtomicLong的原始值是否与expect相等，若相等的话，则设置AtomicLong的值为update。数组类型AtomicLongArray函数列表123456789101112131415161718192021222324252627282930313233// 创建给定长度的新 AtomicLongArray。AtomicLongArray(int length)// 创建与给定数组具有相同长度的新 AtomicLongArray，并从给定数组复制其所有元素。AtomicLongArray(long[] array)// 以原子方式将给定值添加到索引 i 的元素。long addAndGet(int i, long delta)// 如果当前值 == 预期值，则以原子方式将该值设置为给定的更新值。boolean compareAndSet(int i, long expect, long update)// 以原子方式将索引 i 的元素减1。long decrementAndGet(int i)// 获取位置 i 的当前值。long get(int i)// 以原子方式将给定值与索引 i 的元素相加。long getAndAdd(int i, long delta)// 以原子方式将索引 i 的元素减 1。long getAndDecrement(int i)// 以原子方式将索引 i 的元素加 1。long getAndIncrement(int i)// 以原子方式将位置 i 的元素设置为给定值，并返回旧值。long getAndSet(int i, long newValue)// 以原子方式将索引 i 的元素加1。long incrementAndGet(int i)// 最终将位置 i 的元素设置为给定值。void lazySet(int i, long newValue)// 返回该数组的长度。int length()// 将位置 i 的元素设置为给定值。void set(int i, long newValue)// 返回数组当前值的字符串表示形式。String toString()// 如果当前值 == 预期值，则以原子方式将该值设置为给定的更新值。boolean weakCompareAndSet(int i, long expect, long update)源码分析同AtomicLong类似incrementAndGet()源码如下：123public final long incrementAndGet(int i) &#123; return addAndGet(i, 1);&#125;说明：incrementAndGet()的作用是以原子方式将long数组的索引 i 的元素加1，并返回加1之后的值。addAndGet()源码如下：12345678910111213public long addAndGet(int i, long delta) &#123; // 检查数组是否越界 long offset = checkedByteOffset(i); while (true) &#123; // 获取long型数组的索引 offset 的原始值 long current = getRaw(offset); // 修改long型值 long next = current + delta; // 通过CAS更新long型数组的索引 offset的值。 if (compareAndSetRaw(offset, current, next)) return next; &#125;&#125;getRaw()源码如下：123private long getRaw(long offset) &#123; return unsafe.getLongVolatile(array, offset);&#125;说明：unsafe是通过Unsafe.getUnsafe()返回的一个Unsafe对象。通过Unsafe的CAS函数对long型数组的元素进行原子操作。如compareAndSetRaw()就是调用Unsafe的CAS函数，它的源码如下：123private boolean compareAndSetRaw(long offset, long expect, long update) &#123; return unsafe.compareAndSwapLong(array, offset, expect, update);&#125;说明：addAndGet()首先检查数组是否越界。如果没有越界的话，则先获取数组索引i的值；然后通过CAS函数更新i的值。引用类型AtomicReference函数列表12345678910111213141516171819// 使用 null 初始值创建新的 AtomicReference。AtomicReference()// 使用给定的初始值创建新的 AtomicReference。AtomicReference(V initialValue)// 如果当前值 == 预期值，则以原子方式将该值设置为给定的更新值。boolean compareAndSet(V expect, V update)// 获取当前值。V get()// 以原子方式设置为给定值，并返回旧值。V getAndSet(V newValue)// 最终设置为给定值。void lazySet(V newValue)// 设置为给定值。void set(V newValue)// 返回当前值的字符串表示形式。String toString()// 如果当前值 == 预期值，则以原子方式将该值设置为给定的更新值。boolean weakCompareAndSet(V expect, V update)源码123456789101112131415161718192021222324252627282930// AtomicReferenceTest.java的源码import java.util.concurrent.atomic.AtomicReference;public class AtomicReferenceTest &#123; public static void main(String[] args)&#123; // 创建两个Person对象，它们的id分别是101和102。 Person p1 = new Person(101); Person p2 = new Person(102); // 新建AtomicReference对象，初始化它的值为p1对象 AtomicReference ar = new AtomicReference(p1); // 通过CAS设置ar。如果ar的值为p1的话，则将其设置为p2。 ar.compareAndSet(p1, p2); Person p3 = (Person)ar.get(); System.out.println(\"p3 is \"+p3); System.out.println(\"p3.equals(p1)=\"+p3.equals(p1)); &#125;&#125;class Person &#123; volatile long id; public Person(long id) &#123; this.id = id; &#125; public String toString() &#123; return \"id:\"+id; &#125;&#125;AtomicReference的源码比较简单。它是通过”volatile”和”Unsafe提供的CAS函数实现”原子操作。value是volatile类型。这保证了：当某线程修改value的值时，其他线程看到的value值都是最新的value值，即修改之后的volatile的值。通过CAS设置value。这保证了：当某线程池通过CAS函数(如compareAndSet函数)设置value时，它的操作是原子的，即线程在操作value时不会被中断。对象的属性修改类型AtomicIntegerFieldUpdater简单介绍一下同上面的几种类型一样，也是通过原子和CAS的方式来保证多线程使用变量同步不会出问题。AtomicLongFieldUpdater示例代码实例：12345678910111213141516171819202122232425262728293031// LongTest.java的源码import java.util.concurrent.atomic.AtomicLongFieldUpdater;public class LongFieldTest &#123; public static void main(String[] args) &#123; // 获取Person的class对象 Class cls = Person.class; // 新建AtomicLongFieldUpdater对象，传递参数是“class对象”和“long类型在类中对应的名称” AtomicLongFieldUpdater mAtoLong = AtomicLongFieldUpdater.newUpdater(cls, \"id\"); Person person = new Person(12345678L); // 比较person的\"id\"属性，如果id的值为12345678L，则设置为1000。 mAtoLong.compareAndSet(person, 12345678L, 1000); System.out.println(\"id=\"+person.getId()); &#125;&#125;class Person &#123; volatile long id; public Person(long id) &#123; this.id = id; &#125; public void setId(long id) &#123; this.id = id; &#125; public long getId() &#123; return id; &#125;&#125;运行结果：1id=1000参考博文Java多线程系列目录(共43篇)","categories":[{"name":"多线程","slug":"多线程","permalink":"http://www.fufan.me/categories/多线程/"}],"tags":[{"name":"多线程","slug":"多线程","permalink":"http://www.fufan.me/tags/多线程/"}]},{"title":"java多线程系列（四）——CAS和AQS学习","slug":"java多线程系列（四）——CAS和AQS学习","date":"2017-05-20T12:16:00.000Z","updated":"2018-11-05T12:18:32.323Z","comments":true,"path":"2017/05/20/java多线程系列（四）——CAS和AQS学习/","link":"","permalink":"http://www.fufan.me/2017/05/20/java多线程系列（四）——CAS和AQS学习/","excerpt":"","text":"CASCAS的全称是Compare And Swap 即比较交换，其算法核心思想如下1执行函数：CAS(V,E,N)其包含3个参数V表示要更新的变量E表示预期值N表示新值如果V值等于E值，则将V的值设为N。若V值和E值不同，则说明已经有其他线程做了更新，则当前线程什么都不做。通俗的理解就是CAS操作需要我们提供一个期望值，当期望值与当前线程的变量值相同时，说明还没线程修改该值，当前线程可以进行修改，也就是执行CAS操作，但如果期望值与当前线程不符，则说明该值已被其他线程修改，此时不执行更新操作，但可以选择重新读取该变量再尝试再次修改该变量，也可以放弃操作.由于CAS操作属于乐观派，它总认为自己可以成功完成操作，当多个线程同时使用CAS操作一个变量时，只有一个会胜出，并成功更新，其余均会失败，但失败的线程并不会被挂起，仅是被告知失败，并且允许再次尝试，当然也允许失败的线程放弃操作，这点从图中也可以看出来。基于这样的原理，CAS操作即使没有锁，同样知道其他线程对共享资源操作影响，并执行相应的处理措施。同时从这点也可以看出，由于无锁操作中没有锁的存在，因此不可能出现死锁的情况，也就是说无锁操作天生免疫死锁.鲜为人知的指针: Unsafe类Unsafe类存在于sun.misc包中，其内部方法操作可以像C的指针一样直接操作内存，单从名称看来就可以知道该类是非安全的，毕竟Unsafe拥有着类似于C的指针操作，因此总是不应该首先使用Unsafe类，Java官方也不建议直接使用的Unsafe类，但我们还是很有必要了解该类，因为Java中CAS操作的执行依赖于Unsafe类的方法，注意Unsafe类中的所有方法都是native修饰的，也就是说Unsafe类中的方法都直接调用操作系统底层资源执行相应任务.CAS是一些CPU直接支持的指令，也就是我们前面分析的无锁操作，在Java中无锁操作CAS基于以下3个方法实现，在稍后讲解Atomic系列内部方法是基于下述方法的实现的。CAS的ABA问题及其解决方案假设这样一种场景，当第一个线程执行CAS(V,E,U)操作，在获取到当前变量V，准备修改为新值U前，另外两个线程已连续修改了两次变量V的值，使得该值又恢复为旧值，这样的话，我们就无法正确判断这个变量是否已被修改过这就是典型的CAS的ABA问题，一般情况这种情况发现的概率比较小，可能发生了也不会造成什么问题，比如说我们对某个做加减法，不关心数字的过程，那么发生ABA问题也没啥关系。但是在某些情况下还是需要防止的，那么该如何解决呢？在Java中解决ABA问题，我们可以使用以下两个原子类AtomicStampedReference类AtomicStampedReference原子类是一个带有时间戳的对象引用，在每次修改后，AtomicStampedReference不仅会设置新值而且还会记录更改的时间。当AtomicStampedReference设置对象值时，对象值以及时间戳都必须满足期望值才能写入成功，这也就解决了反复读写时，无法预知值是否已被修改的窘境同此类类似，还有AtomicMarkableReference类，这种方式并不能完全防止ABA问题的发生，只能减少ABA问题发生的概率。AtomicMarkableReference的实现原理与AtomicStampedReference类似，这里不再介绍。到此，我们也明白了如果要完全杜绝ABA问题的发生，我们应该使用AtomicStampedReference原子类更新对象，而对于AtomicMarkableReference来说只能减少ABA问题的发生概率，并不能杜绝。AQSCLH队列AQS内部维护着一个FIFO的队列，即CLH队列。AQS的同步机制就是依靠CLH队列实现的。CLH队列是FIFO的双端双向队列，实现公平锁。线程通过AQS获取锁失败，就会将线程封装成一个Node节点，插入队列尾。当有线程释放锁时，后尝试把队头的next节点占用锁。CLH队列结构NodeCLH队列由Node对象组成，Node是AQS中的内部类。 在CLH同步队列中，一个节点表示一个线程，它保存着线程的引用（thread）、状态（waitStatus）、前驱节点（prev）、后继节点（next） 入列addWaiter(Node.EXCLUSIVE)方法会将当前线程封装成Node节点，追加在队尾。1234567891011121314151617private Node addWaiter(Node mode) &#123; //新建Node Node node = new Node(Thread.currentThread(), mode); //快速尝试添加尾节点 Node pred = tail; if (pred != null) &#123; node.prev = pred; //CAS设置尾节点 if (compareAndSetTail(pred, node)) &#123; pred.next = node; return node; &#125; &#125; //多次尝试 enq(node); return node; &#125;addWaiter(Node node)先通过快速尝试设置尾节点，如果失败，则调用enq(Node node)方法设置尾节点123456789101112131415161718private Node enq(final Node node) &#123; //多次尝试，直到成功为止 for (;;) &#123; Node t = tail; //tail不存在，设置为首节点 if (t == null) &#123; if (compareAndSetHead(new Node())) tail = head; &#125; else &#123; //设置为尾节点 node.prev = t; if (compareAndSetTail(t, node)) &#123; t.next = node; return t; &#125; &#125; &#125; &#125;在上面代码中，两个方法都是通过一个CAS方法compareAndSetTail(Node expect, Node update)来设置尾节点，该方法可以确保节点是线程安全添加的。在enq(Node node)方法中，AQS通过“死循环”的方式来保证节点可以正确添加，只有成功添加后，当前线程才会从该方法返回，否则会一直执行下去。过程图如下：出列CLH同步队列遵循FIFO，首节点的线程释放同步状态后，将会唤醒它的后继节点（next），而后继节点将会在获取同步状态成功时将自己设置为首节点，这个过程非常简单，head执行该节点并断开原首节点的next和当前节点的prev即可，注意在这个过程是不需要使用CAS来保证的，因为只有一个线程能够成功获取到同步状态。过程图如下：Jdk的并发包提供了各种锁及同步机制，其实现的核心类是AbstractQueuedSynchronizer，我们简称为AQS框架，它为不同场景提供了实现锁及同步机制的基本框架，为同步状态的原子性管理、线程的阻塞、线程的解除阻塞及排队管理提供了一种通用的机制。Jdk的并发包（juc）的作者是Doug Lea，但其中思想却是结合了多位大师的智慧，如果你想深入理解juc的相关理论可以参考Doug Lea写的《The_java.util.concurrent_Synchronizer_Framework》论文。从这里可以找到AQS的理论基础，包括框架的基本原理、需求、设计、实现思路、用法及性能，由于这些方面篇幅较大，本文不打算涉及所有方面，主要将针对AQS类的结构及相关操作进行分析。AQS框架它维护了一个volatile int state（代表共享资源）和一个FIFO线程等待队列（多线程争用资源被阻塞时会进入此队列）状态维护AQS用的是一个32位的整型来表示同步状态的，它是用volatile修饰的：1private volatile int state;在互斥锁中它表示着线程是否已经获取了锁，0未获取，1已经获取了，大于1表示重入数。同时AQS提供了getState()、setState()、compareAndSetState()方法来获取和修改该值：可重入锁指的是在一个线程中可以多次获取同一把锁，比如：一个线程在执行一个带锁的方法，该方法中又调用了另一个需要相同锁的方法，则该线程可以直接执行调用的方法，而无需重新获得锁。synchronized也可以看做重入锁所以可重入数大于1表示该线程可能调用了多个需要当前锁的方法，或同一个线程调用了多次lock()方法。队列AQS内部维护着一个FIFO的CLH队列，所以AQS并不支持基于优先级的同步策略。至于为何要选择CLH队列，主要在于CLH锁相对于MSC锁，他更加容易处理cancel和timeout，同时他具备进出队列快、无所、畅通无阻、检查是否有线程在等待也非常容易（head != tail,头尾指针不同）。当然相对于原始的CLH队列锁，AQS采用的是一种变种的CLH队列锁：原始CLH使用的locked自旋，而AQS的CLH则是在每个node里面使用一个状态字段来控制阻塞，而不是自旋。为了可以处理timeout和cancel操作，每个node维护一个指向前驱的指针。如果一个node的前驱被cancel，这个node可以前向移动使用前驱的状态字段。head结点使用的是傀儡结点。AQS定义两种资源共享方式：Exclusive（独占，只有一个线程能执行，如ReentrantLock）和Share（共享，多个线程可同时执行，如Semaphore/CountDownLatch），即我们常说的” 独占锁” 和 “共享锁”。不同的自定义同步器争用共享资源的方式也不同。自定义同步器在实现时只需要实现共享资源state的获取与释放方式即可，至于具体线程等待队列的维护（如获取资源失败入队/唤醒出队等），AQS已经在顶层实现好了。自定义同步器实现时主要实现以下几种方法：isHeldExclusively()：该线程是否正在独占资源。只有用到condition才需要去实现它。tryAcquire(int)：独占方式。尝试获取资源，成功则返回true，失败则返回false。tryRelease(int)：独占方式。尝试释放资源，成功则返回true，失败则返回false。tryAcquireShared(int)：共享方式。尝试获取资源。负数表示失败；0表示成功，但没有剩余可用资源；正数表示成功，且有剩余资源。tryReleaseShared(int)：共享方式。尝试释放资源，成功则返回true，失败则返回false。以ReentrantLock为例，state初始化为0，表示未锁定状态。A线程lock()时，会调用tryAcquire()独占该锁并将state+1。此后，其他线程再tryAcquire()时就会失败，直到A线程unlock()到state=0（即释放锁）为止，其它线程才有机会获取该锁。当然，释放锁之前，A线程自己是可以重复获取此锁的（state会累加），这就是可重入的概念。但要注意，获取多少次就要释放多么次，这样才能保证state是能回到零态的。再以CountDownLatch以例，任务分为N个子线程去执行，state也初始化为N（注意N要与线程个数一致）。这N个子线程是并行执行的，每个子线程执行完后countDown()一次，state会CAS减1。等到所有子线程都执行完后(即state=0)，会unpark()主调用线程，然后主调用线程就会从await()函数返回，继续后余动作。一般来说，自定义同步器要么是独占方法，要么是共享方式，他们也只需实现tryAcquire-tryRelease、tryAcquireShared-tryReleaseShared中的一种即可。但AQS也支持自定义同步器同时实现独占和共享两种方式，如ReentrantReadWriteLock，他是一个读写锁。参考博文【死磕Java并发】—–J.U.C之AQS：CLH同步队列Java并发之AQS详解JAVA中的CAS","categories":[{"name":"多线程","slug":"多线程","permalink":"http://www.fufan.me/categories/多线程/"}],"tags":[{"name":"多线程","slug":"多线程","permalink":"http://www.fufan.me/tags/多线程/"}]},{"title":"Mysql深入学习系列（一）——MyISAM与InnoDB比较","slug":"MyISAM与InnoDB比较（一）","date":"2017-05-15T09:39:00.000Z","updated":"2018-11-05T14:58:09.870Z","comments":true,"path":"2017/05/15/MyISAM与InnoDB比较（一）/","link":"","permalink":"http://www.fufan.me/2017/05/15/MyISAM与InnoDB比较（一）/","excerpt":"","text":"MyISAM是MySQL的默认数据库引擎（5.5版之前），由早期的ISAM（Indexed Sequential Access Method：有索引的顺序访问方法）所改良。虽然性能极佳，但却有一个缺点：不支持事务处理（transaction）。不过，在这几年的发展下，MySQL也导入了InnoDB（另一种数据库引擎），以强化参考完整性与并发违规处理机制，后来就逐渐取代MyISAM。InnoDB，是MySQL的数据库引擎之一，为MySQL AB发布binary的标准之一。InnoDB由Innobase Oy公司所开发，2006年五月时由甲骨文公司并购。与传统的ISAM与MyISAM相比，InnoDB的最大特色就是支持了ACID兼容的事务（Transaction）功能，类似于PostgreSQL。目前InnoDB采用双轨制授权，一是GPL授权，另一是专有软件授权。MyISAM与InnoDB的区别是什么？MyISAM构成上的区别：每个MyISAM在磁盘上存储成三个文件。第一个 文件的名字以表的名字开始，扩展名指出文件类型。.frm文件存储表定义。数据文件的扩 展名为.MYD (MYData)。索引文件的扩 展名是.MYI (MYIndex)。基于磁盘的资源是InnoDB表空间数据文件和它的日志文件，InnoDB 表的 大小只受限于操作系统文件的大小，一般为 2GB事务处理上方面:MyISAM类型的表强调的是性能，其执行数 度比InnoDB类型更快，但是不提供事务支持InnoDB提供事务支持事务，外部键等高级 数据库功能SELECTUPDATEINSERTDelete如果执行大量的SELECT，MyISAM是更好的选择1.如果你的数据执行大量的INSERT或UPDATE，出于性能方面的考虑，应该使用InnoDB表2.DELETE FROM table时，InnoDB不会重新建立表，而是一行一行的 删除。3.LOAD TABLE FROM MASTER操作对InnoDB是不起作用的，解决方法是首先把InnoDB表改成MyISAM表，导入数据后再改成InnoDB表，但是对于使用的额外的InnoDB特性（例如外键）的表不适用对AUTO_INCREMENT的 操作每表一个AUTO_INCREMEN列的内部处理。MyISAM为INSERT和UPDATE操 作自动更新这一列。这使得AUTO_INCREMENT列更快（至少10%）。在序列顶的值被删除之后就不 能再利用。(当AUTO_INCREMENT列被定义为多列索引的最后一列， 可以出现重使用从序列顶部删除的值的情况）。AUTO_INCREMENT值可用ALTER TABLE或myisamch来重置对于AUTO_INCREMENT类型的字段，InnoDB中必须包含只有该字段的索引，但 是在MyISAM表中，可以和其他字段一起建立联 合索引更好和更快的auto_increment处理如果你为一个表指定AUTO_INCREMENT列，在数据词典里的InnoDB表句柄包含一个名为自动增长计数 器的计数器，它被用在为该列赋新值。自动增长计数 器仅被存储在主内存中，而不是存在磁盘上表的具体行数select count() from table,MyISAM只要简单的读出保存好的行数，注意的是，当count()语句包含 where条件时，两种表的操作是一样的InnoDB 中不 保存表的具体行数，也就是说，执行select count(*) from table时，InnoDB要扫描一遍整个表来计算有多少行锁表锁提供行锁(locking on row level)，提供与 Oracle 类型一致的不加锁读取(non-locking read in SELECTs)，另外，InnoDB表的行锁也不是绝对的，如果在执 行一个SQL语句时MySQL不能确定要扫描的范围，InnoDB表同样会锁全表，例如update table set num=1 where name like “%aaa%”存储结构MyISAM：每个MyISAM在磁盘上存储成三个文件。第一个文件的名字以表的名字开始，扩展名指出文件类型。.frm文件存储表定义。数据文件的扩展名为.MYD (MYData)。索引文件的扩展名是.MYI (MYIndex)。InnoDB：所有的表都保存在同一个数据文件中（也可能是多个文件，或者是独立的表空间文件），InnoDB表的大小只受限于操作系统文件的大小，一般为2GB。存储空间MyISAM：可被压缩，存储空间较小。支持三种不同的存储格式：静态表(默认，但是注意数据末尾不能有空格，会被去掉)、动态表、压缩表。InnoDB：需要更多的内存和存储，它会在主内存中建立其专用的缓冲池用于高速缓冲数据和索引。可移植性、备份及恢复MyISAM：数据是以文件的形式存储，所以在跨平台的数据转移中会很方便。在备份和恢复时可单独针对某个表进行操作。InnoDB：免费的方案可以是拷贝数据文件、备份 binlog，或者用 mysqldump，在数据量达到几十G的时候就相对痛苦了。事务支持MyISAM：强调的是性能，每次查询具有原子性,其执行数度比InnoDB类型更快，但是不提供事务支持。InnoDB：提供事务支持事务，外部键等高级数据库功能。 具有事务(commit)、回滚(rollback)和崩溃修复能力(crash recovery capabilities)的事务安全(transaction-safe (ACID compliant))型表。AUTO_INCREMENTMyISAM：可以和其他字段一起建立联合索引。引擎的自动增长列必须是索引，如果是组合索引，自动增长可以不是第一列，他可以根据前面几列进行排序后递增。InnoDB：InnoDB中必须包含只有该字段的索引。引擎的自动增长列必须是索引，如果是组合索引也必须是组合索引的第一列。表锁差异MyISAM：只支持表级锁，用户在操作myisam表时，select，update，delete，insert语句都会给表自动加锁，如果加锁以后的表满足insert并发的情况下，可以在表的尾部插入新的数据。InnoDB：支持事务和行级锁，是innodb的最大特色。行锁大幅度提高了多用户并发操作的新能。但是InnoDB的行锁，只是在WHERE的主键是有效的，非主键的WHERE都会锁全表的。全文索引MyISAM：支持 FULLTEXT类型的全文索引InnoDB：不支持FULLTEXT类型的全文索引，但是innodb可以使用sphinx插件支持全文索引，并且效果更好。表主键MyISAM：允许没有任何索引和主键的表存在，索引都是保存行的地址。InnoDB：如果没有设定主键或者非空唯一索引，就会自动生成一个6字节的主键(用户不可见)，数据是主索引的一部分，附加索引保存的是主索引的值。表的具体行数MyISAM：保存有表的总行数，如果select count() from table;会直接取出出该值。InnoDB：没有保存表的总行数，如果使用select count() from table；就会遍历整个表，消耗相当大，但是在加了wehre条件后，myisam和innodb处理的方式都一样。CURD操作MyISAM：如果执行大量的SELECT，MyISAM是更好的选择。InnoDB：如果你的数据执行大量的INSERT或UPDATE，出于性能方面的考虑，应该使用InnoDB表。DELETE 从性能上InnoDB更优，但DELETE FROM table时，InnoDB不会重新建立表，而是一行一行的删除，在innodb上如果要清空保存有大量数据的表，最好使用truncate table这个命令。外键MyISAM：不支持InnoDB：支持通过上述的分析，基本上可以考虑使用InnoDB来替代MyISAM引擎了，原因是InnoDB自身很多良好的特点，比如事务支持、存储 过程、视图、行级锁定等等，在并发很多的情况下，相信InnoDB的表现肯定要比MyISAM强很多。另外，任何一种表都不是万能的，只用恰当的针对业务类型来选择合适的表类型，才能最大的发挥MySQL的性能优势。如果不是很复杂的Web应用，非关键应用，还是可以继续考虑MyISAM的，这个具体情况可以自己斟酌。总结MyISAM：每个MyISAM在磁盘上存储成三个文件。第一个文件的名字以表的名字开始，扩展名指出文件类型。.frm文件存储表定义。数据文件的扩展名为.MYD (MYData)。MyISAM表格可以被压缩，而且它们支持全文搜索。不支持事务，而且也不支持外键。如果事物回滚将造成不完全回滚，不具有原子性。在进行updata时进行表锁，并发量相对较小。如果执行大量的SELECT，MyISAM是更好的选择。MyISAM的索引和数据是分开的，并且索引是有压缩的，内存使用率就对应提高了不少。能加载更多索引，而Innodb是索引和数据是紧密捆绑的，没有使用压缩从而会造成Innodb比MyISAM体积庞大不小MyISAM缓存在内存的是索引，不是数据。而InnoDB缓存在内存的是数据，相对来说，服务器内存越大，InnoDB发挥的优势越大。优点：查询数据相对较快，适合大量的select，可以全文索引。缺点：不支持事务，不支持外键，并发量较小，不适合大量updateInnoDB这种类型是事务安全的。.它与BDB类型具有相同的特性,它们还支持外键。InnoDB表格速度很快。具有比BDB还丰富的特性,因此如果需要一个事务安全的存储引擎，建议使用它。在update时表进行行锁，并发量相对较大。如果你的数据执行大量的INSERT或UPDATE，出于性能方面的考虑，应该使用InnoDB表。优点：支持事务，支持外键，并发量较大，适合大量update缺点：查询数据相对较快，不适合大量的select对于支持事物的InnoDB类型的表，影响速度的主要原因是AUTOCOMMIT默认设置是打开的，而且程序没有显式调用BEGIN 开始事务，导致每插入一条都自动Commit，严重影响了速度。可以在执行sql前调用begin，多条sql形成一个事物（即使autocommit打开也可以），将大大提高性能。基本的差别为：MyISAM类型不支持事务处理等高级处理，而InnoDB类型支持。MyISAM类型的表强调的是性能，其执行数度比InnoDB类型更快，但是不提供事务支持，而InnoDB提供事务支持已经外部键等高级数据库功能。参考博文MySQL存储引擎中的MyISAM和InnoDB区别详解MySQL存储引擎之Myisam和Innodb总结性梳理","categories":[{"name":"mysql","slug":"mysql","permalink":"http://www.fufan.me/categories/mysql/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://www.fufan.me/tags/mysql/"}]},{"title":"分布式一致性——从CAP到BASE","slug":"分布式一致性——从CAP到BASE","date":"2017-05-11T07:24:00.000Z","updated":"2018-11-05T07:25:25.467Z","comments":true,"path":"2017/05/11/分布式一致性——从CAP到BASE/","link":"","permalink":"http://www.fufan.me/2017/05/11/分布式一致性——从CAP到BASE/","excerpt":"","text":"问题的提出火车站售票假如说我们的终端用户是一位经常坐火车的旅行家，通常他是去车站的售票处购买车 票，然后拿着车票去检票口，再坐上火车，开始一段美好的旅行—-一切似乎都是那么和谐。想象一下，如果他选择的目的地是杭州，而某一趟开往杭州的火车 只剩下最后一张车票，可能在同一时刻，不同售票窗口的另一位乘客也购买了同一张车票。假如说售票系统没有进行一致性的保障，两人都购票成功了。而在检票口 检票的时候，其中一位乘客会被告知他的车票无效—-当然，现代的中国铁路售票系统已经很少出现这样的问题了。但在这个例子中我们可以看出，终端用户对 于系统的需求非常简单：“请售票给我，如果没有余票了，请在售票的时候就告诉我票是无效的”这就对购票系统提出了严格的一致性要求—-系统的数据（本例中指的就是那趟开往杭州的火车的余票数）无论在哪个售票窗口，每时每刻都必须是准确无误的！银行转账假如我们的终端用户是一位刚毕业的大学生，通常在拿到第一个月工资的时候，都会选 择向家里汇款。当他来到银行柜台，完成转账操作后，银行的柜台服务员会友善地提醒他：”您的转账将在N个工作日后到账！”。此时这名毕业生有一定的沮丧， 会对那名柜台服务员叮嘱：”好吧，多久没关系，钱不要少就好了！”—-这也成为了几乎所有用户对于现代银行系统最基本的需求网上购物假如说我们的终端用户是一位网购达人，当他看见一件库存量为5的心仪商品，会迅速地确认购买，写下收货地址，然后下单—-然而，在下单的那个瞬间，系统可能会告知该用户：”库存量不足！”。此时绝大部分消费者都会抱怨自己动作太慢，使得心爱的商品被其他人抢走了。但其实有过网购系统开发经验的工程师一定明白，在商品详情页上显示的那个库存量，通常不是该商品的真实库存量，只有在真正下单购买的时候，系统才会检查该商品的真实库存量。但是，谁在意呢？问题的解读对于上面三个例子，相信大家一定看出来了，我们的终端用户在使用不同的计算机产品时对于数据一致性的需求是不一样的：1、有些系统，既要快速地响应用户，同时还要保证系统的数据对于任意客户端都是真实可靠的，就像火车站售票系统2、有些系统，需要为用户保证绝对可靠的数据安全，虽然在数据一致性上存在延时，但最终务必保证严格的一致性，就像银行的转账系统3、有些系统，虽然向用户展示了一些可以说是”错误”的数据，但是在整个系统使用过程中，一定会在某一个流程上对系统数据进行准确无误的检查，从而避免用户发生不必要的损失，就像网购系统分布式一致性在分布式系统中要解决的一个重要问题就是数据的复制。在我们的日常开发经验中，相 信很多开发人员都遇到过这样的问题：假设客户端C1将系统中的一个值K由V1更新为V2，但客户端C2无法立即读取到K的最新值，需要在一段时间之后才能 读取到。这很正常，因为数据库复制之间存在延时。分布式系统对于数据的复制需求一般都来自于以下两个原因：1、为了增加系统的可用性，以防止单点故障引起的系统不可用2、提高系统的整体性能，通过负载均衡技术，能够让分布在不同地方的数据副本都能够为用户提供服务数据复制在可用性和性能方面给分布式系统带来的巨大好处是不言而喻的，然而数据复制所带来的一致性挑战，也是每一个系统研发人员不得不面对的。所谓分布一致性问题，是指在分布式环境中引入数据复制机制之后，不同数据节点之间 可能出现的，并无法依靠计算机应用程序自身解决的数据不一致的情况。简单讲，数据一致性就是指在对一个副本数据进行更新的时候，必须确保也能够更新其他的 副本，否则不同副本之间的数据将不一致。那么如何解决这个问题？一种思路是”既然是由于延时动作引起的问题，那我可以将写入的动作阻塞，直到数据复制完成后，才完成写入动作”。 没错，这似乎能解决问题，而且有一些系统的架构也确实直接使用了这个思路。但这个思路在解决一致性问题的同时，又带来了新的问题：写入的性能。如果你的应 用场景有非常多的写请求，那么使用这个思路之后，后续的写请求都将会阻塞在前一个请求的写操作上，导致系统整体性能急剧下降。总得来说，我们无法找到一种能够满足分布式系统所有系统属性的分布式一致性解决方案。因此，如何既保证数据的一致性，同时又不影响系统运行的性能，是每一个分布式系统都需要重点考虑和权衡的。于是，一致性级别由此诞生：强一致性这种一致性级别是最符合用户直觉的，它要求系统写入什么，读出来的也会是什么，用户体验好，但实现起来往往对系统的性能影响大弱一致性这种一致性级别约束了系统在写入成功后，不承诺立即可以读到写入的值，也不久承诺多久之后数据能够达到一致，但会尽可能地保证到某个时间级别（比如秒级别）后，数据能够达到一致状态最终一致性最终一致性是弱一致性的一个特例，系统会保证在一定时间内，能够达到一个数据一致的状态。这里之所以将最终一致性单独提出来，是因为它是弱一致性中非常推崇的一种一致性模型，也是业界在大型分布式系统的数据一致性上比较推崇的模型分布式环境的各种问题分布式系统体系结构从其出现之初就伴随着诸多的难题和挑战：通信异常从集中式向分布式演变的过程中，必然引入网络因素，由于网络本身的不可靠性，因此 也引入了额外的问题。分布式系统需要在各个节点之间进行网络通信，因此每次网络通信都会伴随着网络不可用的风险，网络光纤、路由器或是DNS等硬件设备或 是系统不可用都会导致最终分布式系统无法顺利完成一次网络通信。另外，即使分布式系统各个节点之间的网络通信能够正常进行，其延时也会大于单机操作。通常 我们认为现代计算机体系结构中，单机内存访问的延时在纳秒数量级（通常是10ns），而正常的一次网络通信的延迟在0.1~1ms左右（相当于内存访问延 时的105倍），如此巨大的延时差别，也会影响到消息的收发过程，因此消息丢失和消息延迟变得非常普遍网络分区当网络由于发生异常情况，导致分布式系统中部分节点之间的网络延时不断增大，最终导致组成分布式系统的所有节点中，只有部分节点之间能够正常通信，而另一些节点则不能—-我们将这个现象称为网络分区。当网络分区出现时，分布式系统会出现局部小集群，在极端情况下，这些局部小集群会独立完成原本需要整个分布式系统才能完成的功能，包括对数据的事物处理，这就对分布式一致性提出了非常大的挑战三态上面两点，我们已经了解到在分布式环境下，网络可能会出现各式各样的问题，因此分布式系统的每一次请求与响应，存在特有的三态概念，即成功、失败、超时。 在传统的单机系统中，应用程序在调用一个函数之后，能够得到一个非常明确的响应：成功或失败。而在分布式系统中，由于网络是不可靠的，虽然在绝大部分情况 下，网络通信也能够接受到成功或失败的响应，当时当网络出现异常的情况下，就可能会出现超时现象，通常有以下两种情况：（1）由于网络原因，该请求并没有被成功地发送到接收方，而是在发送过程中就发生了消息丢失现象（2）该请求成功地被接收方接收后，进行了处理，但是在将响应反馈给发送方的过程中，发生了消息丢失现象当出现这样的超时现象时，网络通信的发起方是无法确定当前请求是否被成功处理的节点故障节点故障则是分布式环境下另一个比较常见的问题，指的是组成分布式系统的服务器节点出现的宕机或”僵死”现象，通常根据经验来说，每个节点都有可能出现故障，并且每天都在发生分布式事物随着分布式计算的发展，事物在分布式计算领域也得到了广泛的应用。在单机数据库中，我们很容易能够实现一套满足ACID特性的事物处理系统，但在分布式数据库中，数据分散在各台不同的机器上，如何对这些数据进行分布式的事物处理具有非常大的挑战。分布式事物是指事物的参与者、支持事物的服务器、资源服务器以及事物管理器分别位于分布式系统的不同节点上，通常一个分布式事物中会涉及对多个数据源或业务系统的操作。可以设想一个最典型的分布式事物场景：一个跨银行的转账操作涉及调用两个异地的银 行服务，其中一个是本地银行提供的取款服务，另一个则是目标银行提供的存款服务，这两个服务本身是无状态并且相互独立的，共同构成了一个完整的分布式事 物。如果从本地银行取款成功，但是因为某种原因存款服务失败了，那么就必须回滚到取款之前的状态，否则用户可能会发现自己的钱不翼而飞了。从这个例子可以看到，一个分布式事务可以看做是多个分布式的操作序列组成的，例如 上面例子的取款服务和存款服务，通常可以把这一系列分布式的操作序列称为子事物。因此，分布式事务也可以被定义为一种嵌套型的事物，同时也就具有了 ACID事物特性。但由于在分布式事务中，各个子事物的执行是分布式的，因此要实现一种能够保证ACID特性的分布式事物处理系统就显得格外复杂。CAP理论一个经典的分布式系统理论。CAP理论告诉我们：一个分布式系统不可能同时满足一致性（C：Consistency）、可用性（A：Availability）和分区容错性（P：Partition tolerance）这三个基本需求，最多只能同时满足其中两项。1、一致性在分布式环境下，一致性是指数据在多个副本之间能否保持一致的特性。在一致性的需求下，当一个系统在数据一致的状态下执行更新操作后，应该保证系统的数据仍然处于一直的状态。对于一个将数据副本分布在不同分布式节点上的系统来说，如果对第一个节点的数据进 行了更新操作并且更新成功后，却没有使得第二个节点上的数据得到相应的更新，于是在对第二个节点的数据进行读取操作时，获取的依然是老数据（或称为脏数 据），这就是典型的分布式数据不一致的情况。在分布式系统中，如果能够做到针对一个数据项的更新操作执行成功后，所有的用户都可以读取到其最新的值，那么 这样的系统就被认为具有强一致性2、可用性可用性是指系统提供的服务必须一直处于可用的状态，对于用户的每一个操作请求总是能够在有限的时间内返回结果。这里的重点是”有限时间内”和”返回结果”。“有限时间内”是指，对于用户的一个操作请求，系统必须能够在指定的时间内返回对 应的处理结果，如果超过了这个时间范围，那么系统就被认为是不可用的。另外，”有限的时间内”是指系统设计之初就设计好的运行指标，通常不同系统之间有很 大的不同，无论如何，对于用户请求，系统必须存在一个合理的响应时间，否则用户便会对系统感到失望。“返回结果”是可用性的另一个非常重要的指标，它要求系统在完成对用户请求的处理后，返回一个正常的响应结果。正常的响应结果通常能够明确地反映出队请求的处理结果，即成功或失败，而不是一个让用户感到困惑的返回结果。3、分区容错性分区容错性约束了一个分布式系统具有如下特性：分布式系统在遇到任何网络分区故障的时候，仍然需要能够保证对外提供满足一致性和可用性的服务，除非是整个网络环境都发生了故障。网络分区是指在分布式系统中，不同的节点分布在不同的子网络（机房或异地网络） 中，由于一些特殊的原因导致这些子网络出现网络不连通的状况，但各个子网络的内部网络是正常的，从而导致整个系统的网络环境被切分成了若干个孤立的区域。 需要注意的是，组成一个分布式系统的每个节点的加入与退出都可以看作是一个特殊的网络分区。既然一个分布式系统无法同时满足一致性、可用性、分区容错性三个特点，所以我们就需要抛弃一样：选 择说 明CA放弃分区容错性，加强一致性和可用性，其实就是传统的单机数据库的选择AP放弃一致性（这里说的一致性是强一致性），追求分区容错性和可用性，这是很多分布式系统设计时的选择，例如很多NoSQL系统就是如此CP放弃可用性，追求一致性和分区容错性，基本不会选择，网络问题会直接让整个系统不可用需要明确的一点是，对于一个分布式系统而言，分区容错性是一个最基本的要求。因为 既然是一个分布式系统，那么分布式系统中的组件必然需要被部署到不同的节点，否则也就无所谓分布式系统了，因此必然出现子网络。而对于分布式系统而言，网 络问题又是一个必定会出现的异常情况，因此分区容错性也就成为了一个分布式系统必然需要面对和解决的问题。因此系统架构师往往需要把精力花在如何根据业务 特点在C（一致性）和A（可用性）之间寻求平衡。BASE理论BASE是Basically Available（基本可用）、Soft state（软状态）和Eventually consistent（最终一致性）三个短语的缩写。BASE理论是对CAP中一致性和可用性权衡的结果，其来源于对大规模互联网系统分布式实践的总结， 是基于CAP定理逐步演化而来的。BASE理论的核心思想是：即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。接下来看一下BASE中的三要素：基本可用基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性—-注意，这绝不等价于系统不可用。比如：（1）响应时间上的损失。正常情况下，一个在线搜索引擎需要在0.5秒之内返回给用户相应的查询结果，但由于出现故障，查询结果的响应时间增加了1~2秒（2）系统功能上的损失：正常情况下，在一个电子商务网站上进行购物的时候，消费者几乎能够顺利完成每一笔订单，但是在一些节日大促购物高峰的时候，由于消费者的购物行为激增，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面软状态软状态指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时最终一致性最终一致性强调的是所有的数据副本，在经过一段时间的同步之后，最终都能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。总的来说，BASE理论面向的是大型高可用可扩展的分布式系统，和传统的事物ACID特性是相反的，它完全不同于ACID的强一致性模型，而是通过牺牲强一致性来获得可用性，并允许数据在一段时间内是不一致的，但最终达到一致状态。但同时，在实际的分布式场景中，不同业务单元和组件对数据一致性的要求是不同的，因此在具体的分布式系统架构设计过程中，ACID特性和BASE理论往往又会结合在一起。","categories":[{"name":"分布式","slug":"分布式","permalink":"http://www.fufan.me/categories/分布式/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"http://www.fufan.me/tags/分布式/"}]},{"title":"消息队列和rpc的比较","slug":"消息队列和rpc的比较","date":"2017-05-10T06:46:00.000Z","updated":"2018-11-05T06:54:35.357Z","comments":true,"path":"2017/05/10/消息队列和rpc的比较/","link":"","permalink":"http://www.fufan.me/2017/05/10/消息队列和rpc的比较/","excerpt":"","text":"消息队列和RPC远程服务调用是微服务目前使用最多的处理任务和调用服务的两种方式，然而什么场景下适合用哪个是非常关键的。消息队列概述消息队列中间件是分布式系统中重要的组件，主要解决应用解耦，异步消息，流量削锋等问题，实现高性能，高可用，可伸缩和最终一致性架构。目前使用较多的消息队列有ActiveMQ，RabbitMQ，ZeroMQ，Kafka，MetaMQ，RocketMQ，当然也可以用有些缓存中间件来模拟消息队列，如redis。应用场景1. 异步处理场景说明：用户注册后，需要发注册邮件和注册短信。传统的做法有两种 1.串行的方式；2.并行方式a、串行方式：将注册信息写入数据库成功后，发送注册邮件，再发送注册短信。以上三个任务全部完成后，返回给客户端。b、并行方式：将注册信息写入数据库成功后，发送注册邮件的同时，发送注册短信。以上三个任务完成后，返回给客户端。与串行的差别是，并行的方式可以提高处理的时间假设三个业务节点每个使用50毫秒钟，不考虑网络等其他开销，则串行方式的时间是150毫秒，并行的时间可能是100毫秒。因为CPU在单位时间内处理的请求数是一定的，假设CPU1秒内吞吐量是100次。则串行方式1秒内CPU可处理的请求量是7次（1000/150）。并行方式处理的请求量是10次（1000/100）小结：如以上案例描述，传统的方式系统的性能（并发量，吞吐量，响应时间）会有瓶颈。如何解决这个问题呢？引入消息队列，将不是必须的业务逻辑，异步处理。改造后的架构如下：按照以上约定，用户的响应时间相当于是注册信息写入数据库的时间，也就是50毫秒。注册邮件，发送短信写入消息队列后，直接返回，因此写入消息队列的速度很快，基本可以忽略，因此用户的响应时间可能是50毫秒。因此架构改变后，系统的吞吐量提高到每秒20 QPS。比串行提高了3倍，比并行提高了两倍。2. 应用解耦场景说明：用户下单后，订单系统需要通知库存系统。传统的做法是，订单系统调用库存系统的接口。如下图：传统模式的缺点：假如库存系统无法访问，则订单减库存将失败，从而导致订单失败，订单系统与库存系统耦合如何解决以上问题呢？引入应用消息队列后的方案，如下图：订单系统：用户下单后，订单系统完成持久化处理，将消息写入消息队列，返回用户订单下单成功库存系统：订阅下单的消息，采用拉/推的方式，获取下单信息，库存系统根据下单信息，进行库存操作假如：在下单时库存系统不能正常使用。也不影响正常下单，因为下单后，订单系统写入消息队列就不再关心其他的后续操作了。实现订单系统与库存系统的应用解耦3. 流量削锋流量削锋也是消息队列中的常用场景，一般在秒杀或团抢活动中使用广泛。应用场景：秒杀活动，一般会因为流量过大，导致流量暴增，应用挂掉。为解决这个问题，一般需要在应用前端加入消息队列。a、可以控制活动的人数b、可以缓解短时间内高流量压垮应用用户的请求，服务器接收后，首先写入消息队列。假如消息队列长度超过最大数量，则直接抛弃用户请求或跳转到错误页面。秒杀业务根据消息队列中的请求信息，再做后续处理4. 日志处理日志处理是指将消息队列用在日志处理中，比如Kafka的应用，解决大量日志传输的问题。架构简化如下日志采集客户端，负责日志数据采集，定时写受写入Kafka队列Kafka消息队列，负责日志数据的接收，存储和转发日志处理应用：订阅并消费kafka队列中的日志数据5. 消息通讯消息通讯是指，消息队列一般都内置了高效的通信机制，因此也可以用在纯的消息通讯。比如实现点对点消息队列，或者聊天室等点对点通讯：客户端A和客户端B使用同一队列，进行消息通讯。聊天室通讯：客户端A，客户端B，客户端N订阅同一主题，进行消息发布和接收。实现类似聊天室效果。以上实际是消息队列的两种消息模式，点对点或发布订阅模式。模型为示意图，供参考。中间件项目架构实例电商系统消息队列采用高可用，可持久化的消息中间件。比如Active MQ，Rabbit MQ，Rocket Mq。（1）应用将主干逻辑处理完成后，写入消息队列。消息发送是否成功可以开启消息的确认模式。（消息队列返回消息接收成功状态后，应用再返回，这样保障消息的完整性）（2）扩展流程（发短信，配送处理）订阅队列消息。采用推或拉的方式获取消息并处理。（3）消息将应用解耦的同时，带来了数据一致性问题，可以采用最终一致性方式解决。比如主数据写入数据库，扩展应用根据消息队列，并结合数据库方式实现基于消息队列的后续处理。日志收集系统分为Zookeeper注册中心，日志收集客户端，Kafka集群和Storm集群（OtherApp）四部分组成。Zookeeper注册中心，提出负载均衡和地址查找服务日志收集客户端，用于采集应用系统的日志，并将数据推送到kafka队列Kafka集群：接收，路由，存储，转发等消息处理Storm集群：与OtherApp处于同一级别，采用拉的方式消费队列中的数据消息模型讲消息队列就不得不提JMS 。JMS（Java Message Service,Java消息服务）API是一个消息服务的标准/规范，允许应用程序组件基于JavaEE平台创建、发送、接收和读取消息。它使分布式通信耦合度更低，消息服务更加可靠以及异步性。在JMS标准中，有两种消息模型P2P（Point to Point）,Publish/Subscribe(Pub/Sub)。p2p模式P2P模式包含三个角色：消息队列（Queue），发送者(Sender)，接收者(Receiver)。每个消息都被发送到一个特定的队列，接收者从队列中获取消息。队列保留着消息，直到他们被消费或超时。P2P的特点每个消息只有一个消费者（Consumer）(即一旦被消费，消息就不再在消息队列中)发送者和接收者之间在时间上没有依赖性，也就是说当发送者发送了消息之后，不管接收者有没有正在运行，它不会影响到消息被发送到队列接收者在成功接收消息之后需向队列应答成功如果希望发送的每个消息都会被成功处理的话，那么需要P2P模式。Pub/sub模式包含三个角色主题（Topic），发布者（Publisher），订阅者（Subscriber） 多个发布者将消息发送到Topic,系统将这些消息传递给多个订阅者。Pub/Sub的特点每个消息可以有多个消费者发布者和订阅者之间有时间上的依赖性。针对某个主题（Topic）的订阅者，它必须创建一个订阅者之后，才能消费发布者的消息为了消费消息，订阅者必须保持运行的状态为了缓和这样严格的时间相关性，JMS允许订阅者创建一个可持久化的订阅。这样，即使订阅者没有被激活（运行），它也能接收到发布者的消息。如果希望发送的消息可以不被做任何处理、或者只被一个消息者处理、或者可以被多个消费者处理的话，那么可以采用Pub/Sub模型。常用消息队列一般商用的容器，比如WebLogic，JBoss，都支持JMS标准，开发上很方便。但免费的比如Tomcat，Jetty等则需要使用第三方的消息中间件。本部分内容介绍常用的消息中间件（Active MQ,Rabbit MQ，Zero MQ,Kafka）以及他们的特点。ActiveMqActiveMQ 是Apache出品，最流行的，能力强劲的开源消息总线。ActiveMQ 是一个完全支持JMS1.1和J2EE 1.4规范的 JMS Provider实现，尽管JMS规范出台已经是很久的事情了，但是JMS在当今的J2EE应用中间仍然扮演着特殊的地位。ActiveMQ特性如下：⒈ 多种语言和协议编写客户端。语言: Java,C,C++,C#,Ruby,Perl,Python,PHP。应用协议： OpenWire,Stomp REST,WS Notification,XMPP,AMQP⒉ 完全支持JMS1.1和J2EE 1.4规范 （持久化，XA消息，事务)⒊ 对Spring的支持，ActiveMQ可以很容易内嵌到使用Spring的系统里面去，而且也支持Spring2.0的特性⒋ 通过了常见J2EE服务器（如 Geronimo,JBoss 4,GlassFish,WebLogic)的测试，其中通过JCA 1.5 resource adaptors的配置，可以让ActiveMQ可以自动的部署到任何兼容J2EE 1.4 商业服务器上⒌ 支持多种传送协议：in-VM,TCP,SSL,NIO,UDP,JGroups,JXTA⒍ 支持通过JDBC和journal提供高速的消息持久化⒎ 从设计上保证了高性能的集群，客户端-服务器，点对点⒏ 支持Ajax⒐ 支持与Axis的整合⒑ 可以很容易得调用内嵌JMS provider，进行测试KafkaKafka是一种高吞吐量的分布式发布订阅消息系统，它可以处理消费者规模的网站中的所有动作流数据。 这种动作（网页浏览，搜索和其他用户的行动）是在现代网络上的许多社会功能的一个关键因素。 这些数据通常是由于吞吐量的要求而通过处理日志和日志聚合来解决。 对于像Hadoop的一样的日志数据和离线分析系统，但又要求实时处理的限制，这是一个可行的解决方案。Kafka的目的是通过Hadoop的并行加载机制来统一线上和离线的消息处理，也是为了通过集群机来提供实时的消费。Kafka是一种高吞吐量的分布式发布订阅消息系统，有如下特性：通过O(1)的磁盘数据结构提供消息的持久化，这种结构对于即使数以TB的消息存储也能够保持长时间的稳定性能。（文件追加的方式写入数据，过期的数据定期删除）高吞吐量：即使是非常普通的硬件Kafka也可以支持每秒数百万的消息支持通过Kafka服务器和消费机集群来分区消息支持Hadoop并行数据加载Kafka相关概念BrokerKafka集群包含一个或多个服务器，这种服务器被称为broker[5]Topic每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic。（物理上不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个broker上但用户只需指定消息的Topic即可生产或消费数据而不必关心数据存于何处）PartitionParition是物理上的概念，每个Topic包含一个或多个Partition.Producer负责发布消息到Kafka brokerConsumer消息消费者，向Kafka broker读取消息的客户端。Consumer Group每个Consumer属于一个特定的Consumer Group（可为每个Consumer指定group name，若不指定group name则属于默认的group）。一般应用在大数据日志处理或对实时性（少量延迟），可靠性（少量丢数据）要求稍低的场景使用。RPC远程调用一. 区别1. 消息队列能够积压消息,让消费者可以按照自己的节奏处理消息,但是RPC不能.2. 消息队列是一个异步的过程(生产者发送消息之后,不会等待消息的处理),RPC是一个同步的过程.3. 消息队列的生产者不能得知谁消费了消息,消费结果是否成功,而RPC的调用者明确知道被调用者是谁,处理结果也能获取到.4. 由于消息队列在生产者和消费者之间还有一个queue节点,系统性能除了受自身因素影响外还受queue节点影响,而RPC没有中间节点,,系统性能只受自己的影响.二. 适用场景由异同大致就能理解出两者的适用场景是什么:1. 消息队列能够让服务器的负载不会过高,降低了并发度,所以效率受到了影响,又由于消息队列是一个异步的过程,且生产者不能得知消费者的信息,所以消息队列一般用于实时性要求不高的花费时间的操作.2. RPC是一个同步的过程,可能会因为突然高的并发量导致系统出问题,但是RPC具有很高的实时性,所以他一般用户需要立即返回结果的操作.参考博文关于消息队列的使用","categories":[{"name":"消息队列","slug":"消息队列","permalink":"http://www.fufan.me/categories/消息队列/"}],"tags":[{"name":"消息队列","slug":"消息队列","permalink":"http://www.fufan.me/tags/消息队列/"}]},{"title":"java多线程系列（三）——锁","slug":"java多线程系列（三）——锁","date":"2017-05-04T17:58:00.000Z","updated":"2018-11-04T18:00:43.820Z","comments":true,"path":"2017/05/05/java多线程系列（三）——锁/","link":"","permalink":"http://www.fufan.me/2017/05/05/java多线程系列（三）——锁/","excerpt":"","text":"这里整理了Java中的各种锁：公平锁、非公平锁、自旋锁、可重入锁、偏向锁、轻量级锁、重量级锁、读写锁、互斥锁等待。公平锁和非公平锁公平锁是指多个线程在等待同一个锁时，必须按照申请锁的先后顺序来一次获得锁。公平锁的好处是等待锁的线程不会饿死，但是整体效率相对低一些；非公平锁的好处是整体效率相对高一些，但是有些线程可能会饿死或者说很早就在等待锁，但要等很久才会获得锁。其中的原因是公平锁是严格按照请求所的顺序来排队获得锁的，而非公平锁时可以抢占的，即如果在某个时刻有线程需要获取锁，而这个时候刚好锁可用，那么这个线程会直接抢占，而这时阻塞在等待队列的线程则不会被唤醒。new ReentrantLock(true)，用参数来觉得是否为公平锁。自旋锁Java的线程是映射到操作系统的原生线程之上的，如果要阻塞或唤醒一个线程，都需要操作系统来帮忙完成，这就需要从用户态转换到核心态中，因此状态装换需要耗费很多的处理器时间，对于代码简单的同步块（如被synchronized修饰的getter()和setter()方法），状态转换消耗的时间有可能比用户代码执行的时间还要长。自旋等待不能代替阻塞。自旋等待本身虽然避免了线程切换的开销，但它是要占用处理器时间的，因此，如果锁被占用的时间很短，自旋当代的效果就会非常好，反之，如果锁被占用的时间很长，那么自旋的线程只会拜拜浪费处理器资源。因此，自旋等待的时间必须要有一定的限度，如果自旋超过了限定次数（默认是10次，可以使用-XX:PreBlockSpin来更改）没有成功获得锁，就应当使用传统的方式去挂起线程了。自旋锁在JDK1.4.2中引入，使用-XX:+UseSpinning来开启。JDK6中已经变为默认开启，并且引入了自适应的自旋锁。自适应意味着自旋的时间不在固定了，而是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定。自旋是在轻量级锁中使用的，在重量级锁中，线程不使用自旋。如果在同一个锁对象上，自旋等待刚刚成功获得过锁，并且持有锁的线程正在运行中，那么虚拟机就会认为这次自旋也是很有可能再次成功，进而它将允许自旋等待持续相对更长的时间，比如100次循环。另外，如果对于某个锁，自旋很少成功获得过，那在以后要获取这个锁时将可能省略掉自旋过程，以避免浪费处理器资源。可重入锁可重入锁，也叫做递归锁，指的是同一线程外层函数获得锁之后 ，内层递归函数仍然有获取该锁的代码，但不受影响。在JAVA环境下 ReentrantLock 和synchronized 都是可重入锁。可重入锁最大的作用是避免死锁。类锁和对象锁类锁：作用在类上的，两种表现形式，一是静态方法被synchronized修饰，二是通过锁住类名.class的代码块方式1234public static synchronized void method1()&#123;&#125;public void method2()&#123; synchronized(LockStrategy.class)&#123;&#125; &#125;对象锁：普通方法被synchronized修饰，二是通过锁住object的代码块方式12345678910public synchronized void method4()&#123;&#125;public void method5()&#123; synchronized(this)&#123;&#125;&#125;public void method6()&#123; synchronized(object1)&#123;&#125;&#125;偏向锁、轻量级锁、重量级锁synchronized的偏向锁、轻量级锁以及重量级锁是通过Java对象头实现的。博主在Java对象大小内幕浅析中提到了Java对象的内存布局分为：对象头、实例数据和对其填充，而对象头又可以分为”Mark Word”和类型指针klass。”Mark Word”是关键，默认情况下，其存储对象的HashCode、分代年龄和锁标记位。偏向锁是JDK6中引入的一项锁优化，它的目的是消除数据在无竞争情况下的同步原语，进一步提高程序的运行性能。偏向锁会偏向于第一个获得它的线程，如果在接下来的执行过程中，该锁没有被其他的线程获取，则持有偏向锁的线程将永远不需要同步。大多数情况下，锁不仅不存在多线程竞争，而且总是由同一线程多次获得，为了让线程获得锁的代价更低而引入了偏向锁。当锁对象第一次被线程获取的时候，线程使用CAS操作把这个锁的线程ID记录再对象Mark Word之中，同时置偏向标志位1。以后该线程在进入和退出同步块时不需要进行CAS操作来加锁和解锁，只需要简单地测试一下对象头的Mark Word里是否存储着指向当前线程的偏向锁。如果测试成功，表示线程已经获得了锁。如果线程使用CAS操作时失败则表示该锁对象上存在竞争并且这个时候另外一个线程获得偏向锁的所有权。当到达全局安全点（safepoint，这个时间点上没有正在执行的字节码）时获得偏向锁的线程被挂起，膨胀为轻量级锁（涉及Monitor Record，Lock Record相关操作，这里不展开），同时被撤销偏向锁的线程继续往下执行同步代码。当有另外一个线程去尝试获取这个锁时，偏向模式就宣告结束。线程在执行同步块之前，JVM会先在当前线程的栈帧中创建用于存储锁记录(Lock Record)的空间，并将对象头中的Mard Word复制到锁记录中，官方称为Displaced Mark Word。然后线程尝试使用CAS将对象头中的Mark Word替换为指向锁记录的指针。如果成功，当前线程获得锁，如果失败，表示其他线程竞争锁，当前线程便尝试使用自旋来获取锁。如果自旋失败则锁会膨胀成重量级锁。如果自旋成功则依然处于轻量级锁的状态。轻量级锁的解锁过程也是通过CAS操作来进行的，如果对象的Mark Word仍然指向线程的锁记录，那就用CAS操作把对象当前的Mark Word和线程中赋值的Displaced Mark Word替换回来，如果替换成功，整个同步过程就完成了，如果替换失败，就说明有其他线程尝试过获取该锁，那就要在释放锁的同时，唤醒被挂起的线程。轻量级锁提升程序同步性能的依据是：对于绝大部分的锁，在整个同步周期内都是不存在竞争的（区别于偏向锁）。这是一个经验数据。如果没有竞争，轻量级锁使用CAS操作避免了使用互斥量的开销，但如果存在锁竞争，除了互斥量的开销外，还额外发生了CAS操作，因此在有竞争的情况下，轻量级锁比传统的重量级锁更慢。整个synchronized锁流程如下：检测Mark Word里面是不是当前线程的ID，如果是，表示当前线程处于偏向锁如果不是，则使用CAS将当前线程的ID替换Mard Word，如果成功则表示当前线程获得偏向锁，置偏向标志位1如果失败，则说明发生竞争，撤销偏向锁，进而升级为轻量级锁。当前线程使用CAS将对象头的Mark Word替换为锁记录指针，如果成功，当前线程获得锁如果失败，表示其他线程竞争锁，当前线程便尝试使用自旋来获取锁。如果自旋成功则依然处于轻量级状态。如果自旋失败，则升级为重量级锁。悲观锁和乐观锁悲观锁：假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作。乐观锁：假定不会发生并发冲突，只在提交操作时检测是否违反数据完整性。（使用版本号或者时间戳来配合实现）共享锁和排它锁共享锁：如果事务T对数据A加上共享锁后，则其他事务只能对A再加共享锁，不能加排它锁。获准共享锁的事务只能读数据，不能修改数据。排它锁：如果事务T对数据A加上排它锁后，则其他事务不能再对A加任何类型的锁。获得排它锁的事务即能读数据又能修改数据。读写锁读写锁是一个资源能够被多个读线程访问，或者被一个写线程访问但不能同时存在读线程。Java当中的读写锁通过ReentrantReadWriteLock实现。具体使用方法这里不展开。互斥锁所谓互斥锁就是指一次最多只能有一个线程持有的锁。在JDK中synchronized和JUC的Lock就是互斥锁。无锁要保证现场安全，并不是一定就要进行同步，两者没有因果关系。同步只是保证共享数据争用时的正确性的手段，如果一个方法本来就不涉及共享数据，那它自然就无须任何同步措施去保证正确性，因此会有一些代码天生就是线程安全的。无状态编程。无状态代码有一些共同的特征：不依赖于存储在对上的数据和公用的系统资源、用到的状态量都由参数中传入、不调用非无状态的方法等。可以参考Servlet。线程本地存储。可以参考ThreadLocalvolatileCAS协程：在单线程里实现多任务的调度，并在单线程里维持多个任务间的切换","categories":[{"name":"多线程","slug":"多线程","permalink":"http://www.fufan.me/categories/多线程/"}],"tags":[{"name":"多线程","slug":"多线程","permalink":"http://www.fufan.me/tags/多线程/"}]},{"title":"java多线程系列（二）——关键字和方法","slug":"java多线程系列（二）——关键字和方法","date":"2017-05-01T13:47:00.000Z","updated":"2018-11-04T14:45:32.414Z","comments":true,"path":"2017/05/01/java多线程系列（二）——关键字和方法/","link":"","permalink":"http://www.fufan.me/2017/05/01/java多线程系列（二）——关键字和方法/","excerpt":"","text":"关键字synchronized用途synchronized关键字解决的是多个线程之间访问资源的同步性，synchronized关键字可以保证被它修饰的方法或者代码块在任意时刻只能有一个线程执行。另外，在 Java 早期版本中，synchronized属于重量级锁，效率低下，因为监视器锁（monitor）是依赖于底层的操作系统的 Mutex Lock 来实现的，Java 的线程是映射到操作系统的原生线程之上的。如果要挂起或者唤醒一个线程，都需要操作系统帮忙完成，而操作系统实现线程之间的切换时需要从用户态转换到内核态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高，这也是为什么早期的 synchronized 效率低的原因。庆幸的是在 Java 6 之后 Java 官方对从 JVM 层面对synchronized 较大优化，所以现在的 synchronized 锁效率也优化得很不错了。JDK1.6对锁的实现引入了大量的优化，如自旋锁、适应性自旋锁、锁消除、锁粗化、偏向锁、轻量级锁等技术来减少锁操作的开销。使用方式修饰实例方法，作用于当前对象实例加锁，进入同步代码前要获得当前对象实例的锁修饰静态方法，作用于当前类对象加锁，进入同步代码前要获得当前类对象的锁 。也就是给当前类加锁，会作用于类的所有对象实例，因为静态成员不属于任何一个实例对象，是类成员（ static 表明这是该类的一个静态资源，不管new了多少个对象，只有一份，所以对该类的所有对象都加了锁）。所以如果一个线程A调用一个实例对象的非静态 synchronized 方法，而线程B需要调用这个实例对象所属类的静态 synchronized 方法，是允许的，不会发生互斥现象，因为访问静态 synchronized 方法占用的锁是当前类的锁，而访问非静态 synchronized 方法占用的锁是当前实例对象锁。修饰代码块，指定加锁对象，对给定对象加锁，进入同步代码库前要获得给定对象的锁。 和 synchronized 方法一样，synchronized(this)代码块也是锁定当前对象的。synchronized 关键字加到 static 静态方法和 synchronized(class)代码块上都是是给 Class 类上锁。这里再提一下：synchronized关键字加到非 static 静态方法上是给对象实例上锁。另外需要注意的是：尽量不要使用 synchronized(String a) 因为JVM中，字符串常量池具有缓冲功能！如写一个单例类的实现：123456789101112131415161718192021public class Singleton &#123; private volatile static Singleton uniqueInstance; private Singleton() &#123; &#125; public static Singleton getUniqueInstance() &#123; //先判断对象是否已经实例过，没有实例化过才进入加锁代码 if (uniqueInstance == null) &#123; //类对象加锁 synchronized (Singleton.class) &#123; if (uniqueInstance == null) &#123; uniqueInstance = new Singleton(); &#125; &#125; &#125; return uniqueInstance; &#125;&#125;uniqueInstance 采用 volatile 关键字修饰也是很有必要的， uniqueInstance = new Singleton(); 这段代码其实是分为三步执行：为 uniqueInstance 分配内存空间初始化 uniqueInstance将 uniqueInstance 指向分配的内存地址但是由于 JVM 具有指令重排的特性，执行顺序有可能变成 1-&gt;3-&gt;2。指令重排在单线程环境下不会出先问题，但是在多线程环境下会导致一个线程获得还没有初始化的实例。例如，线程 T1 执行了 1 和 3，此时 T2 调用 getUniqueInstance() 后发现 uniqueInstance 不为空，因此返回 uniqueInstance，但此时 uniqueInstance 还未被初始化。使用 volatile 可以禁止 JVM 的指令重排，保证在多线程环境下也能正常运行。synchronized和ReenTrantLock 的区别两者都是可重入锁synchronized 依赖于 JVM 而 ReenTrantLock 依赖于 APIReenTrantLock 比 synchronized 增加了一些高级功能ReenTrantLock提供了一种能够中断等待锁的线程的机制，通过lock.lockInterruptibly()来实现这个机制。也就是说正在等待的线程可以选择放弃等待，改为处理其他事情。ReenTrantLock可以指定是公平锁还是非公平锁。而synchronized只能是非公平锁。所谓的公平锁就是先等待的线程先获得锁。 ReenTrantLock默认情况是非公平的，可以通过 ReenTrantLock类的ReentrantLock(boolean fair)构造方法来制定是否是公平的。synchronized关键字与wait()和notify/notifyAll()方法相结合可以实现等待/通知机制，ReentrantLock类当然也可以实现，但是需要借助于Condition接口与newCondition() 方法。Condition是JDK1.5之后才有的，它具有很好的灵活性，比如可以实现多路通知功能也就是在一个Lock对象中可以创建多个Condition实例（即对象监视器），线程对象可以注册在指定的Condition中，从而可以有选择性的进行线程通知，在调度线程上更加灵活。 在使用notify/notifyAll()方法进行通知时，被通知的线程是由 JVM 选择的，用ReentrantLock类结合Condition实例可以实现“选择性通知” ，这个功能非常重要，而且是Condition接口默认提供的。而synchronized关键字就相当于整个Lock对象中只有一个Condition实例，所有的线程都注册在它一个身上。如果执行notifyAll()方法的话就会通知所有处于等待状态的线程这样会造成很大的效率问题，而Condition实例的signalAll()方法 只会唤醒注册在该Condition实例中的所有等待线程。性能已不是选择标准volatile内存可见性所谓可见性，是指当一条线程修改了共享变量的值，新值对于其他线程来说是可以立即得知的。很显然，上述的例子中是没有办法做到内存可见性的。java虚拟机有自己的内存模型（Java Memory Model，JMM），JMM可以屏蔽掉各种硬件和操作系统的内存访问差异，以实现让java程序在各种平台下都能达到一致的内存访问效果。JMM决定一个线程对共享变量的写入何时对另一个线程可见，JMM定义了线程和主内存之间的抽象关系：共享变量存储在主内存(Main Memory)中，每个线程都有一个私有的本地内存（Local Memory），本地内存保存了被该线程使用到的主内存的副本拷贝，线程对变量的所有操作都必须在工作内存中进行，而不能直接读写主内存中的变量。volatile具备两种特性，第一就是保证共享变量对所有线程的可见性。将一个共享变量声明为volatile后，会有以下效应：当写一个volatile变量时，JMM会把该线程对应的本地内存中的变量强制刷新到主内存中去；这个写会操作会导致其他线程中的缓存无效。不能解决原子性但是需要注意的是，我们一直在拿volatile和synchronized做对比，仅仅是因为这两个关键字在某些内存语义上有共通之处，volatile并不能完全替代synchronized，它依然是个轻量级锁，在很多场景下，volatile并不能胜任。看下这个例子：12345678910111213141516171819202122232425package test;import java.util.concurrent.CountDownLatch;public class Counter &#123; public static volatile int num = 0; //使用CountDownLatch来等待计算线程执行完 static CountDownLatch countDownLatch = new CountDownLatch(30); public static void main(String []args) throws InterruptedException &#123; //开启30个线程进行累加操作 for(int i=0;i&lt;30;i++)&#123; new Thread()&#123; public void run()&#123; for(int j=0;j&lt;10000;j++)&#123; num++;//自加操作 &#125; countDownLatch.countDown(); &#125; &#125;.start(); &#125; //等待计算线程执行完 countDownLatch.await(); System.out.println(num); &#125;&#125;执行结果：1224291如果用volatile修饰的共享变量可以保证可见性，那么结果不应该是300000么?问题就出在num++这个操作上，因为num++不是个原子性的操作，而是个复合操作。我们可以简单讲这个操作理解为由这三步组成:读取加一赋值所以，在多线程环境下，有可能线程A将num读取到本地内存中，此时其他线程可能已经将num增大了很多，线程A依然对过期的num进行自加，重新写到主存中，最终导致了num的结果不合预期，而是小于30000。禁止指令重排序jvm在执行代码块的时候，会通过一些策略对代码顺序进行重排序来优化程序，不过有两个原则：重排序操作不会对存在数据依赖关系的操作进行重排序。比如：a=1;b=a; 这个指令序列，由于第二个操作依赖于第一个操作，所以在编译时和处理器运行时这两个操作不会被重排序。重排序是为了优化性能，但是不管怎么重排序，单线程下程序的执行结果不能被改变比如：a=1;b=2;c=a+b这三个操作，第一步（a=1)和第二步(b=2)由于不存在数据依赖关系，所以可能会发生重排序，但是c=a+b这个操作是不会被重排序的，因为需要保证最终的结果一定是c=a+b=3。重排序在单线程模式下是一定会保证最终结果的正确性，但是在多线程环境下，问题就出来了，volatile就可以解决这个问题。重排序在单线程模式下是一定会保证最终结果的正确性，但是在多线程环境下，问题就出来了，来看个例子，我们对第一个TestVolatile的例子稍稍改进，再增加个共享变量a12345678910111213141516171819202122public class TestVolatile &#123; int a = 1; boolean status = false; /** * 状态切换为true */ public void changeStatus()&#123; a = 2;//1 status = true;//2 &#125; /** * 若状态为true，则running。 */ public void run()&#123; if(status)&#123;//3 int b = a+1;//4 System.out.println(b); &#125; &#125;&#125;假设线程A执行changeStatus后，线程B执行run，我们能保证在4处，b一定等于3么？答案依然是无法保证！也有可能b仍然为2。上面我们提到过，为了提供程序并行度，编译器和处理器可能会对指令进行重排序，而上例中的1和2由于不存在数据依赖关系，则有可能会被重排序，先执行status=true再执行a=2。而此时线程B会顺利到达4处，而线程A中a=2这个操作还未被执行，所以b=a+1的结果也有可能依然等于2。使用volatile关键字修饰共享变量便可以禁止这种重排序。若用volatile修饰共享变量，在编译时，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序volatile禁止指令重排序也有一些规则，简单列举一下：1. 当第二个操作是voaltile写时，无论第一个操作是什么，都不能进行重排序2. 当地一个操作是volatile读时，不管第二个操作是什么，都不能进行重排序3. 当第一个操作是volatile写时，第二个操作是volatile读时，不能进行重排序与synchronized的区别和使用在某些场景下，使用synchronized关键字和volatile是等价的：写入变量值时候不依赖变量的当前值，或者能够保证只有一个线程修改变量值。写入的变量值不依赖其他变量的参与。读取变量值时候不能因为其他原因进行加锁。volatile本质是在告诉jvm当前变量在寄存器中的值是不确定的,需要从主存中读取,synchronized则是锁定当前变量,只有当前线程可以访问该变量,其他线程被阻塞住.volatile仅能使用在变量级别,synchronized则可以使用在变量,方法.volatile仅能实现变量的修改可见性,而synchronized则可以保证变量的修改可见性和原子性.volatile不会造成线程的阻塞,而synchronized可能会造成线程的阻塞.当一个域的值依赖于它之前的值时，volatile就无法工作了，如n=n+1,n++等。如果某个域的值受到其他域的值的限制，那么volatile也无法工作，如Range类的lower和upper边界，必须遵循lower&lt;=upper的限制。总结简单总结下，volatile是一种轻量级的同步机制，它主要有两个特性：一是保证共享变量对所有线程的可见性；二是禁止指令重排序优化。同时需要注意的是，volatile对于单个的共享变量的读/写具有原子性，但是像num++这种复合操作，volatile无法保证其原子性。我们可以通过java本身提供的AtomicInteger来解决该问题，其具体实现看后面的博文。wait、notify和notifyAllwait()的作用是让当前线程进入等待状态，同时，wait()也会让当前线程释放它所持有的锁。“直到其他线程调用此对象的 notify() 方法或 notifyAll() 方法”，当前线程被唤醒(进入“就绪状态”)notify()和notifyAll()的作用，则是唤醒当前对象上的等待线程；notify()是唤醒单个线程，而notifyAll()是唤醒所有的线程。wait(long timeout)让当前线程处于“等待(阻塞)状态”，“直到其他线程调用此对象的notify()方法或 notifyAll() 方法，或者超过指定的时间量”，当前线程被唤醒(进入“就绪状态”)。wait()、notify/notifyAll() 方法是Object的本地final方法，无法被重写。wait()使当前线程阻塞，前提是 必须先获得锁，一般配合synchronized 关键字使用，即，一般在synchronized 同步代码块里使用 wait()、notify/notifyAll() 方法。wait()使当前线程阻塞，前提是 必须先获得锁，一般配合synchronized 关键字使用，即，一般在synchronized 同步代码块里使用 wait()、notify/notifyAll() 方法。当线程执行wait()方法时候，会释放当前的锁，然后让出CPU，进入等待状态。只有当 notify/notifyAll() 被执行时候，才会唤醒一个或多个正处于等待状态的线程，然后继续往下执行，直到执行完synchronized 代码块的代码或是中途遇到wait() ，再次释放锁。notify 和 notifyAll的区别notify方法只唤醒一个等待（对象的）线程并使该线程开始执行。所以如果有多个线程等待一个对象，这个方法只会唤醒其中一个线程，选择哪个线程取决于操作系统对多线程管理的实现。notifyAll 会唤醒所有等待(对象的)线程，尽管哪一个线程将会第一个处理取决于操作系统的实现。如果当前情况下有多个线程需要被唤醒，推荐使用notifyAll 方法。比如在生产者-消费者里面的使用，每次都需要唤醒所有的消费者或是生产者，以判断程序是否可以继续往下执行。","categories":[{"name":"多线程","slug":"多线程","permalink":"http://www.fufan.me/categories/多线程/"}],"tags":[{"name":"多线程","slug":"多线程","permalink":"http://www.fufan.me/tags/多线程/"}]},{"title":"java多线程系列（一）——多线程基础","slug":"java多线程系列（一）——多线程基础","date":"2017-04-30T03:39:00.000Z","updated":"2018-11-05T15:10:11.598Z","comments":true,"path":"2017/04/30/java多线程系列（一）——多线程基础/","link":"","permalink":"http://www.fufan.me/2017/04/30/java多线程系列（一）——多线程基础/","excerpt":"","text":"多线程基础基本概念和术语进程所谓进程就是运行在操作系统的一个任务，进程是计算机任务调度的一个单位，操作系统在启动一个程序的时候，会为其创建一个进程，JVM就是一个进程。进程与进程之间是相互隔离的，每个进程都有独立的内存空间。计算机实现并发的原理是：CPU分时间片，交替执行，宏观并行，微观串行。同理，在进程的基础上分出更小的任务调度单元就是线程，我们所谓的多线程就是一个进程并发多个线程。线程在上面我们提到，一个进程可以并发出多个线程，而线程就是最小的任务执行单元，具体来说，一个程序顺序执行的流程就是一个线程，我们常见的main就是一个线程（主线程）。java多线程每一个Java的应用都至少包含一个线程——主线程。尽管后台也会存在一些其他的线程，例如内存管理，系统管理，信号处理等等，但是从应用来看，主函数是第一个线程，并且我们可以从其中创建多个线程。多线程指的是2个或者更多的线程来在一个程序中并发地执行任务。单处理的电脑只能在同一时间执行一个线程，时间分片是操作系统给不同的进程线程用来共享处理器时间的。线程的优先级java 中的线程优先级的范围是1～10，默认的优先级是5。“高优先级线程”会优先于“低优先级线程”执行用户线程和守护线程所谓守护线程是指在程序运行的时候在后台提供一种通用服务的线程，比如垃圾回收线程就是一个很称职的守护者，并且这种线程并不属于程序中不可或缺的部分。因 此，当所有的非守护线程结束时，程序也就终止了，同时会杀死进程中的所有守护线程。反过来说，只要任何非守护线程还在运行，程序就不会终止。守护线程和用户线程的没啥本质的区别：唯一的不同之处就在于虚拟机的离开：如果用户线程已经全部退出运行了，只剩下守护线程存在了，虚拟机也就退出了。 因为没有了被守护者，守护线程也就没有工作可做了，也就没有继续运行程序的必要了。线程安全线程安全概念：当多个线程访问某一个类（对象或方法）时，这个类始终能表现出正确的行为，那么这个类（对象或方法）就是线程安全的。锁线程安全就是多线程访问时，采用了加锁机制，当一个线程访问该类的某个数据时，进行保护，其他线程不能进行访问直到该线程读取完，其他线程才可使用。不会出现数据不一致或者数据污染。 线程不安全就是不提供数据访问保护，有可能出现多个线程先后更改数据造成所得到的数据是脏数据。这里的加锁机制常见的如：synchronized线程的状态Java中的线程的生命周期大体可分为5种状态。新建(NEW)：新创建了一个线程对象。可运行(RUNNABLE)：线程对象创建后，其他线程(比如main线程）调用了该对象的start()方法。该状态的线程位于可运行线程池中，等待被线程调度选中，获取cpu 的使用权 。运行(RUNNING)：可运行状态(runnable)的线程获得了cpu 时间片（timeslice） ，执行程序代码。阻塞(BLOCKED)：阻塞状态是指线程因为某种原因放弃了cpu 使用权，也即让出了cpu timeslice，暂时停止运行。直到线程进入可运行(runnable)状态，才有机会再次获得cpu timeslice 转到运行(running)状态。阻塞的情况分三种：(一). 等待阻塞：运行(running)的线程执行o.wait()方法，JVM会把该线程放入等待队列(waitting queue)中。(二). 同步阻塞：运行(running)的线程在获取对象的同步锁时，若该同步锁被别的线程占用，则JVM会把该线程放入锁池(lock pool)中。(三). 其他阻塞：运行(running)的线程执行Thread.sleep(long ms)或t.join()方法，或者发出了I/O请求时，JVM会把该线程置为阻塞状态。当sleep()状态超时、join()等待线程终止或者超时、或者I/O处理完毕时，线程重新转入可运行(runnable)状态。死亡(DEAD)：线程run()、main() 方法执行结束，或者因异常退出了run()方法，则该线程结束生命周期。死亡的线程不可再次复生。线程状态切换详解线程的状态图初始状态实现Runnable接口和继承Thread可以得到一个线程类，new一个实例出来，线程就进入了初始状态可运行状态可运行状态只是说你资格运行，调度程序没有挑选到你，你就永远是可运行状态。调用线程的start()方法，此线程进入可运行状态。当前线程sleep()方法结束，其他线程join()结束，等待用户输入完毕，某个线程拿到对象锁，这些线程也将进入可运行状态。当前线程时间片用完了，调用当前线程的yield()方法，当前线程进入可运行状态。锁池里的线程拿到对象锁后，进入可运行状态。运行状态线程调度程序从可运行池中选择一个线程作为当前线程时线程所处的状态。这也是线程进入运行状态的唯一一种方式。死亡状态当线程的run()方法完成时，或者主线程的main()方法完成时，我们就认为它死去。这个线程对象也许是活的，但是，它已经不是一个单独执行的线程。线程一旦死亡，就不能复生。在一个死去的线程上调用start()方法，会抛出java.lang.IllegalThreadStateException异常。阻塞状态当前线程T调用Thread.sleep()方法，当前线程进入阻塞状态。运行在当前线程里的其它线程t2调用join()方法，当前线程进入阻塞状态。等待用户输入的时候，当前线程进入阻塞状态。等待队列调用obj的wait(), notify()方法前，必须获得obj锁，也就是必须写在synchronized(obj) 代码段内。与等待队列相关的步骤和图线程1获取对象A的锁，正在使用对象A。线程1调用对象A的wait()方法。线程1释放对象A的锁，并马上进入等待队列。锁池里面的对象争抢对象A的锁。线程5获得对象A的锁，进入synchronized块，使用对象A。线程5调用对象A的notifyAll()方法，唤醒所有线程，所有线程进入锁池。||||| 线程5调用对象A的notify()方法，唤醒一个线程，不知道会唤醒谁，被唤醒的那个线程进入锁池。notifyAll()方法所在synchronized结束，线程5释放对象A的锁。锁池里面的线程争抢对象锁，但线程1什么时候能抢到就不知道了。||||| 原本锁池+第6步被唤醒的线程一起争抢对象锁。锁池状态当前线程想调用对象A的同步方法时，发现对象A的锁被别的线程占有，此时当前线程进入锁池状态。简言之，锁池里面放的都是想争夺对象锁的线程。当一个线程1被另外一个线程2唤醒时，1线程进入锁池状态，去争夺对象锁。锁池是在同步的环境下才有的概念，一个对象对应一个锁池。apiThread.sleep(long millis)，一定是当前线程调用此方法，当前线程进入阻塞，但不释放对象锁，millis后线程自动苏醒进入可运行状态。作用：给其它线程执行机会的最佳方式。Thread.yield()，一定是当前线程调用此方法，当前线程放弃获取的cpu时间片，由运行状态变会可运行状态，让OS再次选择线程。作用：让相同优先级的线程轮流执行，但并不保证一定会轮流执行。实际中无法保证yield()达到让步目的，因为让步的线程还有可能被线程调度程序再次选中。Thread.yield()不会导致阻塞。t.join()/t.join(long millis)，当前线程里调用其它线程1的join方法，当前线程阻塞，但不释放对象锁，直到线程1执行完毕或者millis时间到，当前线程进入可运行状态。obj.wait()，当前线程调用对象的wait()方法，当前线程释放对象锁，进入等待队列。依靠notify()/notifyAll()唤醒或者wait(long timeout)timeout时间到自动唤醒。obj.notify()唤醒在此对象监视器上等待的单个线程，选择是任意性的。notifyAll()唤醒在此对象监视器上等待的所有线程api部分的内容会在之后的部分详解","categories":[{"name":"多线程","slug":"多线程","permalink":"http://www.fufan.me/categories/多线程/"}],"tags":[{"name":"多线程","slug":"多线程","permalink":"http://www.fufan.me/tags/多线程/"}]},{"title":"java常用集合源码分析之Map（二）","slug":"java常用集合源码分析之Map（二）","date":"2017-04-24T18:48:00.000Z","updated":"2018-11-02T18:59:18.187Z","comments":true,"path":"2017/04/25/java常用集合源码分析之Map（二）/","link":"","permalink":"http://www.fufan.me/2017/04/25/java常用集合源码分析之Map（二）/","excerpt":"","text":"HashMap以下基于 JDK1.7 分析。如图所示，HashMap 底层是基于数组和链表实现的。其中有两个重要的参数：容量负载因子容量的默认大小是 16，负载因子是 0.75，当 HashMap 的 size &gt; 16*0.75 时就会发生扩容(容量和负载因子都可以自由调整)。put 方法首先会将传入的 Key 做 hash 运算计算出 hashcode,然后根据数组长度取模计算出在数组中的 index 下标。由于在计算中位运算比取模运算效率高的多，所以 HashMap 规定数组的长度为 2^n 。这样用 2^n - 1 做位运算与取模效果一致，并且效率还要高出许多。由于数组的长度有限，所以难免会出现不同的 Key 通过运算得到的 index 相同，这种情况可以利用链表来解决，HashMap 会在 table[index]处形成链表，采用头插法将数据插入到链表中。get 方法get 和 put 类似，也是将传入的 Key 计算出 index ，如果该位置上是一个链表就需要遍历整个链表，通过 key.equals(k) 来找到对应的元素。遍历方式12345Iterator&lt;Map.Entry&lt;String, Integer&gt;&gt; entryIterator = map.entrySet().iterator(); while (entryIterator.hasNext()) &#123; Map.Entry&lt;String, Integer&gt; next = entryIterator.next(); System.out.println(\"key=\" + next.getKey() + \" value=\" + next.getValue()); &#125;123456Iterator&lt;String&gt; iterator = map.keySet().iterator(); while (iterator.hasNext())&#123; String key = iterator.next(); System.out.println(\"key=\" + key + \" value=\" + map.get(key)); &#125;123map.forEach((key,value)-&gt;&#123; System.out.println(\"key=\" + key + \" value=\" + value);&#125;);强烈建议使用第一种 EntrySet 进行遍历。第一种可以把 key value 同时取出，第二种还得需要通过 key 取一次 value，效率较低, 第三种需要 JDK1.8 以上，通过外层遍历 table，内层遍历链表或红黑树。notice在并发环境下使用 HashMap 容易出现死循环。并发场景发生扩容，调用 resize() 方法里的 rehash() 时，容易出现环形链表。这样当获取一个不存在的 key 时，计算出的 index 正好是环形链表的下标时就会出现死循环。所以 HashMap 只能在单线程中使用，并且尽量的预设容量，尽可能的减少扩容。在 JDK1.8 中对 HashMap 进行了优化：当 hash 碰撞之后写入链表的长度超过了阈值(默认为8)，链表将会转换为红黑树。假设 hash 冲突非常严重，一个数组后面接了很长的链表，此时重新的时间复杂度就是 O(n) 。如果是红黑树，时间复杂度就是 O(logn) 。大大提高了查询效率。线程不安全表现及原因resize死循环，形成环形链表put、addEntry、resize等方法不同步具体查看 谈谈HashMap线程不安全的体现ConcurrentHashMap由于 HashMap 是一个线程不安全的容器，因此需要支持线程安全的并发容器 ConcurrentHashMap 。JDK1.7 实现数据结构如图所示，是由 Segment 数组、HashEntry 数组组成，和 HashMap 一样，仍然是数组加链表组成。ConcurrentHashMap 采用了分段锁技术，其中 Segment 继承于 ReentrantLock。不会像 HashTable 那样不管是 put 还是 get 操作都需要做同步处理，理论上 ConcurrentHashMap 支持 CurrencyLevel (Segment 数组数量)的线程并发。每当一个线程占用锁访问一个 Segment 时，不会影响到其他的 Segment。get 方法ConcurrentHashMap 的 get 方法是非常高效的，因为整个过程都不需要加锁。只需要将 Key 通过 Hash 之后定位到具体的 Segment ，再通过一次 Hash 定位到具体的元素上。由于 HashEntry 中的 value 属性是用 volatile 关键词修饰的，保证了内存可见性，所以每次获取时都是最新值(volatile 相关知识点)。put 方法内部 HashEntry 类 ：12345678910111213static final class HashEntry&lt;K,V&gt; &#123; final int hash; final K key; volatile V value; volatile HashEntry&lt;K,V&gt; next; HashEntry(int hash, K key, V value, HashEntry&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125;&#125;虽然 HashEntry 中的 value 是用 volatile 关键词修饰的，但是并不能保证并发的原子性，所以 put 操作时仍然需要加锁处理。首先也是通过 Key 的 Hash 定位到具体的 Segment，在 put 之前会进行一次扩容校验。这里比 HashMap 要好的一点是：HashMap 是插入元素之后再看是否需要扩容，有可能扩容之后后续就没有插入就浪费了本次扩容(扩容非常消耗性能)。而 ConcurrentHashMap 不一样，它是在将数据插入之前检查是否需要扩容，之后再做插入操作。size 方法每个 Segment 都有一个 volatile 修饰的全局变量 count ,求整个 ConcurrentHashMap 的 size 时很明显就是将所有的 count 累加即可。但是 volatile 修饰的变量却不能保证多线程的原子性，所有直接累加很容易出现并发问题。但如果每次调用 size 方法将其余的修改操作加锁效率也很低。所以做法是先尝试两次将 count 累加，如果容器的 count 发生了变化再加锁来统计 size。至于 ConcurrentHashMap 是如何知道在统计时大小发生了变化呢，每个 Segment 都有一个 modCount 变量，每当进行一次 put remove 等操作，modCount 将会 +1。只要 modCount 发生了变化就认为容器的大小也在发生变化。JDK1.8 实现1.8 中的 ConcurrentHashMap 数据结构和实现与 1.7 还是有着明显的差异。其中抛弃了原有的 Segment 分段锁，而采用了 CAS + synchronized 来保证并发安全性。1234567891011121314151617181920212223242526272829static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; volatile V val; volatile Node&lt;K,V&gt; next; Node(int hash, K key, V val, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.val = val; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return val; &#125; public final int hashCode() &#123; return key.hashCode() ^ val.hashCode(); &#125; public final String toString()&#123; return key + \"=\" + val; &#125; public final V setValue(V value) &#123; throw new UnsupportedOperationException(); &#125; public final boolean equals(Object o) &#123; Object k, v, u; Map.Entry&lt;?,?&gt; e; return ((o instanceof Map.Entry) &amp;&amp; (k = (e = (Map.Entry&lt;?,?&gt;)o).getKey()) != null &amp;&amp; (v = e.getValue()) != null &amp;&amp; (k == key || k.equals(key)) &amp;&amp; (v == (u = val) || v.equals(u))); &#125;也将 1.7 中存放数据的 HashEntry 改为 Node，但作用都是相同的。其中的 val next 都用了 volatile 修饰，保证了可见性。put 方法重点来看看 put 函数：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263final V putVal(K key, V value, boolean onlyIfAbsent) &#123; if (key == null || value == null) throw new NullPointerException(); int hash = spread(key.hashCode()); int binCount = 0; for (Node&lt;K,V&gt;[] tab = table;;) &#123; Node&lt;K,V&gt; f; int n, i, fh; if (tab == null || (n = tab.length) == 0) tab = initTable(); else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin &#125; else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); else &#123; V oldVal = null; synchronized (f) &#123; if (tabAt(tab, i) == f) &#123; if (fh &gt;= 0) &#123; binCount = 1; for (Node&lt;K,V&gt; e = f;; ++binCount) &#123; K ek; if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; &#125; Node&lt;K,V&gt; pred = e; if ((e = e.next) == null) &#123; pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; &#125; &#125; &#125; else if (f instanceof TreeBin) &#123; Node&lt;K,V&gt; p; binCount = 2; if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) &#123; oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125; &#125; if (binCount != 0) &#123; if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break; &#125; &#125; &#125; addCount(1L, binCount); return null;&#125;根据 key 计算出 hashcode 。判断是否需要进行初始化。f 即为当前 key 定位出的 Node，如果为空表示当前位置可以写入数据，利用 CAS 尝试写入，失败则自旋保证成功。如果当前位置的 hashcode == MOVED == -1,则需要进行扩容。如果都不满足，则利用 synchronized 锁写入数据。如果数量大于 TREEIFY_THRESHOLD 则要转换为红黑树。get 方法12345678910111213141516171819public V get(Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; int h = spread(key.hashCode()); if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) &#123; if ((eh = e.hash) == h) &#123; if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) return e.val; &#125; else if (eh &lt; 0) return (p = e.find(h, key)) != null ? p.val : null; while ((e = e.next) != null) &#123; if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; &#125; &#125; return null;&#125;根据计算出来的 hashcode 寻址，如果就在桶上那么直接返回值。如果是红黑树那就按照树的方式获取值。都不满足那就按照链表的方式遍历获取值。总结1.8 在 1.7 的数据结构上做了大的改动，采用红黑树之后可以保证查询效率（O(logn)），甚至取消了 ReentrantLock 改为了 synchronized，这样可以看出在新版的 JDK 中对 synchronized 优化是很到位的。TreeMap简介TreeMap 是一个有序的key-value集合，它是通过红黑树实现的。TreeMap 继承于AbstractMap，所以它是一个Map，即一个key-value集合。TreeMap 实现了NavigableMap接口，意味着它支持一系列的导航方法。比如返回有序的key集合。TreeMap 实现了Cloneable接口，意味着它能被克隆。TreeMap 实现了java.io.Serializable接口，意味着它支持序列化。TreeMap基于红黑树（Red-Black tree）实现。该映射根据其键的自然顺序进行排序，或者根据创建映射时提供的 Comparator 进行排序，具体取决于使用的构造方法。TreeMap的基本操作 containsKey、get、put 和 remove 的时间复杂度是 log(n) 。另外，TreeMap是非同步的。 它的iterator 方法返回的迭代器是fail-fastl的。构造函数1234567891011// 默认构造函数。使用该构造函数，TreeMap中的元素按照自然排序进行排列。TreeMap()// 创建的TreeMap包含MapTreeMap(Map&lt;? extends K, ? extends V&gt; copyFrom)// 指定Tree的比较器TreeMap(Comparator&lt;? super K&gt; comparator)// 创建的TreeSet包含copyFromTreeMap(SortedMap&lt;K, ? extends V&gt; copyFrom)TreeMap与Map关系如下图：从图中可以看出：TreeMap实现继承于AbstractMap，并且实现了NavigableMap接口。TreeMap的本质是R-B Tree(红黑树)，它包含几个重要的成员变量： root, size, comparator。root 是红黑数的根节点。它是Entry类型，Entry是红黑数的节点，它包含了红黑数的6个基本组成成分：key(键)、value(值)、left(左孩子)、right(右孩子)、parent(父节点)、color(颜色)。Entry节点根据key进行排序，Entry节点包含的内容为value。红黑数排序时，根据Entry中的key进行排序；Entry中的key比较大小是根据比较器comparator来进行判断的。size是红黑数中节点的个数。源码解析可以参看此blog遍历方式遍历TreeMap的键值对，使用iterator遍历TreeMap的键，调用keySet方法遍历TreeMap的值，调用values方法","categories":[{"name":"集合","slug":"集合","permalink":"http://www.fufan.me/categories/集合/"}],"tags":[{"name":"集合","slug":"集合","permalink":"http://www.fufan.me/tags/集合/"},{"name":"map","slug":"map","permalink":"http://www.fufan.me/tags/map/"}]},{"title":"java常用集合源码分析之HashSet（三）","slug":"java常用集合源码分析之HashSet（三）","date":"2017-04-19T18:06:00.000Z","updated":"2018-11-02T18:34:30.164Z","comments":true,"path":"2017/04/20/java常用集合源码分析之HashSet（三）/","link":"","permalink":"http://www.fufan.me/2017/04/20/java常用集合源码分析之HashSet（三）/","excerpt":"","text":"HashSetHashSet 是一个不允许存储重复元素的集合，它的实现比较简单，只要理解了 HashMap，HashSet 就水到渠成了。成员变量首先了解下 HashSet 的成员变量:1234private transient HashMap&lt;E,Object&gt; map;// Dummy value to associate with an Object in the backing Mapprivate static final Object PRESENT = new Object();发现主要就两个变量:map ：用于存放最终数据的。PRESENT ：是所有写入 map 的 value 值。构造函数1234567public HashSet() &#123; map = new HashMap&lt;&gt;();&#125;public HashSet(int initialCapacity, float loadFactor) &#123; map = new HashMap&lt;&gt;(initialCapacity, loadFactor);&#125;构造函数很简单，利用了 HashMap 初始化了 map 。add123public boolean add(E e) &#123; return map.put(e, PRESENT)==null;&#125;比较关键的就是这个 add() 方法。可以看出它是将存放的对象当做了 HashMap 的健，value 都是相同的 PRESENT 。由于 HashMap 的 key 是不能重复的，所以每当有重复的值写入到 HashSet 时，value 会被覆盖，但 key 不会受到影响，这样就保证了 HashSet 中只能存放不重复的元素。遍历方式几种方式（iterator，for循环，lambda）jdk1.8 lambda123set.stream.foreach((v) -&gt; &#123; System.out.println(v);&#125;)总结HashSet 的原理比较简单，几乎全部借助于 HashMap 来实现的。所以 HashMap 会出现的问题 HashSet 依然不能避免。","categories":[],"tags":[{"name":"集合","slug":"集合","permalink":"http://www.fufan.me/tags/集合/"},{"name":"set","slug":"set","permalink":"http://www.fufan.me/tags/set/"}]},{"title":"java常用集合源码分析之List（一）","slug":"java常用集合源码分析之List（一）","date":"2017-04-09T16:44:00.000Z","updated":"2018-11-05T03:05:36.775Z","comments":true,"path":"2017/04/10/java常用集合源码分析之List（一）/","link":"","permalink":"http://www.fufan.me/2017/04/10/java常用集合源码分析之List（一）/","excerpt":"","text":"ArrayListArrayList 实现于 List、RandomAccess 接口。可以插入空数据，也支持随机访问。ArrayList默认初始化长度为10个1234/** * Default initial capacity. */private static final int DEFAULT_CAPACITY = 10;ArrayList相当于动态数据，其中最重要的两个属性分别是:elementData 数组，以及 size 大小。在调用 add() 方法的时候：12345public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;&#125;首先进行扩容校验。将插入的值放到尾部，并将 size + 1 。如果是调用 add(index,e) 在指定位置添加的话：12345678910public void add(int index, E element) &#123; rangeCheckForAdd(index); ensureCapacityInternal(size + 1); // Increments modCount!! //复制，向后移动 System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++;&#125;也是首先扩容校验。接着对数据进行复制，目的是把 index 位置空出来放本次插入的数据，并将后面的数据向后移动一个位置。其实扩容最终调用的代码:1234567891011private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity);&#125;也是一个数组复制的过程。由此可见 ArrayList 的主要消耗是数组扩容以及在指定位置添加数据，在日常使用时最好是指定大小，尽量减少扩容。更要减少在指定位置插入数据的操作。序列化由于 ArrayList 是基于动态数组实现的，所以并不是所有的空间都被使用。因此使用了 transient 修饰，可以防止被自动序列化。1transient Object[] elementData;因此 ArrayList 自定义了序列化与反序列化：1234567891011121314151617181920212223242526272829303132333435363738394041private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException&#123; // Write out element count, and any hidden stuff int expectedModCount = modCount; s.defaultWriteObject(); // Write out size as capacity for behavioural compatibility with clone() s.writeInt(size); // Write out all elements in the proper order. //只序列化了被使用的数据 for (int i=0; i&lt;size; i++) &#123; s.writeObject(elementData[i]); &#125; if (modCount != expectedModCount) &#123; throw new ConcurrentModificationException(); &#125;&#125;private void readObject(java.io.ObjectInputStream s) throws java.io.IOException, ClassNotFoundException &#123; elementData = EMPTY_ELEMENTDATA; // Read in size, and any hidden stuff s.defaultReadObject(); // Read in capacity s.readInt(); // ignored if (size &gt; 0) &#123; // be like clone(), allocate array based upon size not capacity ensureCapacityInternal(size); Object[] a = elementData; // Read in all elements in the proper order. for (int i=0; i&lt;size; i++) &#123; a[i] = s.readObject(); &#125; &#125;&#125;当对象中自定义了 writeObject 和 readObject 方法时，JVM 会调用这两个自定义方法来实现序列化与反序列化。从实现中可以看出 ArrayList 只序列化了被使用的数据。遍历方式ArrayList支持3种遍历方式通过迭代器遍历：12345Iterator iter = list.iterator();while (iter.hasNext())&#123; System.out.println(iter.next());&#125;随机访问，通过索引值去遍历，由于ArrayList实现了RandomAccess接口12345int size = list.size();for (int i=0; i&lt;size; i++) &#123; System.out.println(list.get(i)); &#125;for循环遍历1234for(String str:list) &#123; System.out.println(str); &#125;lambda123list.stream.foreach((v) -&gt; &#123; System.out.println(v);&#125;)VectorVector 也是实现于 List 接口，底层数据结构和 ArrayList 类似,也是一个动态数组存放数据。不过是在 add() 方法的时候使用 synchronized 进行同步写数据，但是开销较大，所以 Vector 是一个同步容器并不是一个并发容器。以下是 add() 方法：123456public synchronized boolean add(E e) &#123; modCount++; ensureCapacityHelper(elementCount + 1); elementData[elementCount++] = e; return true;&#125;以及指定位置插入数据:1234567891011121314public void add(int index, E element) &#123; insertElementAt(element, index);&#125;public synchronized void insertElementAt(E obj, int index) &#123; modCount++; if (index &gt; elementCount) &#123; throw new ArrayIndexOutOfBoundsException(index + \" &gt; \" + elementCount); &#125; ensureCapacityHelper(elementCount + 1); System.arraycopy(elementData, index, elementData, index + 1, elementCount - index); elementData[index] = obj; elementCount++;&#125;遍历方式和ArrayList相比, 多了一种Enumeration遍历12345Integer value = null;Enumeration enu = vec.elements();while (enu.hasMoreElements()) &#123; value = (Integer)enu.nextElement();&#125;LinkedList如图所示 LinkedList 底层是基于双向链表实现的，也是实现了 List 接口，所以也拥有 List 的一些特点(JDK1.7/8 之后取消了循环，修改为双向链表)。新增方法123456789101112131415161718public boolean add(E e) &#123; linkLast(e); return true;&#125; /** * Links e as last element. */void linkLast(E e) &#123; final Node&lt;E&gt; l = last; final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null); last = newNode; if (l == null) first = newNode; else l.next = newNode; size++; modCount++;&#125;可见每次插入都是移动指针，和 ArrayList 的拷贝数组来说效率要高上不少。查询方法1234567891011121314151617181920public E get(int index) &#123; checkElementIndex(index); return node(index).item;&#125;Node&lt;E&gt; node(int index) &#123; // assert isElementIndex(index); if (index &lt; (size &gt;&gt; 1)) &#123; Node&lt;E&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; &#125; else &#123; Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; &#125;&#125;上述代码，利用了双向链表的特性，如果index离链表头比较近，就从节点头部遍历。否则就从节点尾部开始遍历。使用空间（双向链表）来换取时间。node()会以O(n/2)的性能去获取一个结点如果索引值大于链表大小的一半，那么将从尾结点开始遍历这样的效率是非常低的，特别是当 index 越接近 size 的中间值时。总结：LinkedList 插入，删除都是移动指针效率很高。查找需要进行遍历查询，效率较低。遍历方式除了arraylist的四种遍历方式以外，还可以通过removeFirst()和removeLast()的方式：1234567891011//removeFirst()try &#123; while(list.removeFirst() != null) ; &#125; catch (NoSuchElementException e) &#123; &#125; //removeLast()try &#123; while(list.removeLast() != null) ; &#125; catch (NoSuchElementException e) &#123;这种方式是效率最高的，不过他会删除原始数据。随机遍历是效率最低的，推荐用for循环的方式。后面我也会拿出小节专门来分析集合的遍历效率","categories":[{"name":"集合","slug":"集合","permalink":"http://www.fufan.me/categories/集合/"}],"tags":[{"name":"集合","slug":"集合","permalink":"http://www.fufan.me/tags/集合/"},{"name":"list","slug":"list","permalink":"http://www.fufan.me/tags/list/"}]},{"title":"OSI和TCP/IP的模型简述","slug":"OSI和TCP-IP的7层模型简述","date":"2017-04-02T12:40:00.000Z","updated":"2018-11-02T13:37:34.149Z","comments":true,"path":"2017/04/02/OSI和TCP-IP的7层模型简述/","link":"","permalink":"http://www.fufan.me/2017/04/02/OSI和TCP-IP的7层模型简述/","excerpt":"","text":"体系结构概述先盗取盗图一张^_^, OSI的七层体系结构概念清楚，理论也很完整，但是它比较复杂而且不实用，总图如下：但是由于7层模型过于复杂，大部分公司和国家支持更简单的5层模型五层协议各层详述应用层（application layer）应用层的任务是通过应用进程间的交互来完成特定网络应用。应用层协议定义的是应用进程（进程：主机中正在运行的程序）间的通信和交互的规则。对于不同的网络应用需要不同的应用层协议。在互联网中应用层协议很多，如域名系统DNS，支持万维网应用的HTTP协议，支持电子邮件的SMTP协议等等。我们把应用层交互的数据单元称为报文。域名系统（Domain Name System缩写DNS，Domain Name被译为域名）域名系统是因特网的一项核心服务，它作为可以将域名和IP地址相互映射的一个分布式数据库，能够使人更方便的访问互联网，而不用去记住能够被机器直接读取的IP数串。（百度百科）例如：一个公司的Web网站可看作是它在网上的门户，而域名就相当于其门牌地址，通常域名都使用该公司的名称或简称。例如上面提到的微软公司的域名，类似的还有：IBM公司的域名是www.ibm.com、Oracle公司的域名是www.oracle.com、Cisco公司的域名是www.cisco.com等。HTTP协议超文本传输协议（HTTP，HyperText Transfer Protocol)是互联网上应用最为广泛的一种网络协议。所有的WWW文件都必须遵守这个标准。设计HTTP最初的目的是为了提供一种发布和接收HTML页面的方法。（百度百科）运输层（transport layer）运输层的主要任务就是负责向两台主机进程之间的通信提供通用的数据传输服务。应用进程利用该服务传送应用层报文。“通用的”是指并不针对某一个特定的网络应用，而是多种应用可以使用同一个运输层服务。由于一台主机可同时运行多个线程，因此运输层有复用和分用的功能。所谓复用就是指多个应用层进程可同时使用下面运输层的服务，分用和复用相反，是运输层把收到的信息分别交付上面应用层中的相应进程。运输层主要使用以下两种协议：传输控制协议TCP（Transmisson Control Protocol）–提供面向连接的，可靠的数据传输服务。用户数据协议UDP（User Datagram Protocol）–提供无连接的，尽最大努力的数据传输服务（不保证数据传输的可靠性）。UDP的主要特点：UDP是无连接的；UDP使用尽最大努力交付，即不保证可靠交付，因此主机不需要维持复杂的链接状态（这里面有许多参数）；UDP是面向报文的；UDP没有拥塞控制，因此网络出现拥塞不会使源主机的发送速率降低（对实时应用很有用，如IP电话，实时视频会议等）；UDP支持一对一、一对多、多对一和多对多的交互通信；UDP的首部开销小，只有8个字节，比TCP的20个字节的首部要短。TCP的主要特点：TCP是面向连接的。（就好像打电话一样，通话前需要先拨号建立连接，通话结束后要挂机释放连接）；每一条TCP连接只能有两个端点，每一条TCP连接只能是点对点的（一对一）；TCP提供可靠交付的服务。通过TCP连接传送的数据，无差错、不丢失、不重复、并且按序到达；TCP提供全双工通信。TCP允许通信双方的应用进程在任何时候都能发送数据。TCP连接的两端都设有发送缓存和接收缓存，用来临时存放双方通信的数据；面向字节流。TCP中的“流”（stream）指的是流入进程或从进程流出的字节序列。“面向字节流”的含义是：虽然应用程序和TCP的交互是一次一个数据块（大小不等），但TCP把应用程序交下来的数据仅仅看成是一连串的无结构的字节流。网络层（network layer）网络层负责为分组交换网上的不同主机提供通信服务。在发送数据时，网络层把运输层产生的报文段或用户数据报封装成分组和包进行传送。在TCP/IP体系结构中，由于网络层使用IP协议，因此分组也叫IP数据报，简称数据报。这里要注意：不要把运输层的“用户数据报UDP”和网络层的“IP数据报”弄混。另外，无论是哪一层的数据单元，都可笼统地用“分组”来表示。网络层的另一个任务就是选择合适的路由，使源主机运输层所传下来的分株，能通过网络层中的路由器找到目的主机。这里强调指出，网络层中的“网络”二字已经不是我们通常谈到的具体网络，而是指计算机网络体系结构模型中第三层的名称.互联网是由大量的异构（heterogeneous）网络通过路由器（router）相互连接起来的。互联网使用的网络层协议是无连接的网际协议（Intert Prococol）和许多路由选择协议，因此互联网的网络层也叫做网际层或IP层。数据链路层（data link layer）数据链路层通常简称为链路层。两台主机之间的数据传输，总是在一段一段的链路上传送的，这就需要使用专门的链路层的协议。 在两个相邻节点之间传送数据时，数据链路层将网络层交下来的IP数据报组装程帧，在两个相邻节点间的链路上传送帧。每一帧包括数据和必要的控制信息（如同步信息，地址信息，差错控制等）。在 接收数据时，控制信息使接收端能够知道一个帧从哪个比特开始和到哪个比特结束。这样，数据链路层在收到一个帧后，就可从中提出数据部分，上交给网络层。控制信息还使接收端能够检测到所收到的帧中有误差错。如果发现差错，数据链路层就简单地丢弃这个出了差错的帧，以避免继续在网络中传送下去白白浪费网络资源。如果需要改正数据在链路层传输时出现差错（这就是说，数据链路层不仅要检错，而且还要纠错），那么就要采用可靠性传输协议来纠正出现的差错。这种方法会使链路层的协议复杂些。物理层（physical layer）在物理层上所传送的数据单位是比特。物理层的作用是实现相邻计算机节点之间比特流的透明传送，尽可能屏蔽掉具体传输介质和物理设备的差异。使其上面的数据链路层不必考虑网络的具体传输介质是什么。“透明传送比特流”表示经实际电路传送后的比特流没有发生变化，对传送的比特流来说，这个电路好像是看不见的。","categories":[{"name":"网络","slug":"网络","permalink":"http://www.fufan.me/categories/网络/"}],"tags":[{"name":"网络","slug":"网络","permalink":"http://www.fufan.me/tags/网络/"}]},{"title":"java常用集合源码分析之绪论","slug":"java常用集合源码分析之绪论","date":"2017-04-01T02:52:00.000Z","updated":"2018-11-05T03:06:23.200Z","comments":true,"path":"2017/04/01/java常用集合源码分析之绪论/","link":"","permalink":"http://www.fufan.me/2017/04/01/java常用集合源码分析之绪论/","excerpt":"","text":"面向对象语言对事物的体现都是以对象的形式，所以为了方便对多个对象的操作，就对对象进行存储，集合就是存储对象最常用的一种方式。数组虽然也可以存储对象，但长度是固定的；集合长度是可变的，数组中可以存储基本数据类型，集合只能存储对象。集合类的特点：集合只用于存储对象，集合长度是可变的，集合可以存储不同类型的对象。java集合继承关系图先来看两张图：上述类图中，实线边框的是实现类，比如ArrayList，LinkedList，HashMap等，折线边框的是抽象类，比如AbstractCollection，AbstractList，AbstractMap等，而点线边框的是接口，比如Collection，Iterator，List等。1、Iterator接口Iterator接口，这是一个用于遍历集合中元素的接口，主要包含hashNext(),next(),remove()三种方法。它的一个子接口LinkedIterator在它的基础上又添加了三种方法，分别是add(),previous(),hasPrevious()。也就是说如果是先Iterator接口，那么在遍历集合中元素的时候，只能往后遍历，被遍历后的元素不会在遍历到，通常无序集合实现的都是这个接口，比如HashSet，HashMap；而那些元素有序的集合，实现的一般都是LinkedIterator接口，实现这个接口的集合可以双向遍历，既可以通过next()访问下一个元素，又可以通过previous()访问前一个元素，比如ArrayList。抽象类的使用。如果要自己实现一个集合类，去实现那些抽象的接口会非常麻烦，工作量很大。这个时候就可以使用抽象类，这些抽象类中给我们提供了许多现成的实现，我们只需要根据自己的需求重写一些方法或者添加一些方法就可以实现自己需要的集合类，工作流昂大大降低。2、Collection （集合的最大接口）继承关系——List 可以存放重复的内容——Set 不能存放重复的内容，所以的重复内容靠hashCode()和equals()两个方法区分——Queue 队列接口——SortedSet 可以对集合中的数据进行排序Collection定义了集合框架的共性功能。add方法的参数类型是Object。以便于接收任意类型对象。集合中存储的都是对象的引用(地址)。3、List的常用子类特有方法。凡是可以操作角标的方法都是该体系特有的方法。——ArrayList 线程不安全，查询速度快——Vector 线程安全，但速度慢，已被ArrayList替代——LinkedList 链表结果，增删速度快4、Set接口Set：元素是无序(存入和取出的顺序不一定一致)，元素不可以重复。——HashSet:底层数据结构是哈希表。是线程不安全的。不同步。HashSet是如何保证元素唯一性的呢？是通过元素的两个方法，hashCode和equals来完成。如果元素的HashCode值相同，才会判断equals是否为true。如果元素的hashcode值不同，不会调用equals。注意,对于判断元素是否存在，以及删除等操作，依赖的方法是元素的hashcode和equals方法。——TreeSet：有序的存放：TreeSet线程不安全，可以对Set集合中的元素进行排序; 通过compareTo或者compare方法来保证元素的唯一性，元素以二叉树的形式存放。5、Object类在实际开发中经常会碰到区分同一对象的问题，一个完整的类最好覆写Object类的hashCode()、equals()、toString()三个方法。6、集合的输出5种常见的输出方式——Iterator： 迭代输出，使用最多的输出方式——ListIterator： Iterator的子接口，专门用于输出List中的内容——Enumeration——foreach——lambda在迭代时，不可以通过集合对象的方法操作集合中的元素，因为会发生ConcurrentModificationException异常。所以，在迭代器时，只能用迭代器的放过操作元素，可是Iterator方法是有限的，只能对元素进行判断，取出，删除的操作，如果想要其他的操作如添加，修改等，就需要使用其子接口，ListIterator。该接口只能通过List集合的listIterator方法获取。7、Map接口Correction、Set、List接口都属于单值的操作，而Map中的每个元素都使用key——&gt;value的形式存储在集合中。Map集合：该集合存储键值对。一对一对往里存。而且要保证键的唯一性。8、Map接口的常用子类Map——HashMap：底层是哈希表数据结构，允许使用 null 值和 null 键，该集合是不同步的。将hashtable替代，jdk1.2.效率高。——TreeMap：底层是二叉树数据结构。线程不同步。可以用于给map集合中的键进行排序。9、集合工具类Collections:集合框架的工具类。里面定义的都是静态方法。Collections和Collection有什么区别？Collection是集合框架中的一个顶层接口，它里面定义了单列集合的共性方法。它有两个常用的子接口，——List：对元素都有定义索引。有序的。可以重复元素。——Set：不可以重复元素。无序。Collections是集合框架中的一个工具类。该类中的方法都是静态的。提供的方法中有可以对list集合进行排序，二分查找等方法。通常常用的集合都是线程不安全的。因为要提高效率。如果多线程操作这些集合时，可以通过该工具类中的同步方法，将线程不安全的集合，转换成安全的。10、比较11、总结List：add/remove/get/set。ArrayList：其实就是数组，容量一大，频繁增删就是噩梦，适合随机查找；LinkedList：增加了push/[pop|remove|pull]，其实都是removeFirst；Vector：历史遗留产物，同步版的ArrayList，代码和ArrayList太像；Stack：继承自Vector。Java里其实没有纯粹的Stack，可以自己实现，用组合的方式，封装一下LinkedList即可；Queue：本来是单独的一类，不过在SUN的JDK里就是用LinkedList来提供这个功能的，主要方法是offer/pull/peek，因此归到这里呢。Set：add/remove。可以用迭代器或者转换成list。HashSet：内部采用HashMap实现的；LinkedHashSet：采用LinkedHashMap实现；TreeSet：TreeMap。Map：put/get/remove。HashMap/HashTable：散列表，和ArrayList一样采用数组实现，超过初始容量会对性能有损耗；LinkedHashMap：继承自HashMap，但通过重写嵌套类HashMap.Entry实现了链表结构，同样有容量的问题；Properties：是继承的HashTable。","categories":[],"tags":[{"name":"集合","slug":"集合","permalink":"http://www.fufan.me/tags/集合/"}]},{"title":"Java面试总结积累（基础篇）之集合问题","slug":"Java面试总结积累（基础篇）之集合问题","date":"2017-01-12T16:35:00.000Z","updated":"2018-11-05T03:12:48.337Z","comments":true,"path":"2017/01/13/Java面试总结积累（基础篇）之集合问题/","link":"","permalink":"http://www.fufan.me/2017/01/13/Java面试总结积累（基础篇）之集合问题/","excerpt":"","text":"这里通过收集网上一些比较好的博客对集合的总结做一下记录和积累。List, Set, Map三者的区别及总结List：对付顺序的好帮手List接口存储一组不唯一（可以有多个元素引用相同的对象），有序的对象Set:注重独一无二的性质不允许重复的集合。不会有多个元素引用相同的对象。Map:用Key来搜索的专家使用键值对存储。Map会维护与Key有关联的值。两个Key可以引用相同的对象，但Key不能重复，典型的Key是String类型，但也可以是任何对象。Arraylist 与 LinkedList 区别Arraylist底层使用的是数组（存读数据效率高，插入删除特定位置效率低），LinkedList底层使用的是双向循环链表数据结构（插入，删除效率特别高）。学过数据结构这门课后我们就知道采用链表存储，插入，删除元素时间复杂度不受元素位置的影响，都是近似O（1）而数组为近似O（n），因此当数据特别多，而且经常需要插入删除元素时建议选用LinkedList.一般程序只用Arraylist就够用了，因为一般数据量都不会蛮大，Arraylist是使用最多的集合类。ArrayList 与 Vector 区别（为什么要用Arraylist取代Vector呢？）Vector类的所有方法都是同步的。可以由两个线程安全地访问一个Vector对象、但是一个线程访问Vector ，代码要在同步操作上耗费大量的时间。Arraylist不是同步的，所以在不需要同步时建议使用Arraylist。HashMap 和 Hashtable 的区别HashMap是非线程安全的，HashTable是线程安全的；HashTable内部的方法基本都经过synchronized修饰。因为线程安全的问题，HashMap要比HashTable效率高一点，HashTable基本被淘汰。HashMap允许有null值的存在，而在HashTable中put进的键值只要有一个null，直接抛出NullPointerException。TIPS: Hashtable和HashMap有几个主要的不同：线程安全以及速度。仅在你需要完全的线程安全的时候使用Hashtable，而如果你使用Java5或以上的话，请使用ConcurrentHashMap吧HashMap 和 ConcurrentHashMap 的区别ConcurrentHashMap对整个桶数组进行了分割分段(Segment)，然后在每一个分段上都用lock锁进行保护，相对于HashTable的synchronized锁的粒度更精细了一些，并发性能更好，而HashMap没有锁机制，不是线程安全的。（JDK1.8之后ConcurrentHashMap启用了一种全新的方式实现,利用CAS算法。）HashMap的键值对允许有null，但是ConCurrentHashMap都不允许。HashSet如何检查重复当你把对象加入HashSet时，HashSet会先计算对象的hashcode值来判断对象加入的位置，同时也会与其他加入的对象的hashcode值作比较，如果没有相符的hashcode，HashSet会假设对象没有重复出现。但是如果发现有相同hashcode值的对象，这时会调用equals（）方法来检查hashcode相等的对象是否真的相同。如果两者相同，HashSet就不会让加入操作成功。==与equals的区别如果两个对象相等，则hashcode一定也是相同的两个对象相等,对两个equals方法返回true两个对象有相同的hashcode值，它们也不一定是相等的综上，equals方法被覆盖过，则hashCode方法也必须被覆盖hashCode()的默认行为是对堆上的对象产生独特值。如果没有重写hashCode()，则该class的两个对象无论如何都不会相等（即使这两个对象指向相同的数据）。==是判断两个变量或实例是不是指向同一个内存空间 equals是判断两个变量或实例所指向的内存空间的值是不是相同==是指对内存地址进行比较 equals()是对字符串的内容进行比较3.==指引用是否相同 equals()指的是值是否相同comparable 和 comparator的区别？comparable接口实际上是出自java.lang包 它有一个 compareTo(Object obj)方法用来排序comparator接口实际上是出自 java.util 包它有一个compare(Object obj1, Object obj2)方法用来排序一般我们需要对一个集合使用自定义排序时，我们就要重写compareTo方法或compare方法，当我们需要对某一个集合实现两种排序方式，比如一个song对象中的歌名和歌手名分别采用一种排序方法的话，我们可以重写compareTo方法和使用自制的Comparator方法或者以两个Comparator来实现歌名排序和歌星名排序，第二种代表我们只能使用两个参数版的Collections.sort().如何对Object的list排序？对objects数组进行排序，我们可以用Arrays.sort()方法对objects的集合进行排序，需要使用Collections.sort()方法如何实现数组与List的相互转换？List转数组:toArray(arraylist.size()方法数组转List:Arrays的asList(a)方法1234567891011121314151617181920212223List&lt;String&gt; arrayList = new ArrayList&lt;String&gt;(); arrayList.add(&quot;s&quot;); arrayList.add(&quot;e&quot;); arrayList.add(&quot;n&quot;); /** * ArrayList转数组 */ int size=arrayList.size(); String[] a = arrayList.toArray(new String[size]); //输出第二个元素 System.out.println(a[1]);//结果：e //输出整个数组 System.out.println(Arrays.toString(a));//结果：[s, e, n] /** * 数组转list */ List&lt;String&gt; list=Arrays.asList(a); /** * list转Arraylist */ List&lt;String&gt; arrayList2 = new ArrayList&lt;String&gt;(); arrayList2.addAll(list); System.out.println(list);如何求ArrayList集合的交集 并集 差集 去重复并集需要用到List接口中定义的几个方法：addAll(Collection&lt;? extends E&gt; c) :按指定集合的Iterator返回的顺序将指定集合中的所有元素追加到此列表的末尾retainAll(Collection&lt;?&gt; c): 仅保留此列表中包含在指定集合中的元素。removeAll(Collection&lt;?&gt; c) :从此列表中删除指定集合中包含的所有元素。TIPS: JAVA8中提供了通过lambda方式处理集合，如Collections类中好多工具方法用stream流方式处理，方便了我们在处理集合时的各种情况，JAVA8集合这块再后面的blog中会单独拿出来总结。HashMap 的工作原理及代码实现集合框架源码学习之HashMap(JDK1.8)ConcurrentHashMap 的工作原理及代码实现ConcurrentHashMap实现原理及源码分析集合框架底层数据结构总结CollectionListArraylist：数组（查询快,增删慢 线程不安全,效率高 ）Vector：数组（查询快,增删慢 线程安全,效率低 ）LinkedList：链表（查询慢,增删快 线程不安全,效率高 ）SetHashSet（无序，唯一）:哈希表或者叫散列集(hash table)LinkedHashSet：链表和哈希表组成 。 由链表保证元素的排序 ， 由哈希表证元素的唯一性TreeSet（有序，唯一）：红黑树(自平衡的排序二叉树。)MapHashMap：基于哈希表的Map接口实现（哈希表对键进行散列，Map结构即映射表存放键值对）LinkedHashMap:HashMap 的基础上加上了链表数据结构HashTable:哈希表TreeMap:红黑树（自平衡的排序二叉树）集合的选用主要根据集合的特点来选用，比如我们需要根据键值获取到元素值时就选用Map接口下的集合，需要排序时选择TreeMap,不需要排序时就选择HashMap,需要保证线程安全就选用ConcurrentHashMap.当我们只需要存放元素值时，就选择实现Collection接口的集合，需要保证元素唯一时选择实现Set接口的集合比如TreeSet或HashSet，不需要就选择实现List接口的比如ArrayList或LinkedList，然后再根据实现这些接口的集合的特点来选用。","categories":[{"name":"面试","slug":"面试","permalink":"http://www.fufan.me/categories/面试/"}],"tags":[{"name":"面试","slug":"面试","permalink":"http://www.fufan.me/tags/面试/"},{"name":"java基础","slug":"java基础","permalink":"http://www.fufan.me/tags/java基础/"}]},{"title":"Java面试总结积累（基础篇）之语法问题","slug":"Java面试总结积累（基础篇）之语法问题","date":"2017-01-02T16:34:00.000Z","updated":"2018-11-05T03:12:29.706Z","comments":true,"path":"2017/01/03/Java面试总结积累（基础篇）之语法问题/","link":"","permalink":"http://www.fufan.me/2017/01/03/Java面试总结积累（基础篇）之语法问题/","excerpt":"","text":"java开发经验固然很重要，但是有很多面试当中会遇到一些基础问题，需要自己来进行总结归类，也算是扫盲和回归吧。 面向对象和面向过程的区别面向过程：优点：性能比面向对象高，因为类调用时需要实例化，开销比较大，比较消耗资源;比如单片机、嵌入式开发、Linux/Unix等一般采用面向过程开发，性能是最重要的因素。缺点：没有面向对象易维护、易复用、易扩展面向对象：优点：易维护、易复用、易扩展，由于面向对象有封装、继承、多态性的特性，可以设计出低耦合的系统，使系统更加灵活、更加易于维护缺点：性能比面向过程低Java语言有哪些特点？1，简单易学；2，面向对象（封装，继承，多态）；3，平台无关性（Java虚拟机实现平台无关性）；4，可靠性；5，安全性；6，支持多线程（C++语言没有内置的多线程机制，因此必须调用操作系统的多线程功能来进行多线程程序设计，而Java语言却提供了多线程支持）；7，支持网络编程并且很方便（Java语言诞生本身就是为简化网络编程设计的，因此Java语言不仅支持网络编程而且很方便）；8，编译与解释并存；什么是字节码？采用字节码的最大好处是什么？什么Java是虚拟机？先看下java中的编译器和解释器：Java中引入了虚拟机的概念，即在机器和编译程序之间加入了一层抽象的虚拟的机器。这台虚拟的机器在任何平台上都提供给编译程序一个的共同的接口。编译程序只需要面向虚拟机，生成虚拟机能够理解的代码，然后由解释器来将虚拟机代码转换为特定系统的机器码执行。在Java中，这种供虚拟机理解的代码叫做字节码（即扩展名为.class的文件），它不面向任何特定的处理器，只面向虚拟机。每一种平台的解释器是不同的，但是实现的虚拟机是相同的。Java源程序经过编译器编译后变成字节码，字节码由虚拟机解释执行，虚拟机将每一条要执行的字节码送给解释器，解释器将其翻译成特定机器上的机器码，然后在特定的机器上运行，这就是上面提到的Java的特点的编译与解释并存的解释。Java源代码—-&gt;编译器—-&gt;jvm可执行的Java字节码(即虚拟指令)—-&gt;jvm—-&gt;jvm中解释器—–&gt;机器可执行的二进制机器码—-&gt;程序运行。采用字节码的好处：Java语言通过字节码的方式，在一定程度上解决了传统解释型语言执行效率低的问题，同时又保留了解释型语言可移植的特点。所以Java程序运行时比较高效，而且，由于字节码并不专对一种特定的机器，因此，Java程序无须重新编译便可在多种不同的计算机上运行。什么是Java虚拟机任何一种可以运行Java字节码的软件均可看成是Java的虚拟机（JVM）字符型常量和字符串常量的区别形式上:字符常量是单引号引起的一个字符字符串常量是双引号引起的若干个字符含义上:字符常量相当于一个整形值(ASCII值),可以参加表达式运算字符串常量代表一个地址值(该字符串在内存中存放位置)占内存大小字符常量只占一个字节字符串常量占若干个字节(至少一个字符结束标志)Java语言采用何种编码方案？有何特点？Java语言采用Unicode编码标准，Unicode（标准码），它为每个字符制订了一个唯一的数值，因此在任何的语言，平台，程序都可以放心的使用。构造器Constructor是否可被override在讲继承的时候我们就知道父类的私有属性和构造方法并不能被继承，所以Constructor也就不能被override,但是可以overload,所以你可以看到一个类中有多个构造函数的情况。重载和重写的区别重载：发生在同一个类中，方法名必须相同，参数类型不同、个数不同、顺序不同，方法返回值和访问修饰符可以不同，发生在编译时。重写：发生在父子类中，方法名、参数列表必须相同，返回值小于等于父类，抛出的异常小于等于父类，访问修饰符大于等于父类；如果父类方法访问修饰符为private则子类中就不是重写。java 面向对象编程三大特性封装、继承、多态String和StringBuffer、StringBuilder的区别是什么？String为什么是不可变的可变性String类中使用字符数组保存字符串，private final char value[]，所以string对象是不可变的。StringBuilder与StringBuffer都继承自AbstractStringBuilder类，在AbstractStringBuilder中也是使用字符数组保存字符串，char[]value，这两种对象都是可变的。线程安全性String中的对象是不可变的，也就可以理解为常量，线程安全。AbstractStringBuilder是StringBuilder与StringBuffer的公共父类，定义了一些字符串的基本操作，如expandCapacity、append、insert、indexOf等公共方法。StringBuffer对方法加了同步锁或者对调用的方法加了同步锁，所以是线程安全的。StringBuilder并没有对方法进行加同步锁，所以是非线程安全的。性能每次对String 类型进行改变的时候，都会生成一个新的String对象，然后将指针指向新的String 对象。StringBuffer每次都会对StringBuffer对象本身进行操作，而不是生成新的对象并改变对象引用。相同情况下使用StirngBuilder 相比使用StringBuffer 仅能获得10%~15% 左右的性能提升，但却要冒多线程不安全的风险。对于三者使用的总结：如果要操作少量的数据用 = String单线程操作字符串缓冲区 下操作大量数据 = StringBuilder多线程操作字符串缓冲区 下操作大量数据 = StringBuffer在一个静态方法内调用一个非静态成员为什么是非法的？由于静态方法可以不通过对象进行调用，因此在静态方法里，不能调用其他非静态变量，也不可以访问非静态变量成员。在Java中定义一个不做事且没有参数的构造方法的作用Java程序在执行子类的构造方法之前，如果没有用super()来调用父类特定的构造方法，则会调用父类中“没有参数的构造方法”。因此，如果父类中只定义了有参数的构造方法，而在子类的构造方法中又没有用super()来调用父类中特定的构造方法，则编译时将发生错误，因为Java程序在父类中找不到没有参数的构造方法可供执行。解决办法是在父类里加上一个不做事且没有参数的构造方法。接口和抽象类的区别是什么？接口的方法默认是public，所有方法在接口中不能有实现，抽象类可以有非抽象的方法接口中的实例变量默认是final类型的，而抽象类中则不一定一个类可以实现多个接口，但最多只能实现一个抽象类一个类实现接口的话要实现接口的所有方法，而抽象类不一定接口不能用new实例化，但可以声明，但是必须引用一个实现该接口的对象从设计层面来说，抽象是对类的抽象，是一种模板设计，接口是行为的抽象，是一种行为的规范。成员变量与局部变量的区别有那些？从语法形式上，看成员变量是属于类的，而局部变量是在方法中定义的变量或是方法的参数；成员变量可以被public,private,static等修饰符所修饰，而局部变量不能被访问控制修饰符及static所修饰；成员变量和局部变量都能被final所修饰；从变量在内存中的存储方式来看，成员变量是对象的一部分，而对象存在于堆内存，局部变量存在于栈内存从变量在内存中的生存时间上看，成员变量是对象的一部分，它随着对象的创建而存在，而局部变量随着方法的调用而自动消失。成员变量如果没有被赋初值，则会自动以类型的默认值而赋值（一种情况例外被final修饰但没有被static修饰的成员变量必须显示地赋值）；而局部变量则不会自动赋值。创建一个对象用什么运算符？对象实体与对象引用有何不同？new运算符，new创建对象实例（对象实例在堆内存中），对象引用指向对象实例（对象引用存放在栈内存中）。一个对象引用可以指向0个或1个对象（一根绳子可以不系气球，也可以系一个气球）;一个对象可以有n个引用指向它（可以用n条绳子系住一个气球）什么是方法的返回值？返回值在类的方法里的作用是什么方法的返回值是指我们获取到的某个方法体中的代码执行后产生的结果！（前提是该方法可能产生结果）。返回值的作用:接收出结果，使得它可以用于其他的操作！一个类的构造方法的作用是什么？若一个类没有声明构造方法，改程序能正确执行吗？为什么？主要作用是完成对类对象的初始化工作。可以执行。因为一个类即使没有声明构造方法也会有默认的不带参数的构造方法。构造方法有哪些特性？名字与类名相同；没有返回值，但不能用void声明构造函数；生成类的对象时自动执行，无需调用。静态方法和实例方法有何不同？静态方法和实例方法的区别主要体现在两个方面：在外部调用静态方法时，可以使用”类名.方法名”的方式，也可以使用”对象名.方法名”的方式。而实例方法只有后面这种方式。也就是说，调用静态方法可以无需创建对象。静态方法在访问本类的成员时，只允许访问静态成员（即静态成员变量和静态方法），而不允许访问实例成员变量和实例方法；实例方法则无此限制对象的相等与指向他们的引用相等，两者有什么不同？对象的相等 比的是内存中存放的内容是否相等而 引用相等 比较的是他们指向的内存地址是否相等。在调用子类构造方法之前会先调用父类没有参数的构造方法，其目的是？帮助子类做初始化工作。equals 和 == 的区别？通俗点讲：==是看看左右是不是一个东西。equals是看看左右是不是长得一样。如何记住嘛。如果单纯是想记住，==：等于。equals：相同。两个长得一样的人，只能说长的相同(equals)，但是不等于他们俩是一个人。你只要记住equals，==就不用记了。术语来讲的区别：==是判断两个变量或实例是不是指向同一个内存空间 equals是判断两个变量或实例所指向的内存空间的值是不是相同==是指对内存地址进行比较 equals()是对字符串的内容进行比较3.==指引用是否相同 equals()指的是值是否相同","categories":[{"name":"面试","slug":"面试","permalink":"http://www.fufan.me/categories/面试/"}],"tags":[{"name":"面试","slug":"面试","permalink":"http://www.fufan.me/tags/面试/"},{"name":"java base","slug":"java-base","permalink":"http://www.fufan.me/tags/java-base/"}]},{"title":"（绪）设计模式之架构中的设计原则","slug":"（绪）设计模式之架构中的设计原则","date":"2016-10-11T10:51:00.000Z","updated":"2018-11-05T08:13:53.199Z","comments":true,"path":"2016/10/11/（绪）设计模式之架构中的设计原则/","link":"","permalink":"http://www.fufan.me/2016/10/11/（绪）设计模式之架构中的设计原则/","excerpt":"","text":"在进行软件架构工作时，需要遵循面向对象原则，这些原则同样在各类设计模式、架构模式之中，在学习过程中可以通过类图、时序图、示例代码等形式，不断体会这些原则在解决“依赖”和变化中的效果。当然，这些“原则”的队伍也在变化。不断有新的“原则加入，也有被淘汰掉的，真正沉淀下来的通用的”原则“其实并不多在使用面向对象的思想进行系统设计时，前任总结出了7条原则，分别是单一职责原则、开闭原则、里氏替换原则、依赖注入原则、接口分离原则、迪米特原则和有限使用组合而不是继承原则。下面来介绍下这几种原则的含义，也为后面学习设计模式打下基础。原则一：单一职责原则但以职责原则的核心思想就是：系统中的每一个对象都应该只有一个单独的职责，而所有对象所关注的就是自身职责的完成，全称即Single Responsibility Principle。其实单一职责的意思就是开发人员经常说的”高内聚、低耦合“。也就是说，每一个类应该只有一个职责，对外只能提供一种功能，而引起类变化的原因应该只有一个。在设计模式中，所有的设计模式都要遵守这个原则。”单一职责“也就是”单一变化原因“。通常一个类的职责越多，导致的变化因素也与阿朵，我们在设计的时候可能会把该类的有关的操作都组合在这个类中，这样做的后果就有可能将多个职责“耦合”到一起。解决这个问题的方法就是“分耦”，将不同的职责分别进行封装，不要将组合在一个类中。例如将用户的属性和用户的行为放在一个接口中声明，如下：12345678910interface User&#123; //身高 public double getHeight(); //体重 public double getWeight(); //吃饭 public void eat(); //玩游戏 public void gaming();&#125;上面的例子就存在这个问题，身高和体重属于业务对象，与之对应的方法主要负责用户的属性，而吃饭和玩游戏是相应的业务逻辑，主要负责用户的行为，这会给人一种不知道这个接口到底是做什么的感觉，职责不清晰，后期维护的时候会造成各种各样的问题。可以将这个接口分为两个。123456interface UserPro&#123; //身高 public double getHeight(); //体重 public double getWeight();&#125;123456interface UserAct&#123; //吃饭 public void eat(); //玩游戏 public void gaming();&#125;然后分别实现这两个接口，这里的实现我就不详细写了，主要通过这种方式可以做到当需要修改用户属性的时候，只需要对UserPro这个接口进行修改，而不会影响到其他类。另外，SRP原则的好处是可以消除耦合，减小因需求变化引起代码僵化的难堪局面。需要注意：一个合理的类，应该仅有一个引起他变化的原因，即单一职责。在没有变化征兆的情况下，应用SRP或其他原则是不明智的。在需求实际发生变化时就应该应用SRP等原则来重构代码。使用测试驱动开发会迫使我们在设计出现劣质趋势之前分离不合理代码如果测试不能迫使职责分离，僵化性和脆弱性的腐朽味会变得很浓烈，那就应该用Facade或者Proxy模式对代码重构原则二：里氏替换原则（LSP）里氏替换原则的核心思想就是：在任何父类出现的地方都可以用他的自雷来替代。它的英文缩写为LSP，全称是Liskov Subsitituition Principle。通俗点讲，就是同一个继承体系中的对象应该有果农共同的行为特征。里氏替换原则关注的是怎样良好地使用继承，也就是说不要滥用继承，它是继承复用的基石。在里氏替换原则中，所有引用基类的地方必须能够透明地使用其子类对象，也就是说，只要父类出现的地方，子类就能出现，而且替换为子类不会产生任何错误或者异常。但是反过来，子类出现的地方，替换为父类就可能出现问题了。主要抓住以下四层含义（子类的范围大于等于父类的范围）：子类必须完全实现父类的方法子类可以有自己的特性覆盖或者实现父类的方法时输入参数可以被放大覆写或者实现父类的方法时输出结果可以被缩小原则三：依赖注入原则（DIP）依赖注入原则的核心思想就是：要依赖于抽象，不要依赖于具体的实现。英文全称就是Dependence Inversion Principle。通俗的讲：在应用程序中，所有的类如果使用或者依赖于其他的类，则都应该依赖于这些其他类的抽象类，而不是这些其他类的具体实现。即要求开发人员在编程时针对接口编程，而不是针对实现编程。依赖注入原则有三点要注意的：高层模块不应该依赖低层模块，两者都应该依赖于抽象（抽象类或接口）。抽象（抽象类或接口）不应该依赖于细节（具体实现类）。细节（具体实现类）应该依赖抽象这里的抽象指的是不能被实例化的抽象类或接口，具体的实现则是可以通过new直接实例化的。这个原则是开闭原则的基础（对扩展开放，对修改关闭）。三种实现方式：通过构造函数传递依赖对象通过setter方法传递依赖对象接口声明实现依赖对象原则四：接口分离原则（ISP）接口分离原则的核心思想就是：不应该强迫客户程序依赖它们不需要使用的方法。它的全称是Interface Segregation Principle。其实接口分离原则的意思就是一个接口不需要提供太多的行为，一个接口应该只提供一种对外的功能，不应该吧所有的操作都封装到一个接口中。这里的接口不仅是interface关键字的实例，接口分为以下两种：对象接口（object Interface）java中声明一个类，通过new出一个实例，它是对一个类型的事务的描述，这也是一种接口。例如：1Phone phone = new Phone(); //这里的类Phone就是实例Phone的一个接口类接口（Class Interface）这种接口就是通过关键字Interface定义的接口。接口分离原则要求的是在一个模块中应该只依赖它需要的接口，以保证接口的纯洁。切勿定义太臃肿的接口。接口分离原则与但以职责原则有点类似，都是说如何设计接口，不过不同在于单一职责原则要求的是类和接口职责单一，注重的是职责，是业务逻辑上的划分。而接口分离原则要求的是接口的方法尽量少，针对一个模块尽量有用。如何做到该原则：接口尽量小：小的概念是保证一个接口之服务于一个子模块或者业务逻辑接口高内聚：指的是对内高度依赖，对外尽可能隔离。即一个接口内部声明的方法相互之间都与某一个子模块相关，且这个子模块必需的。接口设计是有限度的：话说回来，如果过度地遵循该原则，会使得接口数量剧增，复杂度正价，这并不是我们想要的结果。原则五：迪米特原则（LOD）全称是Law of Demeter。核心思想就是：一个对象应当对其他对象尽可能少地了解。意思就是降低各个对象之间的耦合，提高系统的可维护性。在模块之间，应该只通过接口来通信，而不理会模块的内部工作原理，它可以使各个模块偶和程度降到最低，促进软件的复用。它的核心观念还是类间解耦，弱耦合。举个例子，监狱的犯人是不能随便和外面的人打交道，除非探亲，所以狱警就是这个迪米特法则的执行者，监狱就是类，犯人就是类的内部信息。总结下这个原则要注意的地方：在类的划分上，因可更改创建有弱耦合的类。在类的结构设计上，每一个类都应当尽量降低成员的访问权限。在类的设计上，只要有可能，一个类应当设计成不变类在对其他类的引用上，一个对象对其他对象的引用应当降到最低尽量降低类的访问权限谨慎使用序列化功能不要暴露类成员，而应该提供相应的访问方法（属性getter）原则六：开闭原则（OCP）开闭原则的核心思想：一个对象对扩展开放，对修改关闭。其实开闭原则就是：对类的改动是通过增加代码进行的，而不是改动现有的代码。也就是说，软件开发人员一旦写出了可以运行的代码，就不应该去改变它，而是要保证他一直能运行下去，这就需要借助java的抽象和多态，即把可能变化的内容抽象出来，从而使抽象的部分是相对稳定的，而具体的实现层则是可以改变和扩展的。注意：这些设计原则并不是绝对的，而是应根据项目的实际需求来定夺。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://www.fufan.me/categories/设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://www.fufan.me/tags/设计模式/"}]},{"title":"JAVA基本数据结构","slug":"JAVA基本数据结构","date":"2016-09-12T15:18:00.000Z","updated":"2018-11-05T03:13:08.387Z","comments":true,"path":"2016/09/12/JAVA基本数据结构/","link":"","permalink":"http://www.fufan.me/2016/09/12/JAVA基本数据结构/","excerpt":"","text":"java基本数据结构其实学一门语言，基础很重要，现在很多java程序员只是对jdk和各种框架特别熟悉，能熟练地使用各种包和api组件，包括现在很多培训都是灌输这些所谓的实际应用。这会导致学到最后只会照葫芦画瓢。java数据结构的只是体系包括线性表、树、数组、集合、矩阵、排序、查找、哈希表，并将java的设计思想、方法及一些常见的算法、设计模式贯穿其中。其中线性表、链表、哈希表是最为常用的数据结构，在进行java开发时，jdk已经为我们提供了一系列相应的类，如下图。来实现基本的数据结构。这些类均在java.util包中。CollectionListLinkedListArrayListVector(Stack)SetQueueMapHashtableHashMapWeakHashMapCollection接口接口Collection是最基本的集合接口，一个Collection代表一组Object，即Collection的元素（Elements）。主要分为两类，LIst和Set，它们是以是否允许有相同元素来区分。当然其结构也不同。所有实现Collection接口的类都必须提供两个标准的构造函数。无参为空，有参则可复制一个传入的Collection。如何遍历？可以通过迭代器iterator()方法，注意访问Collection中的每一个元素，这种方式也是所有继承于它的类都可以使用的遍历方式。12345Collection collection = new ArrayList&lt;String&gt;();Iterator i = collection.iterator();while(i.hasNext())&#123; Object s = i.next();&#125;它的派生类包括List和Set，以下是他接口中的主要方法：boolean add(Object o)：用于添加对象到集合boolean remove(Object o)：用于删除指定的对象int size()：用于返回当前集合中元素的个数boolean isEmpty()：用于判断集合是否为空。Iterator iterator()：返回一个迭代器boolean contains(Object o)：用于查找集合中是否有指定的对象boolean containsAll(Collection c)：用于查找集合中是否有集合c中的元素boolean addAll（Collection c）：用于将集合c中的元素全部添加到该集合中void clear()：用于清空该集合void removeAll(Collection c)：用于从集合中删除从集合中所有的元素void retainAll(Collection c)：从集合中删除集合c中不包含的元素List接口List是有序的Collection，用户能够使用索引来访问List中的元素，类似数组。LIst包括以下几种子类ArrayList:：是一个数组队列，相当于动态数组。它由数组实现，随机访问效率高，随机插入、随机删除效率低。LinkedList：是一个双向链表。它也可以被当作堆栈、队列或双端队列进行操作。LinkedList随机访问效率高，但随机插入、随机删除效率低。Vector 是矢量队列，和ArrayList一样，它也是一个动态数组，由数组实现。但是ArrayList是非线程安全的，而Vector是线程安全的。Stack 是栈，它继承于Vector。它的特性是：先进后出(FILO, First In Last Out)。如果涉及到“栈”、“队列”、“链表”等操作，应该考虑用List，具体的选择哪个List，根据下面的标准来取舍。(01) 对于需要快速插入，删除元素，应该使用LinkedList。(02) 对于需要快速随机访问元素，应该使用ArrayList。(03)对于“单线程环境” 或者 “多线程环境，但List仅仅只会被单个线程操作”，此时应该使用非同步的类(如ArrayList)。对于“多线程环境，且List可能同时被多个线程操作”，此时，应该使用同步的类(如Vector)。通过下面的测试程序，我们来验证上面的(01)和(02)结论。参考代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081import java.util.*;import java.lang.Class;/* * @desc 对比ArrayList和LinkedList的插入、随机读取效率、删除的效率 * * @author skywang */public class ListCompareTest &#123; private static final int COUNT = 100000; private static LinkedList linkedList = new LinkedList(); private static ArrayList arrayList = new ArrayList(); private static Vector vector = new Vector(); private static Stack stack = new Stack(); public static void main(String[] args) &#123; // 换行符 System.out.println(&quot;插入元素&quot;); // 插入 insertByPosition(stack) ; insertByPosition(vector) ; insertByPosition(linkedList) ; insertByPosition(arrayList) ; // 换行符 System.out.println(&quot;随机读取&quot;); // 随机读取 readByPosition(stack); readByPosition(vector); readByPosition(linkedList); readByPosition(arrayList); // 换行符 System.out.println(&quot;删除元素&quot;); // 删除 deleteByPosition(stack); deleteByPosition(vector); deleteByPosition(linkedList); deleteByPosition(arrayList); &#125; // 获取list的名称 private static String getListName(List list) &#123; if (list instanceof LinkedList) &#123; return &quot;LinkedList&quot;; &#125; else if (list instanceof ArrayList) &#123; return &quot;ArrayList&quot;; &#125; else if (list instanceof Stack) &#123; return &quot;Stack&quot;; &#125; else if (list instanceof Vector) &#123; return &quot;Vector&quot;; &#125; else &#123; return &quot;List&quot;; &#125; &#125; // 向list的指定位置插入COUNT个元素，并统计时间 private static void insertByPosition(List list) &#123; long startTime = System.currentTimeMillis(); // 向list的位置0插入COUNT个数 for (int i=0; i&lt;COUNT; i++) list.add(0, i); long endTime = System.currentTimeMillis(); long interval = endTime - startTime; System.out.println(getListName(list) + &quot; : insert &quot;+COUNT+&quot; elements into the 1st position use time：&quot; + interval+&quot; ms&quot;); &#125; // 从list的指定位置删除COUNT个元素，并统计时间 private static void deleteByPosition(List list) &#123; long startTime = System.currentTimeMillis(); // 删除list第一个位置元素 for (int i=0; i&lt;COUNT; i++) list.remove(0); long endTime = System.currentTimeMillis(); long interval = endTime - startTime; System.out.println(getListName(list) + &quot; : delete &quot;+COUNT+&quot; elements from the 1st position use time：&quot; + interval+&quot; ms&quot;); &#125; // 根据position，不断从list中读取元素，并统计时间 private static void readByPosition(List list) &#123; long startTime = System.currentTimeMillis(); // 读取list元素 for (int i=0; i&lt;COUNT; i++) list.get(i); long endTime = System.currentTimeMillis(); long interval = endTime - startTime; System.out.println(getListName(list) + &quot; : read &quot;+COUNT+&quot; elements by position use time：&quot; + interval+&quot; ms&quot;); &#125;&#125;运行结果如下：插入元素Stack : insert 100000 elements into the 1st position use time：1544 msVector : insert 100000 elements into the 1st position use time：1520 msLinkedList : insert 100000 elements into the 1st position use time：17 msArrayList : insert 100000 elements into the 1st position use time：1519 ms随机读取Stack : read 100000 elements by position use time：7 msVector : read 100000 elements by position use time：7 msLinkedList : read 100000 elements by position use time：8023 msArrayList : read 100000 elements by position use time：2 ms删除元素Stack : delete 100000 elements from the 1st position use time：1553 msVector : delete 100000 elements from the 1st position use time：1525 msLinkedList : delete 100000 elements from the 1st position use time：9 msArrayList : delete 100000 elements from the 1st position use time：1547 ms这里只是对性能做了大致的测试，如果需要研究为何产生如此差异，需要看下数据结构的相关资料。Set接口Set接口是一种不包含重复元素的Collection，也就是说任何两个在Set里面的元素都存在以下e1.equals(e2) == false关系，且Set最多只有一个NULL元素。很明显，Set的构造函数有一个约束条件，就是传入的Collection参数不能包含重复的元素。Queue接口Queue接口与List、Set同一级别，都是继承了Collection接口。LinkedList实现了Queue接 口。Queue接口窄化了对LinkedList的方法的访问权限（即在方法中的参数类型如果是Queue时，就完全只能访问Queue接口所定义的方法 了，而不能直接访问 LinkedList的非Queue的方法），以使得只有恰当的方法才可以使用。BlockingQueue 继承了Queue接口。队列是一种数据结构．它有两个基本操作：在队列尾部加人一个元素，和从队列头部移除一个元素就是说，队列以一种先进先出的方式管理数据，如果你试图向一个 已经满了的阻塞队列中添加一个元素或者是从一个空的阻塞队列中移除一个元索，将导致线程阻塞．在多线程进行合作时，阻塞队列是很有用的工具。工作者线程可 以定期地把中间结果存到阻塞队列中而其他工作者线线程把中间结果取出并在将来修改它们。队列会自动平衡负载。如果第一个线程集运行得比第二个慢，则第二个 线程集在等待结果时就会阻塞。如果第一个线程集运行得快，那么它将等待第二个线程集赶上来。下表显示了jdk1.5中的阻塞队列的操作：add 增加一个元索 如果队列已满，则抛出一个IIIegaISlabEepeplian异常remove 移除并返回队列头部的元素 如果队列为空，则抛出一个NoSuchElementException异常element 返回队列头部的元素 如果队列为空，则抛出一个NoSuchElementException异常offer 添加一个元素并返回true 如果队列已满，则返回falsepoll 移除并返问队列头部的元素 如果队列为空，则返回nullpeek 返回队列头部的元素 如果队列为空，则返回nullput 添加一个元素 如果队列满，则阻塞take 移除并返回队列头部的元素 如果队列为空，则阻塞remove、element、offer 、poll、peek 其实是属于Queue接口。Map接口Map接口没有继承于接口Collection，Map提供key到value的映射。键值对key-value，主要方法如下：boolean equals(Object o)：用于比较对象boolean remove(Object o)：用于删除一个对象void put(Object key, Object value)：用于添加key和valueMap可分为HashMap、HashTable、WeakHashMap、ConcurrentHashMap等。但是我们常用的主要是HashMap和HashTable，下面比较下两者区别：HashMap是非线程安全的，HashTable是线程安全的。HashMap的键和值都允许有null值存在，而HashTable则不行。因为线程安全的问题，HashMap效率比HashTable的要高。能答出上面的三点，简单的面试，算是过了，但是如果再问：Java中的另一个线程安全的与HashMap及其类似的类是什么？(ConcurrentHashMap)同样是线程安全，它与HashTable在线程同步上有什么不同？(synchronized关键字加锁的原理，其实是对对象加锁，不论你是在方法前加synchronized还是语句块前加，锁住的都是对象整体，但是ConcurrentHashMap的同步机制和这个不同，它不是加synchronized关键字，而是基于lock操作的，这样的目的是保证同步的时候，锁住的不是整个对象。事实上，ConcurrentHashMap可以满足concurrentLevel个线程并发无阻塞的操作集合对象)能把第二个问题完整的答出来，说明你的基础算是不错的了。下面浅析更多区别。HashMap1) hashmap的数据结构Hashmap是一个数组和链表的结合体（在数据结构称“链表散列“），如下图示：当我们往hashmap中put元素的时候，先根据key的hash值得到这个元素在数组中的位置（即下标），然后就可以把这个元素放到对应的位置中了。如果这个元素所在的位子上已经存放有其他元素了，那么在同一个位子上的元素将以链表的形式存放，新加入的放在链头，最先加入的放在链尾。2)使用和遍历1234567891011Map map = new HashMap();map.put(&quot;Rajib Sarma&quot;,&quot;100&quot;);map.put(&quot;Rajib Sarma&quot;,&quot;200&quot;);//The value &quot;100&quot; is replaced by &quot;200&quot;.map.put(&quot;Sazid Ahmed&quot;,&quot;200&quot;);Iterator iter = map.entrySet().iterator();while (iter.hasNext()) &#123; Map.Entry entry = (Map.Entry) iter.next(); Object key = entry.getKey(); Object val = entry.getValue();&#125;HashTable和HashMap区别继承不同。public class Hashtable extends Dictionary implements Mappublic class HashMap extends AbstractMap implements MapHashtable 中的方法是同步的，而HashMap中的方法在缺省情况下是非同步的。在多线程并发的环境下，可以直接使用Hashtable，但是要使用HashMap的话就要自己增加同步处理了。Hashtable中，key和value都不允许出现null值。在HashMap中，null可以作为键，这样的键只有一个；可以有一个或多个键所对应的值为null。当get()方法返回null值时，即可以表示 HashMap中没有该键，也可以表示该键所对应的值为null。因此，在HashMap中不能由get()方法来判断HashMap中是否存在某个键， 而应该用containsKey()方法来判断。两个遍历方式的内部实现上不同。Hashtable、HashMap都使用了 Iterator。而由于历史原因，Hashtable还使用了Enumeration的方式 。哈希值的使用不同，HashTable直接使用对象的hashCode。而HashMap重新计算hash值。Hashtable和HashMap它们两个内部实现方式的数组的初始大小和扩容的方式。HashTable中hash数组默认大小是11，增加的方式是 old*2+1。HashMap中hash数组的默认大小是16，而且一定是2的指数。","categories":[{"name":"java base","slug":"java-base","permalink":"http://www.fufan.me/categories/java-base/"}],"tags":[{"name":"java 数据结构","slug":"java-数据结构","permalink":"http://www.fufan.me/tags/java-数据结构/"}]},{"title":"JAVA的Reflection反射机制","slug":"JAVA的Reflection反射机制","date":"2016-09-11T06:55:00.000Z","updated":"2018-11-05T03:13:21.648Z","comments":true,"path":"2016/09/11/JAVA的Reflection反射机制/","link":"","permalink":"http://www.fufan.me/2016/09/11/JAVA的Reflection反射机制/","excerpt":"","text":"反射即reflectionjava反射运用了代理模式，代理模式在之后学习的设计模式中可以了解反射主要用了以下几点：在运行时判断任意一个对象所属的类。在运行时构造任意一个类的对象。在运行时判断任意一个类所具有的成员变量和方法。在运行时调用任意一个对象的方法Class首先要搞清楚Class这个类，每个类在创建的时候都会有Class这个类伴随产生，这个Class是JVM产生的，由于是JVM产生的，所以我们一般获取Class的方法是：object.getClass()Class.forName(“java.lang.String”)Class.getSuperClass()运用.class语法,如java.lang.String.classprimitive wrapper classes的TYPE语法,如：Boolean.TYPE，类似Boolean.class12345Class&lt;?&gt; clazz = s.getClass();Class&lt;?&gt; clazz2 = Class.forName(&quot;com.fufan.reflection.Son&quot;);Class&lt;?&gt; clazz3 = clazz2.getSuperclass();Class&lt;?&gt; clazz4 = com.fufan.reflection.Son.class;Class&lt;?&gt; clazz5 = Boolean.class;Class是Reflection起源。针对任何您想探勘的class，唯有先为它产生一个Class object，接下来才能经由后者唤起为数十多个的Reflection APIs接下来就可以通过Class调用衍生出的一系列API：getName()：获得类的完整名字。getFields()：获得类的public类型的属性。getDeclaredFields()：获得类的所有属性。getMethods()：获得类的public类型的方法。getDeclaredMethods()：获得类的所有方法。getConstructors()：获得类的public类型的构造方法。getMethod(String name, Class[] parameterTypes)：获得类的特定方法，name参数指定方法的名字，parameterTypes 参数指定方法的参数类型。getConstructors()：获得类的public类型的构造方法。getConstructor(Class[] parameterTypes)：获得类的特定构造方法，parameterTypes 参数指定构造方法的参数类型。FieldFiled类：代表类的成员变量（成员变量也称为类的属性）。1String fieldName = field.getName();MethodMethod类：代表类的方法，invoke1Object value = getMethod.invoke(object, new Object[] &#123;&#125;);ConstructorConstructor类：代表类的构造方法，调用有参和无参newInstance()：通过类的不带参数的构造方法创建这个类的一个对象。newInstance(new Object[]{value})：当调用有参构造函数时使用。12345//无参构造方法Constructor constructor1 = classType.getConstructor();//有参构造方法Constructor constructor2 = classType.getConstructor(new Class[] &#123;int.class, String.class&#125;);代码示例11234567891011121314151617181920212223242526272829// 获得对象的类型 Class&lt;?&gt; classType = object.getClass(); System.out.println(&quot;Class:&quot; + classType.getName()); // 通过默认构造方法创建一个新的对象 Object objectCopy = classType.getConstructor(new Class[] &#123;&#125;).newInstance(new Object[] &#123;&#125;); // 获得对象的所有属性 Field fields[] = classType.getDeclaredFields(); for (int i = 0; i &lt; fields.length; i++) &#123; Field field = fields[i]; String fieldName = field.getName(); String firstLetter = fieldName.substring(0, 1).toUpperCase(); // 获得和属性对应的getXXX()方法的名字 String getMethodName = &quot;get&quot; + firstLetter + fieldName.substring(1); // 获得和属性对应的setXXX()方法的名字 String setMethodName = &quot;set&quot; + firstLetter + fieldName.substring(1); // 获得和属性对应的getXXX()方法 Method getMethod = classType.getMethod(getMethodName, new Class[] &#123;&#125;); // 获得和属性对应的setXXX()方法 Method setMethod = classType.getMethod(setMethodName, new Class[] &#123; field.getType() &#125;); // 调用原对象的getXXX()方法 Object value = getMethod.invoke(object, new Object[] &#123;&#125;); System.out.println(fieldName + &quot;:&quot; + value); // 调用拷贝对象的setXXX()方法 setMethod.invoke(objectCopy, new Object[] &#123; value &#125;);&#125;ArrayArray类：提供了动态创建数组，以及访问数组的元素的静态方法代码示例2123456789101112131415161718//一维数组的使用Object array = Array.newInstance(Integer.TYPE, 10);System.out.println(Integer.TYPE);for(int index=1; index&lt;10; index ++)&#123; Array.set(array, index, index);&#125;System.out.println(Array.get(array, 4));//多维数组的使用Object arrays = Array.newInstance(String.class, 3,5);Object array1 = Array.get(arrays, 2);Array.set(array1, 3, &quot;fufan&quot;);String[][] arrayInt = (String[][]) arrays; System.out.println(arrayInt[1][3]);","categories":[{"name":"java base","slug":"java-base","permalink":"http://www.fufan.me/categories/java-base/"}],"tags":[{"name":"java","slug":"java","permalink":"http://www.fufan.me/tags/java/"}]}]}