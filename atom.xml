<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>A Gemini Boy</title>
  
  <subtitle>welcome to my site</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.fufan.me/"/>
  <updated>2018-11-22T09:41:04.506Z</updated>
  <id>http://www.fufan.me/</id>
  
  <author>
    <name>fae88</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>学会写脚本系列（二）—— Python之dict(或对象)与json之间的互相转化</title>
    <link href="http://www.fufan.me/2018/11/22/%E4%BB%8E-I-O-%E6%A8%A1%E5%9E%8B%E5%88%B0-Netty/"/>
    <id>http://www.fufan.me/2018/11/22/从-I-O-模型到-Netty/</id>
    <published>2018-11-22T07:36:00.000Z</published>
    <updated>2018-11-22T09:41:04.506Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Nov 22 2018 17:41:04 GMT+0800 (China Standard Time) --><p>在Python语言中，json数据与dict字典以及对象之间的转化，是必不可少的操作。</p><p>在Python中自带json库。通过import json导入。</p><p>在json模块有2个方法，</p><ul><li>loads()：将json数据转化成dict数据</li><li>dumps()：将dict数据转化成json数据</li><li>load()：读取json文件数据，转成dict数据</li><li>dump()：将dict数据转化成json数据后写入json文件</li></ul><p>示例如下：</p><h3 id="1-dict字典转json数据"><a href="#1-dict字典转json数据" class="headerlink" title="1. dict字典转json数据"></a>1. dict字典转json数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import json</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def dict_to_json():</span><br><span class="line">    dict = &#123;&#125;</span><br><span class="line">    dict[&apos;name&apos;] = &apos;fufan&apos;</span><br><span class="line">    dict[&apos;age&apos;] = 25</span><br><span class="line">    dict[&apos;sex&apos;] = &apos;male&apos;</span><br><span class="line">    print(dict)  # 输出：&#123;&apos;name&apos;: &apos;fufan&apos;, &apos;age&apos;: 25, &apos;sex&apos;: &apos;male&apos;&#125;</span><br><span class="line">    j = json.dumps(dict)</span><br><span class="line">    print(j)  # 输出：&#123;&quot;name&quot;: &quot;fufan&quot;, &quot;age&quot;: 25, &quot;sex&quot;: &quot;male&quot;&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    dict_to_json()</span><br></pre></td></tr></table></figure><h3 id="2-对象转json数据"><a href="#2-对象转json数据" class="headerlink" title="2. 对象转json数据"></a>2. 对象转json数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">import json</span><br><span class="line"></span><br><span class="line">class Student(object):</span><br><span class="line"></span><br><span class="line">    id = &apos;&apos;</span><br><span class="line">    name = &apos;&apos;</span><br><span class="line">    age = 0</span><br><span class="line">    gender = &apos;&apos;</span><br><span class="line">    phone = &apos;&apos;</span><br><span class="line">    email = &apos;&apos;</span><br><span class="line">    def __init__(self, id, name, age, gender, phone, email):</span><br><span class="line">        self.id = id</span><br><span class="line">        self.name = name</span><br><span class="line">        self.age = age</span><br><span class="line">        self.gender = gender</span><br><span class="line">        self.phone = phone</span><br><span class="line">        self.email = email</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def obj_to_json():</span><br><span class="line">    stu = Student(&apos;fufan&apos;, &apos;fufan&apos;, 28, &apos;male&apos;, &apos;13000000000&apos;, &apos;fufan@51dojo.com&apos;)</span><br><span class="line">    print(type(stu))  # &lt;class &apos;json_test.student.Student&apos;&gt;</span><br><span class="line">    stu = stu.__dict__  # 将对象转成dict字典</span><br><span class="line">    print(type(stu))  # &lt;class &apos;dict&apos;&gt;</span><br><span class="line">    print(stu)  # &#123;&apos;id&apos;: &apos;fufan&apos;, &apos;name&apos;: &apos;fufan&apos;, &apos;age&apos;: 28, &apos;gender&apos;: &apos;male&apos;, &apos;phone&apos;: &apos;13000000000&apos;, &apos;email&apos;: &apos;fufan@51dojo.com&apos;&#125;</span><br><span class="line">    j = json.dumps(obj=stu)</span><br><span class="line">    print(j)  # &#123;&quot;id&quot;: &quot;fufan&quot;, &quot;name&quot;: &quot;fufan&quot;, &quot;age&quot;: 28, &quot;gender&quot;: &quot;male&quot;, &quot;phone&quot;: &quot;13000000000&quot;, &quot;email&quot;: &quot;fufan@51dojo.com&quot;&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    obj_to_json()</span><br></pre></td></tr></table></figure><h3 id="3-json数据转成dict字典"><a href="#3-json数据转成dict字典" class="headerlink" title="3. json数据转成dict字典"></a>3. json数据转成dict字典</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">import json</span><br><span class="line"></span><br><span class="line">def json_to_dict():</span><br><span class="line">    j = &apos;&#123;&quot;id&quot;: &quot;fufan&quot;, &quot;name&quot;: &quot;fufan&quot;, &quot;age&quot;: 28, &quot;gender&quot;: &quot;male&quot;, &quot;phone&quot;: &quot;13000000000&quot;, &quot;email&quot;: &quot;fufan@51dojo.com&quot;&#125;&apos;</span><br><span class="line">    dict = json.loads(s=j)</span><br><span class="line">    print(dict)  # &#123;&apos;id&apos;: &apos;fufan&apos;, &apos;name&apos;: &apos;fufan&apos;, &apos;age&apos;: 28, &apos;gender&apos;: &apos;male&apos;, &apos;phone&apos;: &apos;13000000000&apos;, &apos;email&apos;: &apos;fufan@51dojo.com&apos;&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    json_to_dict()</span><br></pre></td></tr></table></figure><h3 id="4-json数据转成对象"><a href="#4-json数据转成对象" class="headerlink" title="4. json数据转成对象"></a>4. json数据转成对象</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import json</span><br><span class="line"></span><br><span class="line">class Student(object):</span><br><span class="line"></span><br><span class="line">    id = &apos;&apos;</span><br><span class="line">    name = &apos;&apos;</span><br><span class="line">    age = 0</span><br><span class="line">    gender = &apos;&apos;</span><br><span class="line">    phone = &apos;&apos;</span><br><span class="line">    email = &apos;&apos;</span><br><span class="line"></span><br><span class="line">def json_to_obj():</span><br><span class="line">    j = &apos;&#123;&quot;id&quot;: &quot;fufan&quot;, &quot;name&quot;: &quot;fufan&quot;, &quot;age&quot;: 28, &quot;gender&quot;: &quot;male&quot;, &quot;phone&quot;: &quot;13000000000&quot;, &quot;email&quot;: &quot;fufan@51dojo.com&quot;&#125;&apos;</span><br><span class="line">    dict = json.loads(s=j)</span><br><span class="line">    stu = Student()</span><br><span class="line">    stu.__dict__ = dict</span><br><span class="line">    print(&apos;id: &apos; + stu.id + &apos; name: &apos; + stu.name + &apos; age: &apos; + str(stu.age) + &apos; gender: &apos; + str(</span><br><span class="line">        stu.gender) + &apos; phone: &apos; + stu.phone + &apos; email: &apos; + stu.email)</span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    json_to_obj()</span><br></pre></td></tr></table></figure><h3 id="5-dump-方法的使用"><a href="#5-dump-方法的使用" class="headerlink" title="5. dump()方法的使用"></a>5. dump()方法的使用</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import json</span><br><span class="line"></span><br><span class="line">def dict_to_json_write_file():</span><br><span class="line">    dict = &#123;&#125;</span><br><span class="line">    dict[&apos;name&apos;] = &apos;fufan&apos;</span><br><span class="line">    dict[&apos;age&apos;] = 26</span><br><span class="line">    dict[&apos;gender&apos;] = &apos;male&apos;</span><br><span class="line">    print(dict)  # &#123;&apos;name&apos;: &apos;fufan&apos;, &apos;age&apos;: 26, &apos;gender&apos;: &apos;male&apos;&#125;</span><br><span class="line">    with open(&apos;test.json&apos;, &apos;w&apos;) as f:</span><br><span class="line">        json.dump(dict, f)  # 会在目录下生成一个test.json的文件，文件内容是dict数据转成的json数据</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    dict_to_json_write_file()</span><br></pre></td></tr></table></figure><h3 id="6-load-的使用"><a href="#6-load-的使用" class="headerlink" title="6. load()的使用"></a>6. load()的使用</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import json</span><br><span class="line"></span><br><span class="line">def json_file_to_dict():</span><br><span class="line">    with open(&apos;test.json&apos;, &apos;r&apos;) as f:</span><br><span class="line">        dict = json.load(fp=f)</span><br><span class="line">        print(dict)  # &#123;&apos;name&apos;: &apos;fufan&apos;, &apos;age&apos;: 26, &apos;gender&apos;: &apos;male&apos;&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    json_file_to_dict()</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Thu Nov 22 2018 17:41:04 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;在Python语言中，json数据与dict字典以及对象之间的转化，是必不可少的操作。&lt;/p&gt;&lt;p&gt;在Python中自带js
      
    
    </summary>
    
    
      <category term="python" scheme="http://www.fufan.me/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>Spring获取bean的几种方式</title>
    <link href="http://www.fufan.me/2018/11/21/Spring%E8%8E%B7%E5%8F%96bean%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F/"/>
    <id>http://www.fufan.me/2018/11/21/Spring获取bean的几种方式/</id>
    <published>2018-11-21T07:08:42.000Z</published>
    <updated>2018-11-21T07:09:22.255Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Nov 22 2018 10:01:02 GMT+0800 (China Standard Time) --><p>Spring获取bean的几种方式</p><ol><li>在初始化时保存ApplicationContext对象</li><li>通过Spring提供的utils类获取ApplicationContext对象</li><li>继承自抽象类ApplicationObjectSupport</li><li>继承自抽象类WebApplicationObjectSupport</li><li>实现接口ApplicationContextAware （推荐）</li><li>通过Spring提供的ContextLoader</li></ol><h4 id="在初始化时保存ApplicationContext对象"><a href="#在初始化时保存ApplicationContext对象" class="headerlink" title="在初始化时保存ApplicationContext对象"></a>在初始化时保存ApplicationContext对象</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ApplicationContext ac = new FileSystemXmlApplicationContext(&quot;applicationContext.xml&quot;); </span><br><span class="line">ac.getBean(&quot;userService&quot;);//比如：&lt;bean id=&quot;userService&quot; class=&quot;com.cloud.service.impl.UserServiceImpl&quot;&gt;&lt;/bean&gt;</span><br></pre></td></tr></table></figure><p>这样的方式适用于採用Spring框架的独立应用程序，须要程序通过配置文件手工初始化Spring的情况。</p><h4 id="通过Spring提供的工具类获取ApplicationContext对象"><a href="#通过Spring提供的工具类获取ApplicationContext对象" class="headerlink" title="通过Spring提供的工具类获取ApplicationContext对象"></a>通过Spring提供的工具类获取ApplicationContext对象</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ApplicationContext ac1 = WebApplicationContextUtils.getRequiredWebApplicationContext(ServletContext sc); </span><br><span class="line">ApplicationContext ac2 = WebApplicationContextUtils.getWebApplicationContext(ServletContext sc); </span><br><span class="line">ac1.getBean(&quot;beanId&quot;); </span><br><span class="line">ac2.getBean(&quot;beanId&quot;);</span><br></pre></td></tr></table></figure><p>这样的方式适合于採用Spring框架的B/S系统，通过ServletContext对象获取ApplicationContext对象。然后在通过它获取须要的类实例。上面两个工具方式的差别是，前者在获取失败时抛出异常。后者返回null。</p><h4 id="继承自抽象类ApplicationObjectSupport"><a href="#继承自抽象类ApplicationObjectSupport" class="headerlink" title="继承自抽象类ApplicationObjectSupport"></a>继承自抽象类ApplicationObjectSupport</h4><p>抽象类ApplicationObjectSupport提供getApplicationContext()方法。能够方便的获取ApplicationContext。</p><p>Spring初始化时。会通过该抽象类的setApplicationContext(ApplicationContext context)方法将ApplicationContext 对象注入。</p><h4 id="继承自抽象类WebApplicationObjectSupport"><a href="#继承自抽象类WebApplicationObjectSupport" class="headerlink" title="继承自抽象类WebApplicationObjectSupport"></a>继承自抽象类WebApplicationObjectSupport</h4><p>类似上面方法。调用getWebApplicationContext()获取WebApplicationContext</p><h4 id="实现接口ApplicationContextAware"><a href="#实现接口ApplicationContextAware" class="headerlink" title="实现接口ApplicationContextAware"></a>实现接口ApplicationContextAware</h4><p>实现该接口的setApplicationContext(ApplicationContext context)方法，并保存ApplicationContext 对象。Spring初始化时，会通过该方法将ApplicationContext对象注入。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">@Component</span><br><span class="line">@Slf4j</span><br><span class="line">public class ApplicationContextProvider implements ApplicationContextAware &#123;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * 上下文对象实例</span><br><span class="line">     */</span><br><span class="line">    private ApplicationContext applicationContext;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123;</span><br><span class="line">        this.applicationContext = applicationContext;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public ApplicationContext getApplicationContext() &#123;</span><br><span class="line">        return applicationContext;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * 通过name获取 Bean.</span><br><span class="line">     * @param name</span><br><span class="line">     * @return</span><br><span class="line">     */</span><br><span class="line">    public Object getBean(String name)&#123;</span><br><span class="line">        return getApplicationContext().getBean(name);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * 通过class获取Bean.</span><br><span class="line">     * @param clazz</span><br><span class="line">     * @param &lt;T&gt;</span><br><span class="line">     * @return</span><br><span class="line">     */</span><br><span class="line">    public &lt;T&gt; T getBean(Class&lt;T&gt; clazz)&#123;</span><br><span class="line">        return getApplicationContext().getBean(clazz);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * 通过name,以及Clazz返回指定的Bean</span><br><span class="line">     * @param name</span><br><span class="line">     * @param clazz</span><br><span class="line">     * @param &lt;T&gt;</span><br><span class="line">     * @return</span><br><span class="line">     */</span><br><span class="line">    public &lt;T&gt; T getBean(String name,Class&lt;T&gt; clazz)&#123;</span><br><span class="line">        return getApplicationContext().getBean(name, clazz);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>此方法在工作中用的比较多，也是比较好的一种方法</p><h4 id="通过Spring提供的ContextLoader"><a href="#通过Spring提供的ContextLoader" class="headerlink" title="通过Spring提供的ContextLoader"></a>通过Spring提供的ContextLoader</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">WebApplicationContext wac = ContextLoader.getCurrentWebApplicationContext();</span><br><span class="line">wac.getBean(beanID);</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Thu Nov 22 2018 10:01:02 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;Spring获取bean的几种方式&lt;/p&gt;&lt;ol&gt;&lt;li&gt;在初始化时保存ApplicationContext对象&lt;/li&gt;&lt;
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>学会写脚本系列（一）—— Python Requests包的使用</title>
    <link href="http://www.fufan.me/2018/11/19/%E5%AD%A6%E4%BC%9A%E5%86%99%E8%84%9A%E6%9C%AC%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94-Python-Requests%E5%8C%85%E7%9A%84%E4%BD%BF%E7%94%A8/"/>
    <id>http://www.fufan.me/2018/11/19/学会写脚本系列（一）——-Python-Requests包的使用/</id>
    <published>2018-11-19T09:39:00.000Z</published>
    <updated>2018-11-22T09:40:11.818Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Nov 22 2018 17:40:12 GMT+0800 (China Standard Time) --><p>由于公司的需求，平时需要测试一些公网爬虫，不过java在写小demo的时候太重的，所以必须要学点python、js、shell等语言来写一些脚本。所以平时抽空需要准备一些工具脚本，来支持临时测试。</p><p>今天我介绍用python来做批量跑http请求的脚本，刚好搜到了requests这个包，感觉用起来还挺方便的，具体的文档的话可以参考<a href="http://docs.python-requests.org/zh_CN/latest/" target="_blank" rel="noopener">中文官方文档</a>和<a href="http://www.python-requests.org/en/master/" target="_blank" rel="noopener">英文官方文档</a></p><p>requests是python的一个HTTP客户端库，跟urllib，urllib2类似，那为什么要用requests而不用urllib2呢？官方文档中是这样说明的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python的标准库urllib2提供了大部分需要的HTTP功能，但是API太逆天了，一个简单的功能就需要一大堆代码。</span><br></pre></td></tr></table></figure><p>官方网站设计的原则如下：</p><ul><li>Beautiful is better than ugly.(美丽优于丑陋)</li><li>Explicit is better than implicit.(清楚优于含糊)</li><li>Simple is better than complex.(简单优于复杂)</li><li>Complex is better than complicated.(复杂优于繁琐)</li><li>Readability counts.(重要的是可读性)</li></ul><h3 id="1-安装Request"><a href="#1-安装Request" class="headerlink" title="1. 安装Request"></a>1. 安装Request</h3><ul><li>pip安装<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install requests</span><br></pre></td></tr></table></figure></li></ul><ul><li>源码安装<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git clone     git://github.com/kennethreitz/requests.git</span><br><span class="line">$ cd requests</span><br><span class="line">$ python setup.py install</span><br></pre></td></tr></table></figure></li></ul><ul><li>pycharm安装</li></ul><h3 id="2-快速上手"><a href="#2-快速上手" class="headerlink" title="2. 快速上手"></a>2. 快速上手</h3><p>既然官网的文档写的那么详细了，我这里就直接上例子了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line">def do_http_test():</span><br><span class="line"></span><br><span class="line"># 请求url</span><br><span class="line">    url = &apos;https://s.weibo.com/ajax_User/follow&apos;</span><br><span class="line">    # header头，Cookie可以放在这里</span><br><span class="line">    headers = &#123;&apos;content-type&apos;: &quot;application/x-www-form-urlencoded&quot;,</span><br><span class="line">               &apos;Referer&apos;: &apos;https://s.weibo.com/user?q=%E6%B8%B8%E6%88%8F&apos;,</span><br><span class="line">               &apos;Cookie&apos;: &apos;SUB=_2A2528mffDeRhGeBJ4lUR9S3EyzyIHXVVht4XrDV8PUNbmtBeLUfekW9NRjmhG0VwLiyjpeOPdOk01GeypRYWWaEf&apos;&#125;</span><br><span class="line"></span><br><span class="line">    # post请求的请求data</span><br><span class="line">    payload = &#123;&apos;uid&apos;: &apos;5126161537&apos;, &apos;type&apos;: &apos;followed&apos;, &apos;action_code&apos;: &apos;71&apos;&#125;</span><br><span class="line"></span><br><span class="line">    # 具体调用的.post方法</span><br><span class="line">    r = requests.post(url, data = payload, headers = headers)</span><br><span class="line"></span><br><span class="line">    # 打印结果</span><br><span class="line">    print(r.text)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    do_http_test()</span><br></pre></td></tr></table></figure><h3 id="3-高级用法"><a href="#3-高级用法" class="headerlink" title="3. 高级用法"></a>3. 高级用法</h3><p>未完待续……</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Thu Nov 22 2018 17:40:12 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;由于公司的需求，平时需要测试一些公网爬虫，不过java在写小demo的时候太重的，所以必须要学点python、js、shel
      
    
    </summary>
    
      <category term="python" scheme="http://www.fufan.me/categories/python/"/>
    
    
      <category term="python" scheme="http://www.fufan.me/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>聚簇索引和非聚簇索引</title>
    <link href="http://www.fufan.me/2018/11/16/%E8%81%9A%E7%B0%87%E7%B4%A2%E5%BC%95%E5%92%8C%E9%9D%9E%E8%81%9A%E7%B0%87%E7%B4%A2%E5%BC%95/"/>
    <id>http://www.fufan.me/2018/11/16/聚簇索引和非聚簇索引/</id>
    <published>2018-11-16T09:37:00.000Z</published>
    <updated>2018-11-16T09:37:40.595Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Nov 21 2018 15:08:21 GMT+0800 (China Standard Time) --><p>1、大多数表都应该有聚簇索引或使用分区来降低对表尾页的竞争，在一个高事务的环境中，对最后一页的封锁严重影响系统的吞吐量。<br>2、在聚簇索引下，数据在物理上按顺序排在数据页上，重复值也排在一起，因而在那些包含范围检查 (between、&lt;、&lt;=、&gt;、&gt;=)或使用group by或orderby的查询时，一旦找到具有范围中第一个键值的行，具有后续索引值的行保证物理上毗连在一起而不必进一步搜索，避免了大范围扫描，可以大 大提高查询速度。<br>3、在一个频繁发生插入操作的表上建立聚簇索引时，不要建在具有单调上升值的列(如IDENTITY)上，否则会经常引起封锁冲突。<br>4、在聚簇索引中不要包含经常修改的列，因为码值修改后，数据行必须移动到新的位置。<br>5、选择聚簇索引应基于where子句和连接操作的类型。</p><p>不知从什么角度来对比，只能说说各自的特点，希望对你有用。</p><h4 id="1、聚簇索引"><a href="#1、聚簇索引" class="headerlink" title="1、聚簇索引"></a>1、聚簇索引</h4><p>a) 一个索引项直接对应实际数据记录的存储页，可谓“直达”<br>b) 主键缺省使用它<br>c) 索引项的排序和数据行的存储排序完全一致，利用这一点，想修改数据的存储顺序，可以通过改变主键的方法（撤销原有主键，另找也能满足主键要求的一个字段或一组字段，重建主键）<br>d) 一个表只能有一个聚簇索引（理由：数据一旦存储，顺序只能有一种）</p><h4 id="2、非聚簇索引"><a href="#2、非聚簇索引" class="headerlink" title="2、非聚簇索引"></a>2、非聚簇索引</h4><p>a) 不能“直达”，可能链式地访问多级页表后，才能定位到数据页<br>b) 一个表可以有多个非聚簇索引</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Wed Nov 21 2018 15:08:21 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;1、大多数表都应该有聚簇索引或使用分区来降低对表尾页的竞争，在一个高事务的环境中，对最后一页的封锁严重影响系统的吞吐量。&lt;b
      
    
    </summary>
    
      <category term="数据库" scheme="http://www.fufan.me/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="http://www.fufan.me/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>从session到token</title>
    <link href="http://www.fufan.me/2018/11/14/%E4%BB%8Esession%E5%88%B0token/"/>
    <id>http://www.fufan.me/2018/11/14/从session到token/</id>
    <published>2018-11-14T06:15:00.000Z</published>
    <updated>2018-11-14T07:34:31.747Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Nov 21 2018 15:08:21 GMT+0800 (China Standard Time) --><h3 id="很久以前"><a href="#很久以前" class="headerlink" title="很久以前"></a>很久以前</h3><p>30年前，互联网刚起步的时候，大部分的web请求基本都是在浏览网页，既然是浏览，作为一个服务器， 为什么要记住谁在一段时间里都浏览了什么文档呢？</p><h3 id="开始记录session"><a href="#开始记录session" class="headerlink" title="开始记录session"></a>开始记录session</h3><p>但是好日子没持续多久， 很快大家就不满足于静态的Html 文档了， 交互式的Web应用开始兴起， 尤其是论坛， 在线购物等网站。那么必须记住哪些人登录系统， 哪些人往自己的购物车中放了商品， 也就是说必须把每个人区分开。</p><p>由于HTTP协议的无状态特性， 我必须加点小手段，才能完成会话管理。</p><p>所以在这个时候，服务器端一般会使用sessionId（会话标识）来标识该用户在一段时间内的请求，其实就是一个随机的字符串。这样每次请求过来，服务器端需要区分谁是谁。</p><h3 id="沉重的负担"><a href="#沉重的负担" class="headerlink" title="沉重的负担"></a>沉重的负担</h3><p>由于互联网的飞速发展，web请求量剧增，导致服务器压力增大，同时开销也增大。比如每个人只需要保存自己的session id，而服务器需要保存所有人的session id 。</p><p>虽然可以通过分布式集群扩展服务能力，但是这里又会出现session分布的问题。比如说我用两个机器组成了一个集群， 小F通过机器A登录了系统， 那session id会保存在机器A上， 假设小F的下一次请求被转发到机器B怎么办？ 机器B可没有小F的 session id啊。</p><p>虽然session sticky、缓存可以在一定程度上解决该问题，但是都会存在单点故障、机器宕机的问题。导致可靠性降低，同时增大了服务器的架构复杂度，不便于后期的扩展和维护。</p><h3 id="时间换空间"><a href="#时间换空间" class="headerlink" title="时间换空间"></a>时间换空间</h3><p>同时，session的安全性也是一个隐患，不怀好意的人可以通过伪造sessionId来请求。</p><p>渐渐地，很多网站开始使用JWT、UUID的方式登录或者保存用户状态信息。</p><p><a href="https://jwt.io/" target="_blank" rel="noopener">JWT</a>是json web token缩写。它将用户信息加密到token里，服务器不保存任何用户信息。服务器通过使用保存的密钥验证token的正确性，只要正确即通过验证。</p><ul><li>优点是在分布式系统中，很好地解决了单点登录问题，很容易解决了session共享的问题。</li><li>缺点是无法作废已颁布的令牌/不易应对数据过期。</li></ul><p>这样的话，就无状态的用token来替代了session，用CPU的计算时间来替换session的存储空间</p><p>JWT 的实践其实还是挺简单。安全性也是得到了保证，后端只需要保存着密匙，其他数据可以保存在token，由前端携带，这样可以减低后端的内心消耗。<br>虽然token是加密的，但是携带的验证数据还是不要是敏感数据.</p><h3 id="思想的迁移"><a href="#思想的迁移" class="headerlink" title="思想的迁移"></a>思想的迁移</h3><p>目前公司爬虫业务也有类似的问题，我们每一个任务的taskId是通过生成一个UUID来绑定和溯源的，这样可以保证全局唯一性，是很好的对分布式系统任务分离的一个实践。UUID生成的规则:</p><p><img src="/image/session-token-0.png" alt=""></p><p>如图所示，这里第1位不可用，前41位表示时间，中间10位用来表示工作机器的id，后12位的序列号.<br>其中时间比较好理解，工作机器id则是机器标识，序列号是一个自增序列。有多少位表示在这一个单位时间内，此机器最多可以支持2^12个并发。在进入下一个时间单位后，序列号归0。</p><p>虽然这种模式再生产上单日百万量级的服务已经实践了一年多，微服务的切分也是达到了40-50个之多，但是会发现有一个问题，就是token里未带任何任务相关的信息，而随着我们的业务量的增长，遇到的单点故障及其他复杂问题也增加（特别是aliyun中redis中间件常常会出现闪断等问题，包括db、oss、ots等都会有类似的情况）。所以要完全解决这种方式的话，就必须要有主备环境，随时切换，以应对单点故障带来对服务的影响（毕竟是24*7的服务，出一次事故对客户带来损失巨大）。</p><p>而切换环境的瞬间，情况复杂度也是非常复杂的，因为你首先要保证之前的任务是正常在老环境里跑的，而新的任务也要按比例分流到新环境中来，但是需要让用户是无感知切换的话，入口是不能变的。所以我们就会考虑后端其他的微服务如何去将gateway中生成的taskId分配到哪个环境中。</p><p>因为环境完全是两套，假设我们分为C区任务和D区任务，而如果能够在taskId中添加一位来表示是哪个区的话，就可以路由到哪个环境中去正常的执行下去。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Wed Nov 21 2018 15:08:21 GMT+0800 (China Standard Time) --&gt;&lt;h3 id=&quot;很久以前&quot;&gt;&lt;a href=&quot;#很久以前&quot; class=&quot;headerlink&quot; title=&quot;很久以前&quot;&gt;&lt;/a
      
    
    </summary>
    
      <category term="分布式" scheme="http://www.fufan.me/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
    
      <category term="jwt" scheme="http://www.fufan.me/tags/jwt/"/>
    
  </entry>
  
  <entry>
    <title>JVM专题（八）---强、软、弱、虚的知识点总结</title>
    <link href="http://www.fufan.me/2018/11/13/Java%E5%9B%9B%E7%A7%8D%E5%BC%95%E7%94%A8-%E5%BC%BA%E3%80%81%E8%BD%AF%E3%80%81%E5%BC%B1%E3%80%81%E8%99%9A%E7%9A%84%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93/"/>
    <id>http://www.fufan.me/2018/11/13/Java四种引用-强、软、弱、虚的知识点总结/</id>
    <published>2018-11-13T09:47:00.000Z</published>
    <updated>2018-11-14T03:27:48.331Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Nov 21 2018 15:08:21 GMT+0800 (China Standard Time) --><p>本文会按照以下思路进行：<br><a href="#1">（1）Java的四种对象引用的基本概念</a><br><br><a href="#2">（2）四种对象引用的差异对比</a><br><br><a href="#3">（3）对象可及性的判断以及与垃圾回收机制的关系</a><br><br><a href="#4">（4）引用队列ReferenceQueue的介绍</a><br><br><a href="#5">（5）WeakHashMap的相关介绍</a><br></p><h3 id="1">Java的四种对象引用的基本概念</h3><p>从JDK1.2版本开始，把对象的引用分为四种级别，从而使程序更加灵活的控制对象的生命周期。这四种级别由高到低依次为：强引用、软引用、弱引用和虚引用。</p><h4 id="1、强引用"><a href="#1、强引用" class="headerlink" title="1、强引用"></a>1、强引用</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Object obj =new Object();</span><br></pre></td></tr></table></figure><p>上述Object这类对象就具有强引用，属于不可回收的资源，垃圾回收器绝不会回收它。当内存空间不足，Java虚拟机宁愿抛出OutOfMemoryError错误，使程序异常终止，也不会靠回收具有强引用的对象，来解决内存不足的问题。</p><p>值得注意的是：如果想中断或者回收强引用对象，可以显式地将引用赋值为null，这样的话JVM就会在合适的时间，进行垃圾回收。</p><p>下图是堆区的内存示意图，分为新生代，老生代，而垃圾回收主要也是在这部分区域中进行。</p><p><img src="/image/java_reference-0.png" alt=""></p><h4 id="2、软引用（SoftReference）"><a href="#2、软引用（SoftReference）" class="headerlink" title="2、软引用（SoftReference）"></a>2、软引用（SoftReference）</h4><p>如果一个对象只具有软引用，那么它的性质属于可有可无的那种。如果此时内存空间足够，垃圾回收器就不会回收它，如果内存空间不足了，就会回收这些对象的内存。只要垃圾回收器没有回收它，该对象就可以被程序使用。</p><p>软引用可用来实现内存敏感的告诉缓存。软引用可以和一个引用队列联合使用，如果软件用所引用的对象被垃圾回收，Java虚拟机就会把这个软引用加入到与之关联的引用队列中。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Object obj = new Object();</span><br><span class="line">       ReferenceQueue queue = new ReferenceQueue();</span><br><span class="line">       SoftReference reference = new SoftReference(obj, queue);</span><br><span class="line">       //强引用对象滞空，保留软引用</span><br><span class="line">       obj = null;</span><br></pre></td></tr></table></figure><p>当内存不足时，软引用对象被回收时，reference.get()为null，此时软引用对象的作用已经发挥完毕，这时将其添加进ReferenceQueue 队列中</p><p>如果要判断哪些软引用对象已经被清理：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SoftReference ref = null;</span><br><span class="line">        while ((ref = (SoftReference) queue.poll()) != null) &#123;</span><br><span class="line">            //清除软引用对象</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><h4 id="3、弱引用-WeakReference"><a href="#3、弱引用-WeakReference" class="headerlink" title="3、弱引用(WeakReference)"></a>3、弱引用(WeakReference)</h4><p>如果一个对象具有弱引用，那其的性质也是可有可无的状态。</p><p>而弱引用和软引用的区别在于：弱引用的对象拥有更短的生命周期，只要垃圾回收器扫描到它，不管内存空间充足与否，都会回收它的内存。</p><p>同样的弱引用也可以和引用队列一起使用。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Object obj = new Object();</span><br><span class="line">        ReferenceQueue queue = new ReferenceQueue();</span><br><span class="line">        WeakReference reference = new WeakReference(obj, queue);</span><br><span class="line">        //强引用对象滞空，保留软引用</span><br><span class="line">        obj = null;</span><br></pre></td></tr></table></figure><h4 id="4、虚引用（PhantomReference）"><a href="#4、虚引用（PhantomReference）" class="headerlink" title="4、虚引用（PhantomReference）"></a>4、虚引用（PhantomReference）</h4><p>虚引用和前面的软引用、弱引用不同，它并不影响对象的生命周期。如果一个对象与虚引用关联，则跟没有引用与之关联一样，在任何时候都可能被垃圾回收器回收。</p><p>注意：虚引用必须和引用队列关联使用，当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会把这个虚引用加入到与之关联的引用队列中。</p><p>程序可以通过判断引用队列中是否已经加入了虚引用，来了解被引用的对象是否将要被垃圾回收。如果程序发现某个虚引用已经被加入到引用队列，那么就可以在所引用的对象的内存被回收之前采取必要的行动。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Object obj = new Object();</span><br><span class="line">        ReferenceQueue queue = new ReferenceQueue();</span><br><span class="line">        PhantomReference reference = new PhantomReference(obj, queue);</span><br><span class="line">        //强引用对象滞空，保留软引用</span><br><span class="line">        obj = null;</span><br></pre></td></tr></table></figure><p>引用总结</p><p>1.对于强引用，平时在编写代码时会经常使用。</p><p>2.而其他三种类型的引用，使用得最多就是软引用和弱引用，这两种既有相似之处又有区别，他们都来描述非必须对象。</p><p>3.被软引用关联的对象只有在内存不足时才会被回收，而被弱引用关联的对象在JVM进行垃圾回收时总会被回收。</p><p><img src="/image/java_reference-1.png" alt=""></p><h3 id="2">四种对象引用的差异对比</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">强引用 &gt; 软引用 &gt; 弱引用 &gt; 虚引用</span><br></pre></td></tr></table></figure><p><img src="/image/java_reference-2.png" alt=""></p><p>最后总结成一张表格：</p><table><thead><tr><th>引用类型</th><th>被垃圾回收时间</th><th>用途</th><th>生存时间</th></tr></thead><tbody><tr><td>强引用</td><td>从来不会</td><td>对象的一般状态</td><td>JVM停止运行时终止</td></tr><tr><td>软引用</td><td>在内存不足时</td><td>对象缓存</td><td>内存不足时终止</td></tr><tr><td>弱引用</td><td>在垃圾回收时</td><td>对象缓存</td><td>垃圾回收时终止</td></tr><tr><td>虚引用</td><td>Unkonwn</td><td>Unkonwn</td><td>Unkonwn</td></tr></tbody></table><h3 id="3">对象可及性的判断</h3><p>在很多的时候，一个对象并不是从根集直接引用的，而是一个对象被其他对象引用，甚至同时被几个对象所引用，从而构成一个以根集为顶的树形结构。</p><p><img src="/image/java_reference-3.png" alt=""></p><p>在这个树形的引用链中，箭头的方向代表了引用的方向，所指向的对象是被引用对象。由图可以看出，从根集到一个对象可以由很多条路径。</p><p>比如到达对象5的路径就有① -&gt; ⑤，③ -&gt;⑦两条路径。由此带来了一个问题，那就是某个对象的可及性如何判断：</p><p>（1）单条引用路径可及性判断：</p><p>在这条路径中，最弱的一个引用决定对象的可及性。</p><p>（2）多条引用路径可及性判断：</p><p>几条路径中，最强的一条的引用决定对象的可及性。</p><p>比如，我们假设图2中引用①和③为强引用，⑤为软引用，⑦为弱引用，对于对象5按照这两个判断原则，路径①-⑤取最弱的引用⑤，因此该路径对对象5的引用为软引用。同样，③-⑦为弱引用。在这两条路径之间取最强的引用，于是对象5是一个软可及对象。</p><p>另外两个重要的点：</p><p>**1. 强可达的对象一定不会被清理</p><ol start="2"><li>JVM保证抛出out of memory之前，清理所有的软引用对象**</li></ol><h3 id="4">引用队列ReferenceQueue的介绍</h3><p>引用队列配合Reference的子类等使用,当引用对象所指向的对象被垃圾回收后,该Reference则被追加到引用队列的末尾.</p><h3 id="5">WeakHashMap的相关介绍</h3><p>在Java集合中有一种特殊的Map类型即WeakHashMap,在这种Map中存放了键对象的弱引用,当一个键对象被垃圾回收器回收时,那么相应的值对象的引用会从Map中删除.</p><p>WeakHashMap能够节约储存空间,可用来缓存那些非必须存在的数据.</p><p>而WeakHashMap是主要通过expungeStaleEntries()这个方法来实现的,而WeakHashMap也内置了一个ReferenceQueue,来获取键对象的引用情况.</p><p>这个方法,相当于遍历ReferenceQueue然后,将已经被回收的键对象,对应的值对象滞空.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">private void expungeStaleEntries() &#123;</span><br><span class="line">        for (Object x; (x = queue.poll()) != null; ) &#123;</span><br><span class="line">            synchronized (queue) &#123;</span><br><span class="line">                @SuppressWarnings(&quot;unchecked&quot;)</span><br><span class="line">                    Entry&lt;K,V&gt; e = (Entry&lt;K,V&gt;) x;</span><br><span class="line">                int i = indexFor(e.hash, table.length);</span><br><span class="line"></span><br><span class="line">                Entry&lt;K,V&gt; prev = table[i];</span><br><span class="line">                Entry&lt;K,V&gt; p = prev;</span><br><span class="line">                while (p != null) &#123;</span><br><span class="line">                    Entry&lt;K,V&gt; next = p.next;</span><br><span class="line">                    if (p == e) &#123;</span><br><span class="line">                        if (prev == e)</span><br><span class="line">                            table[i] = next;</span><br><span class="line">                        else</span><br><span class="line">                            prev.next = next;</span><br><span class="line">                        // Must not null out e.next;</span><br><span class="line">                        // stale entries may be in use by a HashIterator</span><br><span class="line">                        //通过滞空,来帮助垃圾回收</span><br><span class="line">                        e.value = null; </span><br><span class="line">                        size--;</span><br><span class="line">                        break;</span><br><span class="line">                    &#125;</span><br><span class="line">                    prev = p;</span><br><span class="line">                    p = next;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>而且需要注意的是:</p><p>expungeStaleEntries()并不是自动调用的,需要外部对WeakHashMap对象进行查询或者操作,才会进行自动释放的操作</p><p>来一张总结图:</p><p><img src="/image/java_reference-4.png" alt=""></p><h2 id="参考博文"><a href="#参考博文" class="headerlink" title="参考博文"></a>参考博文</h2><p><a href="https://blog.csdn.net/l540675759/article/details/73733763" target="_blank" rel="noopener">Java四种引用—强、软、弱、虚的知识点总结</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Wed Nov 21 2018 15:08:21 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;本文会按照以下思路进行：&lt;br&gt;&lt;a href=&quot;#1&quot;&gt;（1）Java的四种对象引用的基本概念&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;a
      
    
    </summary>
    
      <category term="jvm" scheme="http://www.fufan.me/categories/jvm/"/>
    
    
      <category term="jvm" scheme="http://www.fufan.me/tags/jvm/"/>
    
  </entry>
  
  <entry>
    <title>QPS/TPS/并发量/系统吞吐量的概念</title>
    <link href="http://www.fufan.me/2018/11/11/QPS-TPS-%E5%B9%B6%E5%8F%91%E9%87%8F-%E7%B3%BB%E7%BB%9F%E5%90%9E%E5%90%90%E9%87%8F%E7%9A%84%E6%A6%82%E5%BF%B5/"/>
    <id>http://www.fufan.me/2018/11/11/QPS-TPS-并发量-系统吞吐量的概念/</id>
    <published>2018-11-11T07:52:32.000Z</published>
    <updated>2018-11-11T07:53:00.332Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Nov 21 2018 15:08:20 GMT+0800 (China Standard Time) --><h3 id="系统吞吐量的关键参数"><a href="#系统吞吐量的关键参数" class="headerlink" title="系统吞吐量的关键参数"></a>系统吞吐量的关键参数</h3><p>一个系统的吞度量（承压能力）与request对CPU的消耗、外部接口、IO等等紧密关联。单个reqeust 对CPU消耗越高，外部系统接口、IO影响速度越慢。系统吞吐能力越低，反之越高。</p><ul><li><p>QPS: 每秒钟处理完请求的次数；注意这里是处理完。具体是指发出请求到服务器处理完成功返回结果。可以理解在server中有个counter，每处理一个请求加1，1秒后counter=QPS。</p></li><li><p>TPS：每秒钟处理完的事务次数，一般TPS是对整个系统来讲的。一个应用系统1s能完成多少事务处理，一个事务在分布式处理中，可能会对应多个请求，对于衡量单个接口服务的处理能力，用QPS比较多。</p></li><li><p>并发量：系统能同时处理的请求数</p></li><li><p>RT：响应时间，处理一次请求所需要的平均处理时间</p></li></ul><p>计算关系：</p><ul><li><p>QPS = 并发量 / 平均响应时间</p></li><li><p>并发量 = QPS * 平均响应时间</p></li></ul><p>一个典型的上班签到系统，早上8点上班。7点半到8点这30分钟的时间里用户会登录签到系统进行签到。公司员工为1000人，平均每一个员上登录签到系统的时长为5分钟。能够用以下的方法计算：</p><p>QPS = 1000/(30<em>60) 事务/秒<br>平均响应时间为 = 5 </em>60 秒<br>并发数= QPS<em>平均响应时间 = 1000/(30 </em>60) <em>(5 </em>60)=166.7</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Wed Nov 21 2018 15:08:20 GMT+0800 (China Standard Time) --&gt;&lt;h3 id=&quot;系统吞吐量的关键参数&quot;&gt;&lt;a href=&quot;#系统吞吐量的关键参数&quot; class=&quot;headerlink&quot; titl
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Java常用集合源码分析之map遍历效率比较（六）</title>
    <link href="http://www.fufan.me/2018/06/19/Java%E5%B8%B8%E7%94%A8%E9%9B%86%E5%90%88%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8Bmap%E9%81%8D%E5%8E%86%E6%95%88%E7%8E%87%E6%AF%94%E8%BE%83%EF%BC%88%E5%85%AD%EF%BC%89/"/>
    <id>http://www.fufan.me/2018/06/19/Java常用集合源码分析之map遍历效率比较（六）/</id>
    <published>2018-06-19T14:21:00.000Z</published>
    <updated>2018-11-11T14:21:41.799Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Nov 21 2018 15:08:20 GMT+0800 (China Standard Time) --><h3 id="遍历方式"><a href="#遍历方式" class="headerlink" title="遍历方式"></a>遍历方式</h3><h4 id="1-遍历key"><a href="#1-遍历key" class="headerlink" title="1. 遍历key"></a>1. 遍历key</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">for (String key : map.keySet()) &#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="2-遍历value"><a href="#2-遍历value" class="headerlink" title="2. 遍历value"></a>2. 遍历value</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">for (String value : map.values()) &#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="3-遍历key-value"><a href="#3-遍历key-value" class="headerlink" title="3. 遍历key+value"></a>3. 遍历key+value</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">for (Entry&lt;String, String&gt; entry: map.entrySet()) &#123;</span><br><span class="line"></span><br><span class="line">    key = entry.getKey();</span><br><span class="line"></span><br><span class="line">    value = entry.getValue();</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="如果你使用HashMap"><a href="#如果你使用HashMap" class="headerlink" title="如果你使用HashMap"></a>如果你使用HashMap</h3><ol><li>同时遍历key和value时，keySet与entrySet方法的性能差异取决于key的复杂度，总体来说还是推荐使用entrySet。换言之，取决于HashMap查找value的开销。entrySet一次性取出所有key和value的操作是有性能开销的，当这个损失小于HashMap查找value的开销时，entrySet的性能优势就会体现出来。例如上述对比测试中，当key是最简单的数值字符串时，keySet可能反而会更高效，耗时比entrySet少10%。总体来说还是推荐使用entrySet。因为当key很简单时，其性能或许会略低于keySet，但却是可控的；而随着key的复杂化，entrySet的优势将会明显体现出来。当然，我们可以根据实际情况进行选择</li><li>只遍历key时，keySet方法更为合适，因为entrySet将无用的value也给取出来了，浪费了性能和空间。在上述测试结果中，keySet比entrySet方法耗时少23%。</li><li>只遍历value时，使用vlaues方法是最佳选择，entrySet会略好于keySet方法。</li></ol><h3 id="如果你使用TreeMap"><a href="#如果你使用TreeMap" class="headerlink" title="如果你使用TreeMap"></a>如果你使用TreeMap</h3><ol><li>同时遍历key和value时，entrySet的性能远远高于keySet。这是由TreeMap的查询效率决定的，也就是说，TreeMap查找value的开销较大，明显高于entrySet一次性取出所有key和value的开销。因此，遍历TreeMap时强烈推荐使用entrySet方法。</li><li>只遍历key时，keySet方法更为合适，因为entrySet将无用的value也给取出来了，浪费了性能和空间。在上述测试结果中，keySet比entrySet方法耗时少24%。</li><li>只遍历value时，使用vlaues方法是最佳选择，entrySet也明显优于keySet方法。</li></ol><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Wed Nov 21 2018 15:08:20 GMT+0800 (China Standard Time) --&gt;&lt;h3 id=&quot;遍历方式&quot;&gt;&lt;a href=&quot;#遍历方式&quot; class=&quot;headerlink&quot; title=&quot;遍历方式&quot;&gt;&lt;/a
      
    
    </summary>
    
    
      <category term="集合" scheme="http://www.fufan.me/tags/%E9%9B%86%E5%90%88/"/>
    
  </entry>
  
  <entry>
    <title>JVM专题（四）—— 图解垃圾回收</title>
    <link href="http://www.fufan.me/2018/06/09/JVM%E4%B8%93%E9%A2%98%EF%BC%88%E5%9B%9B%EF%BC%89%E2%80%94%E2%80%94-%E5%9B%BE%E8%A7%A3%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/"/>
    <id>http://www.fufan.me/2018/06/09/JVM专题（四）——-图解垃圾回收/</id>
    <published>2018-06-09T09:00:00.000Z</published>
    <updated>2018-11-09T09:01:08.471Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Nov 21 2018 15:08:20 GMT+0800 (China Standard Time) --><p>对于调优之前，我们必须要了解其运行原理，java 的垃圾收集Garbage Collection 通常被称为“GC”，它诞生于1960年 MIT 的 Lisp 语言，经过半个多世纪，目前已经十分成熟了。因此本篇主要从这三个方面来了解:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1. 哪些对象需要被回收？</span><br><span class="line"></span><br><span class="line">2. 什么时候回收？</span><br><span class="line"></span><br><span class="line">3. 如何回收？</span><br></pre></td></tr></table></figure><h3 id="一、谁要被回收"><a href="#一、谁要被回收" class="headerlink" title="一、谁要被回收"></a>一、谁要被回收</h3><p>java虚拟机在执行java程序的过程中会把它所管理的内存划分为若干个不同是数据区域，这些区域有各自各自的用途。主要包含以下几个部分组成：<br><img src="/image/jvm-4-1.png" alt=""><br><img src="/image/jvm-4-0.png" alt=""></p><h4 id="1、程序计数器"><a href="#1、程序计数器" class="headerlink" title="1、程序计数器"></a>1、程序计数器</h4><p>程序计数器占用的内存空间我们可以忽略不计，它是每个线程所执行的字节码的行号指示器。</p><h4 id="2、虚拟机栈"><a href="#2、虚拟机栈" class="headerlink" title="2、虚拟机栈"></a>2、虚拟机栈</h4><p>java的虚拟机栈是线程私有的，生命周期和线程相同。它描述的是方法执行的内存模型。同时用于存储局部变量、操作数栈、动态链接、方法出口等。</p><h4 id="3、本地方法栈"><a href="#3、本地方法栈" class="headerlink" title="3、本地方法栈"></a>3、本地方法栈</h4><p>本地方法栈，类似虚拟机栈，它调用的是是native方法。</p><h4 id="4、堆"><a href="#4、堆" class="headerlink" title="4、堆"></a>4、堆</h4><p>堆是jvm中管理内存中最大一块。它是被共享，存放对象实例。也被称为“gc堆”。垃圾回收的主要管理区域</p><h4 id="5、方法区"><a href="#5、方法区" class="headerlink" title="5、方法区"></a>5、方法区</h4><p>方法区也是共享的内存区域。它主要存储已被虚拟机加载的类信息、常量、静态变量、即时编译器（jit）编译后的代码数据。</p><p>以上就是jvm在运行时期主要的内存组成，我们看到常见的内存使用不但存在于堆中，还会存在于其他区域，虽然堆的管理对程序的管理至关重要，但我们不能只局限于这一个区域，特别是当出现内存泄露的时候，我们除了要排查堆内存的情况，还得考虑虚拟机栈的以及方法区域的情况。</p><p>知道了要对谁以及那些区域进行内存管理，我还需要知道什么时候对这些区域进行垃圾回收。</p><h3 id="二、什么时候回收"><a href="#二、什么时候回收" class="headerlink" title="二、什么时候回收"></a>二、什么时候回收</h3><p>在垃圾回收之前，我们必须确定的一件事就是对象是否存活？这就牵扯到了判断对象是否存活的算法了。</p><h4 id="引用计数算法："><a href="#引用计数算法：" class="headerlink" title="引用计数算法："></a>引用计数算法：</h4><p>给对象中添加一个引用计数器，每当有一个地方引用它时，计数器+1，当引用失效，计数器-1.任何时刻计数器为0的对象就是不可能再被使用的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">给对象中添加一个引用计数器，每当有一个地方引用它时，计数器+1，当引用失效，计数器-1.任何时刻计数器为0的对象就是不可能再被使用的。</span><br></pre></td></tr></table></figure><h4 id="可达性分析算法："><a href="#可达性分析算法：" class="headerlink" title="可达性分析算法："></a>可达性分析算法：</h4><p>通过一系列称为“GC Roots”的对象作为起始点，从这些节点开始向下搜索，搜索所走过的路径称为引用链，当一个对象到GCRoots没有任何引用链相连的时候，则证明此对象是不可用的。</p><p>比如如下，右侧的对象是到GCRoot时不可达的，可以判定为可回收对象。</p><p><img src="/image/jvm-4-2.png" alt=""></p><p>在java中，可以作为GCRoot的对象包括以下几种：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">* 虚拟机栈中引用的对象。</span><br><span class="line"></span><br><span class="line">* 方法区中静态属性引用的对象。</span><br><span class="line"></span><br><span class="line">* 方法区中常量引用的对象。</span><br><span class="line"></span><br><span class="line">* 本地方法中JNI引用的对象。</span><br></pre></td></tr></table></figure><p>基于以上，我们可以知道，当当前对象到GCRoot中不可达时候，即会满足被垃圾回收的可能。</p><p>那么是不是这些对象就非死不可，也不一定，此时只能宣判它们存在于一种“缓刑”的阶段，要真正的宣告一个对象死亡。至少要经历两次标记：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">第一次：对象可达性分析之后，发现没有与GCRoots相连接，此时会被第一次标记并筛选。</span><br><span class="line"></span><br><span class="line">第二次：对象没有覆盖finalize（）方法，或者finalize（）方法已经被虚拟机调用过，此时会被认定为没必要执行。</span><br><span class="line">    (大致描述一下finalize流程：当对象变成(GC Roots)不可达时，GC会判断该对象是否覆盖了finalize方法，若未覆盖，则直接将其回收。否则，若对象未执行过finalize方法，将其放入F-Queue队列，由一低优先级线程执行该队列中对象的finalize方法。执行finalize方法完毕后，GC会再次判断该对象是否可达，若不可达，则进行回收，否则，对象“复活”。)</span><br></pre></td></tr></table></figure><h3 id="三、如何回收"><a href="#三、如何回收" class="headerlink" title="三、如何回收"></a>三、如何回收</h3><p>上述的两点讲解之后，我们大概明白了，哪些对象会被回收，以及回收的依据是什么，但回收的这个工作实现起来并不简单，首先它需要扫描所有的对象，鉴别谁能够被回收，其次在扫描期间需要 ”stop the world“ 对象能被冻结，不然你刚扫描，他的引用信息有变化，你就等于白做了。</p><h4 id="分代回收"><a href="#分代回收" class="headerlink" title="分代回收"></a>分代回收</h4><p>我们从一个object1来说明其在分代垃圾回收算法中的回收轨迹。</p><p>1、object1新建，出生于新生代的Eden区域。</p><p><img src="/image/jvm-4-3.png" alt=""><br>2、minor GC，object1 还存活，移动到Fromsuvivor空间，此时还在新生代。<br><img src="/image/jvm-4-4.png" alt=""></p><p>3、minor GC，object1 仍然存活，此时会通过复制算法，将object1移动到ToSuv区域，此时object1的年龄age+1。<br><img src="/image/jvm-4-5.png" alt=""></p><p>4、minor GC，object1 仍然存活，此时survivor中和object1同龄的对象并没有达到survivor的一半，所以此时通过复制算法，将fromSuv和Tosuv 区域进行互换，存活的对象被移动到了Tosuv。<br><img src="/image/jvm-4-6.png" alt=""></p><p>5、minor GC，object1 仍然存活，此时survivor中和object1同龄的对象已经达到survivor的一半以上（toSuv的区域已经满了），object1被移动到了老年代区域。<br><img src="/image/jvm-4-7.png" alt=""></p><p>6、object1存活一段时间后，发现此时object1不可达GcRoots，而且此时老年代空间比率已经超过了阈值,触发了majorGC（也可以认为是fullGC，但具体需要垃圾收集器来联系），此时object1被回收了。fullGC会触发 stop the world。<br><img src="/image/jvm-4-8.png" alt=""></p><p>在以上的新生代中，我们有提到对象的age，对象存活于survivor状态下，不会立即晋升为老生代对象，以避免给老生代造成过大的影响，它们必须要满足以下条件才可以晋升：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1、minor gc 之后，存活于survivor 区域的对象的age会+1，当超过（默认）15的时候，转移到老年代。</span><br><span class="line"></span><br><span class="line">2、动态对象，如果survivor空间中相同年龄所有的对象大小的综合和大于survivor空间的一半，年级大于或等于该年级的对象就可以直接进入老年代。</span><br></pre></td></tr></table></figure><p>以上采用分代垃圾收集的思想，对一个对象从存活到死亡所经历的历程。期间，在新生代的时刻，会用到复制算法，在老年代时，有可能会用到标记-清楚算法（mark-sweep）算法或者标记-整理算法，这些都是垃圾回收算法基于不同区域的实现，我们看下这几种回收算法的实现原理。</p><h3 id="四、垃圾收集器"><a href="#四、垃圾收集器" class="headerlink" title="四、垃圾收集器"></a>四、垃圾收集器</h3><p>垃圾收集器是内存回收的具体实现，不同的厂商提供的垃圾收集器有很大的差别，一般的垃圾收集器都会作用于不同的分代，需要搭配使用。以下是各种垃圾收集器的组合方式：</p><p><img src="/image/jvm-4-9.png" alt=""></p><p><img src="/image/jvm-4-10.png" alt=""></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Wed Nov 21 2018 15:08:20 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;对于调优之前，我们必须要了解其运行原理，java 的垃圾收集Garbage Collection 通常被称为“GC”，它诞生
      
    
    </summary>
    
    
      <category term="jvm" scheme="http://www.fufan.me/tags/jvm/"/>
    
  </entry>
  
  <entry>
    <title>java常用集合源码分析之ArrayList遍历方式以及效率比较（五）</title>
    <link href="http://www.fufan.me/2018/06/08/java%E5%B8%B8%E7%94%A8%E9%9B%86%E5%90%88%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8BArrayList%E9%81%8D%E5%8E%86%E6%96%B9%E5%BC%8F%E4%BB%A5%E5%8F%8A%E6%95%88%E7%8E%87%E6%AF%94%E8%BE%83%EF%BC%88%E4%BA%94%EF%BC%89/"/>
    <id>http://www.fufan.me/2018/06/08/java常用集合源码分析之ArrayList遍历方式以及效率比较（五）/</id>
    <published>2018-06-08T13:51:00.000Z</published>
    <updated>2018-11-11T13:53:57.516Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Nov 21 2018 15:08:20 GMT+0800 (China Standard Time) --><h3 id="一、遍历方式"><a href="#一、遍历方式" class="headerlink" title="一、遍历方式"></a>一、遍历方式</h3><p>ArrayList支持三种遍历方式。</p><p>1、第一种，随机访问，它是通过索引值去遍历</p><p>2、第二种，foreach语句</p><p>3、第三种，Iterator迭代器方式</p><p>迭代器是一种模式，它可以使得对于序列类型的数据结构的遍历行为与被遍历的对象分离，即我们无需关心该序列的底层结构是什么样子的。只要拿到这个对象,使用迭代器就可以遍历这个对象的内部。</p><h3 id="二、几种遍历方式效率的比较"><a href="#二、几种遍历方式效率的比较" class="headerlink" title="二、几种遍历方式效率的比较"></a>二、几种遍历方式效率的比较</h3><p>从实验结果来看，在遍历ArrayList中，效率最高的是普通for循环遍历，foreach遍历和Iterator方式之间关系不明确，但在增大运行次数时，iterator效率高于foreach。</p><h3 id="三、效率分析"><a href="#三、效率分析" class="headerlink" title="三、效率分析"></a>三、效率分析</h3><h4 id="1-为什么基本的for循环效率高于Iterator遍历？"><a href="#1-为什么基本的for循环效率高于Iterator遍历？" class="headerlink" title="1. 为什么基本的for循环效率高于Iterator遍历？"></a>1. 为什么基本的for循环效率高于Iterator遍历？</h4><p>ArrayList实现了RandomAccess接口，RandomAccess接口为ArrayList带来了什么好处呢？</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">As a rule of thumb, a List implementation should implement this interface if, for typical instances of the class, this loop:</span><br><span class="line"></span><br><span class="line">     for (int i=0, n=list.size(); i &lt; n; i++)</span><br><span class="line">         list.get(i);</span><br><span class="line"> </span><br><span class="line">runs faster than this loop:</span><br><span class="line"></span><br><span class="line">     for (Iterator i=list.iterator(); i.hasNext(); )</span><br><span class="line">         i.next();</span><br></pre></td></tr></table></figure><p>从描述中，可以看出实现RandomAccess接口的集合类，使用for循环的效率会比Iterator高。<br>RandomAccess接口为ArrayList带来的好处：</p><ul><li>1、可以快速随机访问集合。</li><li>2、使用快速随机访问（for循环）效率可以高于Iterator。</li></ul><h4 id="2-为什么foreach循环效率与Iterator效率有点相似"><a href="#2-为什么foreach循环效率与Iterator效率有点相似" class="headerlink" title="2. 为什么foreach循环效率与Iterator效率有点相似"></a>2. 为什么foreach循环效率与Iterator效率有点相似</h4><p>从底层实现中可以看出：</p><ul><li>foreach不是关键字，它的关键字是for，它的语句是由iterator实现的。</li><li>forEach就是为了让用iterator循环访问的形式简单，写起来更方便。</li></ul><h3 id="四、扩展"><a href="#四、扩展" class="headerlink" title="四、扩展"></a>四、扩展</h3><h4 id="1、基本的for循环的效率一定比iterator迭代器的高吗？"><a href="#1、基本的for循环的效率一定比iterator迭代器的高吗？" class="headerlink" title="1、基本的for循环的效率一定比iterator迭代器的高吗？"></a>1、基本的for循环的效率一定比iterator迭代器的高吗？</h4><p>不一定，主要还要看集合的数据结构组成。例如，ArrayList和LinkedList中就不同</p><ul><li>ArrayList实现了RandomAccess随机访问接口，因此它对随机访问的速度快，而基本的for循环中的get()方法，采用的即是随机访问的方法，因而在ArrayList中，for循环速度快。</li><li>LinkedList采取的是顺序访问方式，iterator中的next()方法，采用的即是顺序访问方法，因此在LinkedList中，使用iterator的速度较快。</li></ul><h4 id="2、for、foreach、iterator之间的差别"><a href="#2、for、foreach、iterator之间的差别" class="headerlink" title="2、for、foreach、iterator之间的差别"></a>2、for、foreach、iterator之间的差别</h4><h5 id="1）形式差别"><a href="#1）形式差别" class="headerlink" title="1）形式差别"></a>1）形式差别</h5><h5 id="2）条件差别"><a href="#2）条件差别" class="headerlink" title="2）条件差别"></a>2）条件差别</h5><ul><li>for：需要知道集合或数组的大小，而且需要是有序的，不然无法遍历；</li><li>foreach、iterator：都不需要知道集合或数组的大小，他们都是得到集合内的每个元素然后进行处理。</li></ul><h5 id="3）多态差别"><a href="#3）多态差别" class="headerlink" title="3）多态差别"></a>3）多态差别</h5><ul><li>for、foreach：都需要先知道集合的类型，甚至是集合内元素的类型，即需要访问内部的成员，不能实现态；</li><li>iterator：是一个接口类型，它不关心集合或者数组的类型，而且它还能随时修改和删除集合的元素。</li></ul><h5 id="4）用法差别"><a href="#4）用法差别" class="headerlink" title="4）用法差别"></a>4）用法差别</h5><ul><li>for循环：一般用来处理比较简单的有序的，可预知大小的集合或数组</li><li>foreach：可用于遍历任何集合或数组，而且操作简单易懂，他唯一的不好就是需要了解集合内部类型</li><li>iterator：是最强大的，它可以随时修改或者删除集合内部的元素，并且是在不需要知道元素和集合的类型的情况下进行的（原因可参考第三点：多态差别），当你需要对不同的容器实现同样的遍历方式时，迭代器是最好的选择！</li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Wed Nov 21 2018 15:08:20 GMT+0800 (China Standard Time) --&gt;&lt;h3 id=&quot;一、遍历方式&quot;&gt;&lt;a href=&quot;#一、遍历方式&quot; class=&quot;headerlink&quot; title=&quot;一、遍历方
      
    
    </summary>
    
      <category term="集合" scheme="http://www.fufan.me/categories/%E9%9B%86%E5%90%88/"/>
    
    
      <category term="集合" scheme="http://www.fufan.me/tags/%E9%9B%86%E5%90%88/"/>
    
  </entry>
  
  <entry>
    <title>Java8语法特性讲解（三）—— Examples：Strings, Numbers, Math and Files</title>
    <link href="http://www.fufan.me/2018/05/12/Java8%E8%AF%AD%E6%B3%95%E7%89%B9%E6%80%A7%E8%AE%B2%E8%A7%A3%EF%BC%88%E4%B8%89%EF%BC%89%E2%80%94%E2%80%94-Examples%EF%BC%9AStrings-Numbers-Math-and-Files-1/"/>
    <id>http://www.fufan.me/2018/05/12/Java8语法特性讲解（三）——-Examples：Strings-Numbers-Math-and-Files-1/</id>
    <published>2018-05-11T17:21:00.000Z</published>
    <updated>2018-11-11T17:21:52.570Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Nov 21 2018 15:08:21 GMT+0800 (China Standard Time) --><h3 id="Slicing-Strings"><a href="#Slicing-Strings" class="headerlink" title="Slicing Strings"></a>Slicing Strings</h3><p>Two new methods are available on the String class: join and chars. The first method joins any number of strings into a single string with the given delimiter:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">String.join(&quot;:&quot;, &quot;foobar&quot;, &quot;foo&quot;, &quot;bar&quot;);</span><br><span class="line">// =&gt; foobar:foo:bar</span><br></pre></td></tr></table></figure><p>The second method chars creates a stream for all characters of the string, so you can use stream operations upon those characters:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&quot;foobar:foo:bar&quot;</span><br><span class="line">    .chars()</span><br><span class="line">    .distinct()</span><br><span class="line">    .mapToObj(c -&gt; String.valueOf((char)c))</span><br><span class="line">    .sorted()</span><br><span class="line">    .collect(Collectors.joining());</span><br><span class="line">// =&gt; :abfor</span><br></pre></td></tr></table></figure><p>Not only strings but also regex patterns now benefit from streams. Instead of splitting strings into streams for each character we can split strings for any pattern and create a stream to work upon as shown in this example:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Pattern.compile(&quot;:&quot;)</span><br><span class="line">    .splitAsStream(&quot;foobar:foo:bar&quot;)</span><br><span class="line">    .filter(s -&gt; s.contains(&quot;bar&quot;))</span><br><span class="line">    .sorted()</span><br><span class="line">    .collect(Collectors.joining(&quot;:&quot;));</span><br><span class="line">// =&gt; bar:foobar</span><br></pre></td></tr></table></figure><p>Additionally regex patterns can be converted into predicates. Those predicates can for example be used to filter a stream of strings:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Pattern pattern = Pattern.compile(&quot;.*@gmail\\.com&quot;);</span><br><span class="line">Stream.of(&quot;bob@gmail.com&quot;, &quot;alice@hotmail.com&quot;)</span><br><span class="line">    .filter(pattern.asPredicate())</span><br><span class="line">    .count();</span><br><span class="line">// =&gt; 1</span><br></pre></td></tr></table></figure><p>The above pattern accepts any string which ends with @gmail.com and is then used as a Java 8 Predicate to filter a stream of email addresses.</p><h3 id="Crunching-Numbers"><a href="#Crunching-Numbers" class="headerlink" title="Crunching Numbers"></a>Crunching Numbers</h3><p>Java 8 adds additional support for working with unsigned numbers. Numbers in Java had always been signed. Let’s look at Integer for example:</p><p>An int represents a maximum of 2³² binary digits. Numbers in Java are per default signed, so the last binary digit represents the sign (0 = positive, 1 = negative). Thus the maximum positive signed int is 2³¹ - 1 starting with the decimal zero.</p><p>You can access this value via Integer.MAX_VALUE:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">System.out.println(Integer.MAX_VALUE);      // 2147483647</span><br><span class="line">System.out.println(Integer.MAX_VALUE + 1);  // -2147483648</span><br></pre></td></tr></table></figure><p>Java 8 adds support for parsing unsigned ints. Let’s see how this works:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">long maxUnsignedInt = (1l &lt;&lt; 32) - 1;</span><br><span class="line">String string = String.valueOf(maxUnsignedInt);</span><br><span class="line">int unsignedInt = Integer.parseUnsignedInt(string, 10);</span><br><span class="line">String string2 = Integer.toUnsignedString(unsignedInt, 10);</span><br></pre></td></tr></table></figure><p>As you can see it’s now possible to parse the maximum possible unsigned number 2³² - 1 into an integer. And you can also convert this number back into a string representing the unsigned number.</p><p>This wasn’t possible before with parseInt as this example demonstrates:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">try &#123;</span><br><span class="line">    Integer.parseInt(string, 10);</span><br><span class="line">&#125;</span><br><span class="line">catch (NumberFormatException e) &#123;</span><br><span class="line">    System.err.println(&quot;could not parse signed int of &quot; + maxUnsignedInt);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The number is not parseable as a signed int because it exceeds the maximum of 2³¹ - 1.</p><h3 id="Do-the-Math"><a href="#Do-the-Math" class="headerlink" title="Do the Math"></a>Do the Math</h3><p>The utility class Math has been enhanced by a couple of new methods for handling number overflows. What does that mean? We’ve already seen that all number types have a maximum value. So what happens when the result of an arithmetic operation doesn’t fit into its size?</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">System.out.println(Integer.MAX_VALUE);      // 2147483647</span><br><span class="line">System.out.println(Integer.MAX_VALUE + 1);  // -2147483648</span><br></pre></td></tr></table></figure><p>As you can see a so called integer overflow happens which is normally not the desired behavior.</p><p>Java 8 adds support for strict math to handle this problem. Math has been extended by a couple of methods who all ends with exact, e.g. addExact. Those methods handle overflows properly by throwing an ArithmeticException when the result of the operation doesn’t fit into the number type:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">try &#123;</span><br><span class="line">    Math.addExact(Integer.MAX_VALUE, 1);</span><br><span class="line">&#125;</span><br><span class="line">catch (ArithmeticException e) &#123;</span><br><span class="line">    System.err.println(e.getMessage());</span><br><span class="line">    // =&gt; integer overflow</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The same exception might be thrown when trying to convert longs to int via toIntExact:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">try &#123;</span><br><span class="line">    Math.toIntExact(Long.MAX_VALUE);</span><br><span class="line">&#125;</span><br><span class="line">catch (ArithmeticException e) &#123;</span><br><span class="line">    System.err.println(e.getMessage());</span><br><span class="line">    // =&gt; integer overflow</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Working-with-Files"><a href="#Working-with-Files" class="headerlink" title="Working with Files"></a>Working with Files</h3><p>The utility class Files was first introduced in Java 7 as part of Java NIO. The JDK 8 API adds a couple of additional methods which enables us to use functional streams with files. Let’s deep-dive into a couple of code samples.</p><h4 id="Listing-files"><a href="#Listing-files" class="headerlink" title="Listing files"></a>Listing files</h4><p>The method Files.list streams all paths for a given directory, so we can use stream operations like filter and sorted upon the contents of the file system.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">try (Stream&lt;Path&gt; stream = Files.list(Paths.get(&quot;&quot;))) &#123;</span><br><span class="line">    String joined = stream</span><br><span class="line">        .map(String::valueOf)</span><br><span class="line">        .filter(path -&gt; !path.startsWith(&quot;.&quot;))</span><br><span class="line">        .sorted()</span><br><span class="line">        .collect(Collectors.joining(&quot;; &quot;));</span><br><span class="line">    System.out.println(&quot;List: &quot; + joined);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The above example lists all files for the current working directory, then maps each path to it’s string representation. The result is then filtered, sorted and finally joined into a string. If you’re not yet familiar with functional streams you should read my Java 8 Stream Tutorial.</p><p>You might have noticed that the creation of the stream is wrapped into a try/with statement. Streams implement AutoCloseable and in this case we really have to close the stream explicitly since it’s backed by IO operations.</p><pre><code>The returned stream encapsulates a DirectoryStream. If timely disposal of file system resources is required, the try-with-resources construct should be used to ensure that the stream&apos;s close method is invoked after the stream operations are completed.</code></pre><h4 id="Finding-files"><a href="#Finding-files" class="headerlink" title="Finding files"></a>Finding files</h4><p>The next example demonstrates how to find files in a directory or it’s sub-directories.</p><p>Path start = Paths.get(“”);<br>int maxDepth = 5;<br>try (Stream<path></path>stream = Files.find(start, maxDepth, (path, attr) -&gt;<br>String.valueOf(path).endsWith(“.js”))) {<br>String joined = stream<br>.sorted()<br>.map(String::valueOf)<br>.collect(Collectors.joining(“; “));<br>System.out.println(“Found: “ + joined);<br>}</p><p>The method find accepts three arguments: The directory path start is the initial starting point and maxDepth defines the maximum folder depth to be searched. The third argument is a matching predicate and defines the search logic. In the above example we search for all JavaScript files (filename ends with .js).</p><p>We can achieve the same behavior by utilizing the method Files.walk. Instead of passing a search predicate this method just walks over any file.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Path start = Paths.get(&quot;&quot;);</span><br><span class="line">int maxDepth = 5;</span><br><span class="line">try (Stream&lt;Path&gt; stream = Files.walk(start, maxDepth)) &#123;</span><br><span class="line">    String joined = stream</span><br><span class="line">        .map(String::valueOf)</span><br><span class="line">        .filter(path -&gt; path.endsWith(&quot;.js&quot;))</span><br><span class="line">        .sorted()</span><br><span class="line">        .collect(Collectors.joining(&quot;; &quot;));</span><br><span class="line">    System.out.println(&quot;walk(): &quot; + joined);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>In this example we use the stream operation filter to achieve the same behavior as in the previous example.</p><h4 id="Reading-and-writing-files"><a href="#Reading-and-writing-files" class="headerlink" title="Reading and writing files"></a>Reading and writing files</h4><p>Reading text files into memory and writing strings into a text file in Java 8 is finally a simple task. No messing around with readers and writers. The method Files.readAllLines reads all lines of a given file into a list of strings. You can simply modify this list and write the lines into another file via Files.write:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">List&lt;String&gt; lines = Files.readAllLines(Paths.get(&quot;res/nashorn1.js&quot;));</span><br><span class="line">lines.add(&quot;print(&apos;foobar&apos;);&quot;);</span><br><span class="line">Files.write(Paths.get(&quot;res/nashorn1-modified.js&quot;), lines);</span><br></pre></td></tr></table></figure><p>Please keep in mind that those methods are not very memory-efficient because the whole file will be read into memory. The larger the file the more heap-size will be used.</p><p>As an memory-efficient alternative you could use the method Files.lines. Instead of reading all lines into memory at once, this method reads and streams each line one by one via functional streams.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">try (Stream&lt;String&gt; stream = Files.lines(Paths.get(&quot;res/nashorn1.js&quot;))) &#123;</span><br><span class="line">    stream</span><br><span class="line">        .filter(line -&gt; line.contains(&quot;print&quot;))</span><br><span class="line">        .map(String::trim)</span><br><span class="line">        .forEach(System.out::println);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>If you need more fine-grained control you can instead construct a new buffered reader:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Path path = Paths.get(&quot;res/nashorn1.js&quot;);</span><br><span class="line">try (BufferedReader reader = Files.newBufferedReader(path)) &#123;</span><br><span class="line">    System.out.println(reader.readLine());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Or in case you want to write to a file simply construct a buffered writer instead:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Path path = Paths.get(&quot;res/output.js&quot;);</span><br><span class="line">try (BufferedWriter writer = Files.newBufferedWriter(path)) &#123;</span><br><span class="line">    writer.write(&quot;print(&apos;Hello World&apos;);&quot;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Buffered readers also have access to functional streams. The method lines construct a functional stream upon all lines denoted by the buffered reader:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Path path = Paths.get(&quot;res/nashorn1.js&quot;);</span><br><span class="line">try (BufferedReader reader = Files.newBufferedReader(path)) &#123;</span><br><span class="line">    long countPrints = reader</span><br><span class="line">        .lines()</span><br><span class="line">        .filter(line -&gt; line.contains(&quot;print&quot;))</span><br><span class="line">        .count();</span><br><span class="line">    System.out.println(countPrints);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><a href="https://github.com/winterbe/java8-tutorial" target="_blank" rel="noopener">code in GITHUB</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Wed Nov 21 2018 15:08:21 GMT+0800 (China Standard Time) --&gt;&lt;h3 id=&quot;Slicing-Strings&quot;&gt;&lt;a href=&quot;#Slicing-Strings&quot; class=&quot;header
      
    
    </summary>
    
    
      <category term="java" scheme="http://www.fufan.me/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>分布式锁的实现方式（一）——数据库和Redis锁</title>
    <link href="http://www.fufan.me/2018/04/07/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E7%9A%84%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94%E6%95%B0%E6%8D%AE%E5%BA%93%E5%92%8CRedis%E9%94%81/"/>
    <id>http://www.fufan.me/2018/04/07/分布式锁的实现方式（一）——数据库和Redis锁/</id>
    <published>2018-04-07T02:29:00.000Z</published>
    <updated>2018-11-07T06:07:36.483Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Nov 21 2018 15:08:21 GMT+0800 (China Standard Time) --><p>分布式锁一般有三种实现方式：1. 数据库乐观锁；2. 基于Redis的分布式锁；3. 基于ZooKeeper的分布式锁。4.基于consul的分布式锁。</p><p>在之前的java多线程系列中（java多线程系列（三）——锁），我已经学习到了各种各样的锁在jdk中的使用。如今大部分互联网系统都是分布式系统，所以实现支持具体业务的高可用分布式锁是我们经常要做的事情。</p><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><h4 id="什么是锁"><a href="#什么是锁" class="headerlink" title="什么是锁"></a>什么是锁</h4><ul><li>在单进程的系统中，当存在多个线程可以同时改变某个变量（可变共享变量）时，就需要对变量或代码块做同步，使其在修改这种变量时能够线性执行消除并发修改变量。</li><li>而同步的本质是通过锁来实现的。为了实现多个线程在一个时刻同一个代码块只能有一个线程可执行，那么需要在某个地方做个标记，这个标记必须每个线程都能看到，当标记不存在时可以设置该标记，其余后续线程发现已经有标记了则等待拥有标记的线程结束同步代码块取消标记后再去尝试设置标记。这个标记可以理解为锁。</li><li>不同地方实现锁的方式也不一样，只要能满足所有线程都能看得到标记即可。如 Java 中 synchronize 是在对象头设置标记，Lock 接口的实现类基本上都只是某一个 volitile 修饰的 int 型变量其保证每个线程都能拥有对该 int 的可见性和原子修改，linux 内核中也是利用互斥量或信号量等内存数据做标记。</li><li>除了利用内存数据做锁其实任何互斥的都能做锁（只考虑互斥情况），如流水表中流水号与时间结合做幂等校验可以看作是一个不会释放的锁，或者使用某个文件是否存在作为锁等。只需要满足在对标记进行修改能保证原子性和内存可见性即可。</li></ul><h4 id="什么是分布式？"><a href="#什么是分布式？" class="headerlink" title="什么是分布式？"></a>什么是分布式？</h4><p>分布式的 CAP 理论告诉我们:<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">任何一个分布式系统都无法同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance），最多只能同时满足两项。</span><br></pre></td></tr></table></figure><p></p><p>目前很多大型网站及应用都是分布式部署的，分布式场景中的数据一致性问题一直是一个比较重要的话题。基于 CAP理论，很多系统在设计之初就要对这三者做出取舍。在互联网领域的绝大多数的场景中，都需要牺牲强一致性来换取系统的高可用性，系统往往只需要保证最终一致性。</p><p>在许多的场景中，我们为了保证数据的最终一致性，需要很多的技术方案来支持，比如分布式事务、分布式锁等。很多时候我们需要保证一个方法在同一时间内只能被同一个线程执行。在单机环境中，通过 Java 提供的并发 API 我们可以解决，但是在分布式环境下，就没有那么简单啦。</p><ul><li>分布式与单机情况下最大的不同在于其不是多线程而是多进程。</li><li>多线程由于可以共享堆内存，因此可以简单的采取内存作为标记存储位置。而进程之间甚至可能都不在同一台物理机上，因此需要将标记存储在一个所有进程都能看到的地方。</li></ul><h4 id="什么是分布式锁？"><a href="#什么是分布式锁？" class="headerlink" title="什么是分布式锁？"></a>什么是分布式锁？</h4><ul><li>当在分布式模型下，数据只有一份（或有限制），此时需要利用锁的技术控制某一时刻修改数据的进程数。</li><li>与单机模式下的锁不仅需要保证进程可见，还需要考虑进程与锁之间的网络问题。（我觉得分布式情况下之所以问题变得复杂，主要就是需要考虑到网络的延时和不可靠。。。一个大坑）</li><li>分布式锁还是可以将标记存在内存，只是该内存不是某个进程分配的内存而是公共内存如 Redis、Memcache。至于利用数据库、文件等做锁与单机的实现是一样的，只要保证标记能互斥就行。</li></ul><h4 id="我们需要怎样的分布式锁？"><a href="#我们需要怎样的分布式锁？" class="headerlink" title="我们需要怎样的分布式锁？"></a>我们需要怎样的分布式锁？</h4><ul><li>可以保证在分布式部署的应用集群中，同一个方法在同一时间只能被一台机器-上的一个线程执行。</li><li>这把锁要是一把可重入锁（避免死锁）</li><li>这把锁最好是一把阻塞锁（根据业务需求考虑要不要这条）</li><li>这把锁最好是一把公平锁（根据业务需求考虑要不要这条）</li><li>有高可用的获取锁和释放锁功能</li><li>获取锁和释放锁的性能要好</li></ul><h3 id="基于数据库做分布式锁"><a href="#基于数据库做分布式锁" class="headerlink" title="基于数据库做分布式锁"></a>基于数据库做分布式锁</h3><h4 id="乐观锁"><a href="#乐观锁" class="headerlink" title="乐观锁"></a>乐观锁</h4><p>基于表主键唯一做分布式锁</p><p><strong><em>思路：</em></strong> 利用主键唯一的特性，如果有多个请求同时提交到数据库的话，数据库会保证只有一个操作可以成功，那么我们就可以认为操作成功的那个线程获得了该方法的锁，当方法执行完毕之后，想要释放锁的话，删除这条数据库记录即可。</p><p>上面这种简单的实现有以下几个问题：</p><ul><li>这把锁强依赖数据库的可用性，数据库是一个单点，一旦数据库挂掉，会导致业务系统不可用。</li><li>这把锁没有失效时间，一旦解锁操作失败，就会导致锁记录一直在数据库中，其他线程无法再获得到锁。</li><li>这把锁只能是非阻塞的，因为数据的 insert操作，一旦插入失败就会直接报错。没有获得锁的线程并不会进入排队队列，要想再次获得锁就要再次触发获得锁操作。</li><li>这把锁是非重入的，同一个线程在没有释放锁之前无法再次获得该锁。因为数据中数据已经存在了。</li><li>这把锁是非公平锁，所有等待锁的线程凭运气去争夺锁。</li><li>在 MySQL 数据库中采用主键冲突防重，在大并发情况下有可能会造成锁表现象。</li></ul><p>当然，我们也可以有其他方式解决上面的问题。</p><ul><li>数据库是单点？搞两个数据库，数据之前双向同步，一旦挂掉快速切换到备库上。</li><li>没有失效时间？只要做一个定时任务，每隔一定时间把数据库中的超时数据清理一遍。</li><li>非阻塞的？搞一个 while 循环，直到 insert 成功再返回成功。</li><li>非重入的？在数据库表中加个字段，记录当前获得锁的机器的主机信息和线程信息，那么下次再获取锁的时候先查询数据库，如果当前机器的主机信息和线程信息在数据库可以查到的话，直接把锁分配给他就可以了。</li><li>非公平的？再建一张中间表，将等待锁的线程全记录下来，并根据创建时间排序，只有最先创建的允许获取锁。</li><li>比较好的办法是在程序中生产主键进行防重。</li></ul><h4 id="悲观锁"><a href="#悲观锁" class="headerlink" title="悲观锁"></a>悲观锁</h4><p>基于表字段版本号做分布式锁</p><p>这个策略源于 mysql 的 mvcc 机制，使用这个策略其实本身没有什么问题，唯一的问题就是对数据表侵入较大，我们要为每个表设计一个版本号字段，然后写一条判断 sql 每次进行判断，增加了数据库操作的次数，在高并发的要求下，对数据库连接的开销也是无法忍受的。</p><h4 id="基于数据库排他锁做分布式锁"><a href="#基于数据库排他锁做分布式锁" class="headerlink" title="基于数据库排他锁做分布式锁"></a>基于数据库排他锁做分布式锁</h4><p>在查询语句后面增加for update，数据库会在查询过程中给数据库表增加排他锁 (注意： InnoDB 引擎在加锁的时候，只有通过索引进行检索的时候才会使用行级锁，否则会使用表级锁。这里我们希望使用行级锁，就要给要执行的方法字段名添加索引，值得注意的是，这个索引一定要创建成唯一索引，否则会出现多个重载方法之间无法同时被访问的问题。重载方法的话建议把参数类型也加上。)。当某条记录被加上排他锁之后，其他线程无法再在该行记录上增加排他锁。</p><p>我们可以认为获得排他锁的线程即可获得分布式锁，当获取到锁之后，可以执行方法的业务逻辑，执行完方法之后，通过connection.commit()操作来释放锁。</p><p>这种方法可以有效的解决上面提到的无法释放锁和阻塞锁的问题。</p><ul><li>阻塞锁？ for update语句会在执行成功后立即返回，在执行失败时一直处于阻塞状态，直到成功。</li><li>锁定之后服务宕机，无法释放？使用这种方式，服务宕机之后数据库会自己把锁释放掉。</li></ul><p>但是还是无法直接解决数据库单点和可重入问题。</p><p>这里还可能存在另外一个问题，虽然我们对方法字段名使用了唯一索引，并且显示使用 for update 来使用行级锁。但是，MySQL 会对查询进行优化，即便在条件中使用了索引字段，但是否使用索引来检索数据是由 MySQL 通过判断不同执行计划的代价来决定的，如果 MySQL 认为全表扫效率更高，比如对一些很小的表，它就不会使用索引，这种情况下 InnoDB 将使用表锁，而不是行锁。如果发生这种情况就悲剧了。。。</p><p>还有一个问题，就是我们要使用排他锁来进行分布式锁的 lock，那么一个排他锁长时间不提交，就会占用数据库连接。一旦类似的连接变得多了，就可能把数据库连接池撑爆。</p><h4 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h4><ul><li><p>优点：简单，易于理解</p></li><li><p>缺点：会有各种各样的问题（操作数据库需要一定的开销，使用数据库的行级锁并不一定靠谱，性能不靠谱）</p></li></ul><h3 id="基于-Redis-做分布式锁"><a href="#基于-Redis-做分布式锁" class="headerlink" title="基于 Redis 做分布式锁"></a>基于 Redis 做分布式锁</h3><h4 id="使用redis的setNX命令实现分布式锁"><a href="#使用redis的setNX命令实现分布式锁" class="headerlink" title="使用redis的setNX命令实现分布式锁　　"></a>使用redis的setNX命令实现分布式锁</h4><h5 id="实现的原理"><a href="#实现的原理" class="headerlink" title="实现的原理"></a>实现的原理</h5><p>Redis为单进程单线程模式，采用队列模式将并发访问变成串行访问，且多客户端对Redis的连接并不存在竞争关系。redis的SETNX命令可以方便的实现分布式锁。</p><h5 id="基本命令解析"><a href="#基本命令解析" class="headerlink" title="基本命令解析"></a>基本命令解析</h5><p>1）setNX（SET if Not eXists）</p><p>语法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SETNX key value</span><br></pre></td></tr></table></figure><p>将 key 的值设为 value ，当且仅当 key 不存在。</p><p>若给定的 key 已经存在，则 SETNX 不做任何动作。</p><p>SETNX 是『SET if Not eXists』(如果不存在，则 SET)的简写</p><p>返回值：<br>设置成功，返回 1 。<br>设置失败，返回 0 。</p><p>所以我们使用执行下面的命令<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SETNX lock.foo &lt;current Unix time + lock timeout + 1&gt;</span><br></pre></td></tr></table></figure><p></p><ul><li>如返回1，则该客户端获得锁，把lock.foo的键值设置为时间值表示该键已被锁定，该客户端最后可以通过DEL lock.foo来释放该锁。</li><li>如返回0，表明该锁已被其他客户端取得，这时我们可以先返回或进行重试等对方完成或等待锁超时。</li></ul><p>2）getSET</p><p>语法：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GETSET key value</span><br></pre></td></tr></table></figure><p></p><p>将给定 key 的值设为 value ，并返回 key 的旧值(old value)。</p><p>当 key 存在但不是字符串类型时，返回一个错误。</p><p>返回值：</p><p>返回给定 key 的旧值。<br>当 key 没有旧值时，也即是， key 不存在时，返回 nil 。</p><p>3）get<br>语法：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GET key</span><br></pre></td></tr></table></figure><p></p><p>返回值：</p><p>当 key 不存在时，返回 nil ，否则，返回 key 的值。<br>如果 key 不是字符串类型，那么返回一个错误</p><h5 id="解决死锁问题"><a href="#解决死锁问题" class="headerlink" title="解决死锁问题"></a>解决死锁问题</h5><p>如果一个持有锁的客户端失败或崩溃了不能释放锁，该怎么解决？</p><ul><li>设置超时时间来解决</li></ul><h2 id="参考博文"><a href="#参考博文" class="headerlink" title="参考博文"></a>参考博文</h2><ol><li><a href="http://www.cnblogs.com/seesun2012/p/9214653.html" target="_blank" rel="noopener">Java分布式锁看这篇就够了</a></li></ol><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Wed Nov 21 2018 15:08:21 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;分布式锁一般有三种实现方式：1. 数据库乐观锁；2. 基于Redis的分布式锁；3. 基于ZooKeeper的分布式锁。4.
      
    
    </summary>
    
      <category term="分布式" scheme="http://www.fufan.me/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
    
      <category term="分布式" scheme="http://www.fufan.me/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
      <category term="锁" scheme="http://www.fufan.me/tags/%E9%94%81/"/>
    
  </entry>
  
  <entry>
    <title>Kafka学习笔记（三）——kafka设计的要点</title>
    <link href="http://www.fufan.me/2018/04/02/Kafka%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89%E2%80%94%E2%80%94kafka%E8%AE%BE%E8%AE%A1%E7%9A%84%E8%A6%81%E7%82%B9/"/>
    <id>http://www.fufan.me/2018/04/02/Kafka学习笔记（三）——kafka设计的要点/</id>
    <published>2018-04-02T10:36:00.000Z</published>
    <updated>2018-11-08T10:37:39.328Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Nov 21 2018 15:08:21 GMT+0800 (China Standard Time) --><h2 id="kafka的特点"><a href="#kafka的特点" class="headerlink" title="kafka的特点"></a>kafka的特点</h2><h3 id="1、吞吐量"><a href="#1、吞吐量" class="headerlink" title="1、吞吐量"></a>1、吞吐量</h3><p>高吞吐是kafka需要实现的核心目标之一，为此kafka做了以下一些设计：</p><ol><li>数据磁盘持久化：消息不在内存中cache，直接写入到磁盘，充分利用磁盘的顺序读写性能</li><li>zero-copy：减少IO操作步骤</li><li>数据批量发送</li><li>数据压缩</li><li>Topic划分为多个partition，提高parallelism</li></ol><h3 id="2、负载均衡"><a href="#2、负载均衡" class="headerlink" title="2、负载均衡"></a>2、负载均衡</h3><ol><li>producer根据用户指定的算法，将消息发送到指定的partition</li><li>存在多个partiiton，每个partition有自己的replica，每个replica分布在不同的Broker节点上</li><li>多个partition需要选取出lead partition，lead partition负责读写，并由zookeeper负责fail over</li><li>通过zookeeper管理broker与consumer的动态加入与离开</li></ol><h3 id="3、拉取系统"><a href="#3、拉取系统" class="headerlink" title="3、拉取系统"></a>3、拉取系统</h3><p>由于kafka broker会持久化数据，broker没有内存压力，因此，consumer非常适合采取pull的方式消费数据，具有以下几点好处：</p><ol><li>简化kafka设计</li><li>consumer根据消费能力自主控制消息拉取速度</li><li>consumer根据自身情况自主选择消费模式，例如批量，重复消费，从尾端开始消费等</li></ol><h3 id="4、可扩展性"><a href="#4、可扩展性" class="headerlink" title="4、可扩展性"></a>4、可扩展性</h3><p>当需要增加broker结点时，新增的broker会向zookeeper注册，而producer及consumer会根据注册在zookeeper上的watcher感知这些变化，并及时作出调整。</p><h2 id="kafka的使用场景"><a href="#kafka的使用场景" class="headerlink" title="kafka的使用场景"></a>kafka的使用场景</h2><h3 id="1、消息队列"><a href="#1、消息队列" class="headerlink" title="1、消息队列"></a>1、消息队列</h3><p>比起大多数的消息系统来说，Kafka有更好的吞吐量，内置的分区，冗余及容错性，这让Kafka成为了一个很好的大规模消息处理应用的解决方案。消息系统一般吞吐量相对较低，但是需要更小的端到端延时，并尝尝依赖于Kafka提供的强大的持久性保障。在这个领域，Kafka足以媲美传统消息系统，如ActiveMR或RabbitMQ。</p><h3 id="2、行为跟踪"><a href="#2、行为跟踪" class="headerlink" title="2、行为跟踪"></a>2、行为跟踪</h3><p>Kafka的另一个应用场景是跟踪用户浏览页面、搜索及其他行为，以发布-订阅的模式实时记录到对应的topic里。那么这些结果被订阅者拿到后，就可以做进一步的实时处理，或实时监控，或放到hadoop/离线数据仓库里处理。</p><h3 id="3、元信息监控"><a href="#3、元信息监控" class="headerlink" title="3、元信息监控"></a>3、元信息监控</h3><p>作为操作记录的监控模块来使用，即汇集记录一些操作信息，可以理解为运维性质的数据监控吧。</p><h3 id="4、日志收集"><a href="#4、日志收集" class="headerlink" title="4、日志收集"></a>4、日志收集</h3><p>日志收集方面，其实开源产品有很多，包括Scribe、Apache Flume。很多人使用Kafka代替日志聚合（log aggregation）。日志聚合一般来说是从服务器上收集日志文件，然后放到一个集中的位置（文件服务器或HDFS）进行处理。然而Kafka忽略掉文件的细节，将其更清晰地抽象成一个个日志或事件的消息流。这就让Kafka处理过程延迟更低，更容易支持多数据源和分布式数据处理。比起以日志为中心的系统比如Scribe或者Flume来说，Kafka提供同样高效的性能和因为复制导致的更高的耐用性保证，以及更低的端到端延迟。</p><h2 id="kafka性能"><a href="#kafka性能" class="headerlink" title="kafka性能"></a>kafka性能</h2><p><img src="/image/kafka-2-0.png" alt=""></p><p>为了使得Kafka的吞吐率可以线性提高，物理上把Topic分成一个或多个Partition，每个Partition在物理上对应一个文件夹，该文件夹下存储这个Partition的所有消息和索引文件.每个分区都是有序的，不可变的记录序列，不断追加到结构化的提交日志中。分区中的记录每个分配一个连续的id号，称为offset(偏移量)，用于唯一标识分区内的每条记录。</p><p>实际上，保留在每个消费者基础上的唯一元数据是该消费者在日志中的抵消或位置。这个偏移量是由消费者控制的：消费者通常会在读取记录时线性地推进其偏移量，但实际上，由于位置由消费者控制，因此它可以按任何喜欢的顺序消费记录。例如，消费者可以重置为较旧的offset(偏移量)以重新处理来自过去的数据，或者跳至最近的记录并从“now”开始消费。随你喜欢爱怎么读怎么读,而且这些操作对集群或其他消费者没有太大影响。</p><p>这样的操作也就说kafka不用考虑加锁的问题,不存在消费完就要删除信息的问题,有效的保证了高吞吐率,这样没有锁竞争，充分发挥了横向的扩展性，吞吐量极高。这也就形成了分布式消费的概念。</p><p><img src="/image/kafka-2-1.png" alt=""></p><p>这里要注意，因为Kafka读取特定消息的时间复杂度为O(1)，即与文件大小无关，所以这里删除过期文件与提高Kafka性能无关,</p><p>当然kafka也提供了删除旧数据的策略:</p><ol><li><p>时间,可以自己设置一个储存的最大时间.</p></li><li><p>partition大小,可以给分区设置最大储存值.</p><p><img src="/image/kafka-2-2.png" alt=""></p><h3 id="consumer"><a href="#consumer" class="headerlink" title="consumer"></a>consumer</h3><p>p0=&gt;p3三个partition,而partition中的每个message只能被组（Consumer group）中的一个consumer（consumer 线程)消费.也就说一个partition只能被一个消费者消费（一个消费者可以同时消费多个partition）</p></li></ol><p>如果consumer从多个partition读到数据，不保证数据间的顺序性，kafka只保证在一个partition上数据是有序的，但多个partition，根据你读的顺序会有不同</p><h3 id="producer"><a href="#producer" class="headerlink" title="producer"></a>producer</h3><h4 id="Kakfa-Broker-Leader选举"><a href="#Kakfa-Broker-Leader选举" class="headerlink" title="Kakfa Broker Leader选举"></a>Kakfa Broker Leader选举</h4><p>kafka集群是受zookeeper来管理的,这里需要将所有的kafka broker节点一起注册到zookeeper上,而这个过程中只有一个kafka broker能注册成功,在zookeeper上注册一个临时节点,这个kafka broker叫kafka broker Controller,其他的叫kafka broker follower,一旦这个kafka broker Controller发生宕机,临时节点会消失,其他的kafka broker follower会在竞争去zookeeper上注册,产生一个新Leader.(注:Kafka集群中broker之间的关系,不是主从关系，各个broker在集群中地位一样，我们可以随意的增加或删除任何一个broker节点。),还有一种情况是有Controller下的一个follower宕机了,这时Controller会去读取这个follower在zookeeper上所有的partition leader信息(host:port),并且找到这些partition的备份们,让他们选一个成为这个partition的leader.如果该partition的所有的备份都宕机了，则将新的leader设置为-1，等待恢复，等待任一个备份“活”过来，并且选它作为Leader.</p><h4 id="在Producer向kafka-broker推送message"><a href="#在Producer向kafka-broker推送message" class="headerlink" title="在Producer向kafka broker推送message"></a>在Producer向kafka broker推送message</h4><p>kafka在所有broker中产生一个controller，所有Partition的Leader选举都由controller决定。controller会将Leader的改变直接通过RPC的方式（比Zookeeper Queue的方式更高效）通知需为此作出响应的Broker。</p><p>每个partition(分区)都有一台服务器充当“leader”，零个或多个服务器充当“follower”。leader处理分区的所有读取和写入请求，而follower被动地复制leader。如果leader失败，其中一个follower将自动成为新leader。每个服务器都充当其中一些分区的leader和其他人的follower，因此负载在集群内平衡良好。</p><h4 id="举个栗子"><a href="#举个栗子" class="headerlink" title="举个栗子"></a>举个栗子</h4><p>消息生产者,就是向 kafka broker发消息的客户端。 Producer 采用异步 push 方式, 极大提高 Kafka 系统的吞吐率(可以通过参数控制是采用同步还是异步方式)。 producer 端 , 可以将消息 buffer 起来 , 当消息的条数达到一定阀值时 , 批量发送给 broker 。</p><p>小数据 IO 太多,会拖慢整体的网络延迟,批量延迟发送事实上提升了网络效率。不过 这也有一定的隐患,比如说当 producer 失效时,那些尚未发送的消息将会丢失。</p><p>producer将会和Topic下所有partition leader保持 socket 连接 ; 消息由 producer 直接 通过 socket 发送到 broker, 中间不会经过任何 ” 路由层 “. 事实上 , 消息被路由到哪个 partition 上 , 由 producer 客户端决定。 partition leader的位置 (host:port)注册在 zookeeper 中 ,producer 作为 zookeeper client,已经注册了 watch 用来监听 partition leader的变更事件。</p><p><img src="/image/kafka-2-3.png" alt=""></p><p>如上图kafka集群有四个broker,一个topic有四个partition,并且每一个partition都有一个follower(其实就是备份);一个消息流输入之后会先储存一个topic在不同的partition leader中(并行写入),然后在由partition leader同步到各自的备份中.</p><p><img src="/image/kafka-2-5.png" alt=""></p><p>我们加两个broker5,6,这个时候partition的变化</p><h3 id="partition-分区-机制的优势"><a href="#partition-分区-机制的优势" class="headerlink" title="partition(分区)机制的优势:"></a>partition(分区)机制的优势:</h3><p>当Producer发送消息到broker时，会根据Paritition机制选择将其存储到哪一个Partition。也就是我们上面说的机制，所有消息可以均匀分布到不同的Partition里，这样就实现了负载均衡。如果一个Topic对应一个文件，那这个文件所在的机器I/O将会成为这个Topic的性能瓶颈，而有了Partition后，不同的消息可以并行写入不同broker的不同Partition里，极大的提高了吞吐率。<strong>所以说kafka可以水平扩展，也就是扩展partition。</strong>segment</p><p>一个partition可以实现跨服务器,可以一个分区占有一个服务器.</p><h2 id="参考博文"><a href="#参考博文" class="headerlink" title="参考博文"></a>参考博文</h2><p><a href="https://blog.csdn.net/mr_hou2016/article/details/79653242" target="_blank" rel="noopener">日志收集为什么用kafka</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Wed Nov 21 2018 15:08:21 GMT+0800 (China Standard Time) --&gt;&lt;h2 id=&quot;kafka的特点&quot;&gt;&lt;a href=&quot;#kafka的特点&quot; class=&quot;headerlink&quot; title=&quot;k
      
    
    </summary>
    
      <category term="kafka" scheme="http://www.fufan.me/categories/kafka/"/>
    
    
      <category term="kafka" scheme="http://www.fufan.me/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka学习笔记（二）——Kafka shell命令</title>
    <link href="http://www.fufan.me/2018/03/26/Kafka%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94Kafka-shell%E5%91%BD%E4%BB%A4/"/>
    <id>http://www.fufan.me/2018/03/26/Kafka学习笔记（二）——Kafka-shell命令/</id>
    <published>2018-03-26T10:17:00.000Z</published>
    <updated>2018-11-08T10:18:12.049Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Nov 21 2018 15:08:20 GMT+0800 (China Standard Time) --><p>排查问题的时候可能会用到终端的一些命令，下面列举一下常用的一些命令</p><h3 id="启动zookeeper"><a href="#启动zookeeper" class="headerlink" title="启动zookeeper"></a>启动zookeeper</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/zookeeper-server-start.sh config/zookeeper.properties &amp;</span><br></pre></td></tr></table></figure><h3 id="启动kafka"><a href="#启动kafka" class="headerlink" title="启动kafka"></a>启动kafka</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-server-start.sh config/server.properties &amp;</span><br></pre></td></tr></table></figure><h3 id="停止kafka"><a href="#停止kafka" class="headerlink" title="停止kafka"></a>停止kafka</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-server-stop.sh</span><br></pre></td></tr></table></figure><h3 id="停止zookeeper"><a href="#停止zookeeper" class="headerlink" title="停止zookeeper"></a>停止zookeeper</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/zookeeper-server-stop.sh</span><br></pre></td></tr></table></figure><h3 id="创建topic"><a href="#创建topic" class="headerlink" title="创建topic"></a>创建topic</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test</span><br></pre></td></tr></table></figure><h3 id="展示topic"><a href="#展示topic" class="headerlink" title="展示topic"></a>展示topic</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --list --zookeeper localhost:2181</span><br></pre></td></tr></table></figure><h3 id="描述topic"><a href="#描述topic" class="headerlink" title="描述topic"></a>描述topic</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic</span><br></pre></td></tr></table></figure><h3 id="生产者"><a href="#生产者" class="headerlink" title="生产者"></a>生产者</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-producer.sh --broker-list 130.51.23.95:9092 --topic my-replicated-topic</span><br></pre></td></tr></table></figure><h3 id="消费者"><a href="#消费者" class="headerlink" title="消费者"></a>消费者</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh --zookeeper 130.51.23.95:2181 --topic test --from-beginning</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Wed Nov 21 2018 15:08:20 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;排查问题的时候可能会用到终端的一些命令，下面列举一下常用的一些命令&lt;/p&gt;&lt;h3 id=&quot;启动zookeeper&quot;&gt;&lt;a h
      
    
    </summary>
    
    
      <category term="kafka" scheme="http://www.fufan.me/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka学习笔记（一）——Kafka入门</title>
    <link href="http://www.fufan.me/2018/03/18/Kafka%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94Kafka%E5%85%A5%E9%97%A8/"/>
    <id>http://www.fufan.me/2018/03/18/Kafka学习笔记（一）——Kafka入门/</id>
    <published>2018-03-18T10:32:00.000Z</published>
    <updated>2018-11-08T10:17:19.016Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Nov 21 2018 15:08:20 GMT+0800 (China Standard Time) --><p>Apache Kafka：一个分布式流处理平台</p><ul><li><p>| 对比指标 | kafka | activemq | rabbitmq | rocketmq |<br>| — | — | — | — | — |<br>| 背景 | Kafka 是LinkedIn 开发的一个高性能、分布式的消息系统，广泛用于日志收集、流式数据处理、在线和离线消息分发等场景 | ActiveMQ是一种开源的，实现了JMS1.1规范的，面向消息(MOM)的中间件， 为应用程序提供高效的、可扩展的、稳定的和安全的企业级消息通信。 | RabbitMQ是一个由erlang开发的AMQP协议（Advanced Message Queue ）的开源实现。 | RocketMQ是阿里巴巴在2012年开源的分布式消息中间件，目前已经捐赠给Apache基金会，已经于2016年11月成为 Apache 孵化项目 |<br>|开发语言 | Java、Scala | Java | Erlang | Java |<br>|协议支持 | 自己实现的一套 | JMS协议 | AMQP | JMS、MQTT |<br>|持久化 | 支持 | 支持 | 支持 | 支持 |<br>| producer容错 | 在kafka中提供了acks配置选项, acks=0 生产者在成功写入悄息之前不会等待任何来自服务器的响应 acks=1 只要集群的首领节点收到消息，生产者就会收到一个来自服务器的成功响应 acks=all 只有当所有参与复制的节点全部收到消息时，生产者才会收到一个来自服务器的成功响应,这种模式最安全 | 发送失败后即可重试 | 有ack模型。 ack模型可能重复消息 ，事务模型保证完全一致 | 和kafka类似 |<br>| 吞吐量 | kafka具有高的吞吐量，内部采用消息的批量处理，zero-copy机制，数据的存储和获取是本地磁盘顺序批量操作，具有O(1)的复杂度，消息处理的效率很高 | | rabbitMQ在吞吐量方面稍逊于kafka，他们的出发点不一样，rabbitMQ支持对消息的可靠的传递，支持事务，不支持批量的操作；基于存储的可靠性的要求存储可以采用内存或者硬盘。 | kafka在topic数量不多的情况下吞吐量比rocketMq高，在topic数量多的情况下rocketMq比kafka高 |<br>| 负载均衡 | kafka采用zookeeper对集群中的broker、consumer进行管理，可以注册topic到zookeeper上；通过zookeeper的协调机制，producer保存对应topic的broker信息，可以随机或者轮询发送到broker上；并且producer可以基于语义指定分片，消息发送到broker的某分片上 | | rabbitMQ的负载均衡需要单独的loadbalancer进行支持 | NamerServer进行负载均衡<br>|</p></li></ul><p><img src="/image/kafka-0-0.jpg" alt=""></p><h3 id="相关名词"><a href="#相关名词" class="headerlink" title="相关名词"></a>相关名词</h3><ul><li>Producer :消息生产者，向Broker发送消息的客户端</li><li>Consumer :消息消费者，从Broker读取消息的客户端,消费者&lt;=消息的分区数量</li><li>broker :消息中间件处理节点，一个Kafka节点就是一个broker，一个或者多个Broker可以组成一个Kafka集群</li><li>topic : 主题，Kafka根据topic对消息进行归类，发布到Kafka集群的每条消息都需要指定一个topic</li><li>Partition : 分区，物理上的概念，一个topic可以分为多个partition，每个partition内部是有序的，kafka默认根据key%partithon确定消息发送到具体的partition</li><li>ConsumerGroup : 每个Consumer属于一个特定的Consumer Group，一条消息可以发送到多个不同的Consumer Group，但是一个Consumer Group中只能有一个Consumer能够消费该消息</li></ul><h4 id="Topic-和-Partition"><a href="#Topic-和-Partition" class="headerlink" title="Topic 和 Partition"></a>Topic 和 Partition</h4><p>一个Topic中的消息会按照指定的规则(默认是key的hash值%分区的数量，当然你也可以自定义)，发送到某一个分区上面；<br>每一个分区都是一个顺序的、不可变的消息队列，并且可以持续的添加。分区中的消息都被分了一个序列号，称之为偏移量(offset)，在每个分区中此偏移量都是唯一的<br>消费者所持有的元数据就是这个偏移量，也就是消费者在这个log（分区）中的位置。这个偏移量由消费者控制：正常情况当消费者消费消息的时候，偏移量也线性的的增加</p><p><img src="/image/kafka-0-1.jpg" alt=""></p><h4 id="Consumer-和-Partition"><a href="#Consumer-和-Partition" class="headerlink" title="Consumer 和 Partition"></a>Consumer 和 Partition</h4><p>通常来讲，消息模型可以分为两种， 队列和发布-订阅式。队列的处理方式 是一个消费者组从队列的一端拉取数据，这个数据消费完就没了。在发布-订阅模型中，消息被广播给所有的消费者，接受到消息的消费者都能处理此消息。在Kafka模型中抽象出来了：消费者组（consumer group）<br>消费者组（consumer group）：每个组中有若干个消费者，如果所有的消费者都在一个组中，那么这个就变成了队列模型；如果笑消费者在不同的组中，这就成了发布-订阅模型<br>一个分区里面的数据只会由一个分组中的消费者处理，同分组的其他消费者不会重复处理<br>消费者组中的消费者数量&lt;=分区数量，如果大于分区数量，多出来的消费者会处于收不到消息的状态，造成不必要的浪费。</p><p><img src="/image/kafka-0-2.jpg" alt=""></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Wed Nov 21 2018 15:08:20 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;Apache Kafka：一个分布式流处理平台&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;| 对比指标 | kafka | activem
      
    
    </summary>
    
      <category term="kafka" scheme="http://www.fufan.me/categories/kafka/"/>
    
    
      <category term="kafka" scheme="http://www.fufan.me/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Consul学习笔记（一）——安装和命令使用</title>
    <link href="http://www.fufan.me/2018/02/28/Consul%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94%E5%AE%89%E8%A3%85%E5%92%8C%E5%91%BD%E4%BB%A4%E4%BD%BF%E7%94%A8/"/>
    <id>http://www.fufan.me/2018/02/28/Consul学习笔记（一）——安装和命令使用/</id>
    <published>2018-02-28T10:14:00.000Z</published>
    <updated>2018-11-08T10:14:33.306Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Nov 21 2018 15:08:20 GMT+0800 (China Standard Time) --><p>由于公司目前工作当中微服务用到了consul集群来作为分布式系统中间件，用到了他内置了服务注册与发现框 架、分布一致性协议实现、健康检查、Key/Value存储、多数据中心方案。不再需要依赖其他工具（比如ZooKeeper等）。使用起来也较 为简单。</p><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>Consul 是 HashiCorp 公司推出的开源工具，用于实现分布式系统的服务发现与配置。与其他分布式服务注册与发现的方案，Consul的方案更“一站式”，内置了服务注册与发现框 架、分布一致性协议实现、健康检查、Key/Value存储、多数据中心方案，不再需要依赖其他工具（比如ZooKeeper等）。使用起来也较 为简单。Consul使用Go语言编写，因此具有天然可移植性(支持Linux、windows和Mac OS X)；安装包仅包含一个可执行文件，方便部署，与Docker等轻量级容器可无缝配合 。</p><ul><li>Service Discovery (服务发现)</li><li>Health Check (健康检查)</li><li>Multi Datacenter (多数据中心)</li><li>Key/Value Storage</li></ul><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>mac：64bit（查看mac位数：打开终端–&gt;”uname -a”）<br>consul_0.6.4_darwin_amd64.zip和consul_0.6.4_web_ui.zip，从consul官网<a href="https://www.consul.io/downloads.html进行下载就好（选择好OS和位数）" target="_blank" rel="noopener">https://www.consul.io/downloads.html进行下载就好（选择好OS和位数）</a></p><p>1、解压consul_0.6.4_darwin_amd64.zip<br>2、将解压后的二进制文件consul（上边画红框的部分拷贝到/usr/local/bin下）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo scp consul /usr/local/bin/</span><br></pre></td></tr></table></figure><p>说明：使用sudo是因为权限问题。</p><p>3、查看是否安装成功</p><p>调用其命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"> fufan@fufans-MacBook-Pro-2  ~  consul help</span><br><span class="line">Usage: consul [--version] [--help] &lt;command&gt; [&lt;args&gt;]</span><br><span class="line"></span><br><span class="line">Available commands are:</span><br><span class="line">    agent          Runs a Consul agent</span><br><span class="line">    catalog        Interact with the catalog</span><br><span class="line">    connect        Interact with Consul Connect</span><br><span class="line">    event          Fire a new event</span><br><span class="line">    exec           Executes a command on Consul nodes</span><br><span class="line">    force-leave    Forces a member of the cluster to enter the &quot;left&quot; state</span><br><span class="line">    info           Provides debugging information for operators.</span><br><span class="line">    intention      Interact with Connect service intentions</span><br><span class="line">    join           Tell Consul agent to join cluster</span><br><span class="line">    keygen         Generates a new encryption key</span><br><span class="line">    keyring        Manages gossip layer encryption keys</span><br><span class="line">    kv             Interact with the key-value store</span><br><span class="line">    leave          Gracefully leaves the Consul cluster and shuts down</span><br><span class="line">    lock           Execute a command holding a lock</span><br><span class="line">    maint          Controls node or service maintenance mode</span><br><span class="line">    members        Lists the members of a Consul cluster</span><br><span class="line">    monitor        Stream logs from a Consul agent</span><br><span class="line">    operator       Provides cluster-level tools for Consul operators</span><br><span class="line">    reload         Triggers the agent to reload configuration files</span><br><span class="line">    rtt            Estimates network round trip time between nodes</span><br><span class="line">    snapshot       Saves, restores and inspects snapshots of Consul server state</span><br><span class="line">    validate       Validate config files/directories</span><br><span class="line">    version        Prints the Consul version</span><br><span class="line">    watch          Watch for changes in Consul</span><br></pre></td></tr></table></figure><h3 id="consul相关知识点"><a href="#consul相关知识点" class="headerlink" title="consul相关知识点"></a>consul相关知识点</h3><ol><li>Agent</li></ol><ul><li>Agent 是一个守护进程</li><li>运行在Consul集群的每个成员上</li><li>有Client 和 Server 两种模式</li><li>所有Agent都可以被调用DNS或者HTTP API,并负责检查和维护同步</li></ul><ol start="2"><li>Client</li></ol><ul><li>Client 将所有RPC请求转发至Server</li><li>Client 是相对无状态的</li><li>Client 唯一做的就是参与LAN Gossip Pool</li><li>Client 只消耗少量的资源和少量的网络带宽</li></ul><ol start="3"><li>Server</li></ol><ul><li>参与 Raft quorum(一致性判断)</li><li>响应RPC查询请求</li><li>维护集群的状态</li><li>转发查询到Leader 或 远程数据中心</li></ul><ol start="4"><li>Datacenter数据中心</li></ol><ul><li>私有的</li><li>低延迟</li><li>高带宽</li></ul><ol start="5"><li>Consensus (一致性)</li></ol><p>Consul 使用consensus protocol 来提供CAP(一致性,高可用,分区容错性)</p><ol start="5"><li>Gossip</li></ol><p>一种协议: 用来保证 最终一致性 , 即: 无法保证在某个时刻, 所有节点状态一致, 但可以保证”最终”一致</p><h3 id="注册服务"><a href="#注册服务" class="headerlink" title="注册服务"></a>注册服务</h3><ul><li>服务可以通过提供服务定义或通过对HTTP API进行适当的调用来注册。</li><li>服务定义是注册服务最常用的方式，所以我们将在这一步中使用这种方法。 我们将建立在上一步中介绍的代理配置。</li><li>首先，为Consul配置创建一个目录。 Consul将所有配置文件加载到配置目录中，因此Unix系统上的一个通用约定是将目录命名为/etc/consul.d（.d后缀意味着“该目录包含一组配置文件”）。</li></ul><p>建立服务配置目录:mkdir /etc/consul.d<br>添加文件:echo ‘{“service”: {“name”: “web”, “tags”: [“rails”], “port”: 80}}’ | sudo tee /etc/consul.d/web.json</p><ul><li>以开发模式启动:consul agent -dev -config-dir=/etc/consul.d</li><li>以服务方式启动:consul agent -server -bootstrap-expect 2 -data-dir ./tmp/consul -node=n1 -bind=192.168.109.241 -ui-dir ./dist -dc=dc1</li><li>以客户端方式启动:consul agent -data-dir ./tmp/consul -ui-dir ./dist -bind=192.168.109.204 -dc=dc1</li></ul><h3 id="加入集群"><a href="#加入集群" class="headerlink" title="加入集群"></a>加入集群</h3><p>将新节点添加到集群:consul join 192.168.100.101(其中101这个节点是master)</p><p>显示成员:consul members</p><h3 id="查看UI管理页面"><a href="#查看UI管理页面" class="headerlink" title="查看UI管理页面"></a>查看UI管理页面</h3><p><a href="http://192.168.0.70:8500/ui" target="_blank" rel="noopener">http://192.168.0.70:8500/ui</a></p><p><img src="/image/consul-0-1.png" alt=""></p><h3 id="常用命令参数"><a href="#常用命令参数" class="headerlink" title="常用命令参数"></a>常用命令参数</h3><p>consul agent 命令的常用选项，如下：</p><ul><li>-data-dir<ul><li>作用：指定agent储存状态的数据目录</li><li>这是所有agent都必须的</li><li>对于server尤其重要，因为他们必须持久化集群的状态</li></ul></li><li>-config-dir<ul><li>作用：指定service的配置文件和检查定义所在的位置</li><li>通常会指定为”某一个路径/consul.d”（通常情况下，.d表示一系列配置文件存放的目录）</li></ul></li><li>-config-file<ul><li>作用：指定一个要装载的配置文件</li><li>该选项可以配置多次，进而配置多个配置文件（后边的会合并前边的，相同的值覆盖）</li></ul></li><li>-dev<ul><li>作用：创建一个开发环境下的server节点</li><li>该参数配置下，不会有任何持久化操作，即不会有任何数据写入到磁盘</li><li>这种模式不能用于生产环境（因为第二条）</li></ul></li><li>-bootstrap-expect<ul><li>作用：该命令通知consul server我们现在准备加入的server节点个数，该参数是为了延迟日志复制的启动直到我们指定数量的server节点成功的加入后启动。</li></ul></li><li>-node<ul><li>作用：指定节点在集群中的名称</li><li>该名称在集群中必须是唯一的（默认采用机器的host）</li><li>推荐：直接采用机器的IP</li></ul></li><li>-bind<ul><li>作用：指明节点的IP地址</li><li>有时候不指定绑定IP，会报Failed to get advertise address: Multiple private IPs found. Please configure one. 的异常</li></ul></li><li>-server<ul><li>作用：指定节点为server</li><li>每个数据中心（DC）的server数推荐至少为1，至多为5</li><li>所有的server都采用raft一致性算法来确保事务的一致性和线性化，事务修改了集群的状态，且集群的状态保存在每一台server上保证可用性</li><li>server也是与其他DC交互的门面（gateway）</li></ul></li><li>-client<ul><li>作用：指定节点为client，指定客户端接口的绑定地址，包括：HTTP、DNS、RPC</li><li>默认是127.0.0.1，只允许回环接口访问</li><li>若不指定为-server，其实就是-client</li></ul></li><li>-join<ul><li>作用：将节点加入到集群</li></ul></li><li>-datacenter（老版本叫-dc，-dc已经失效）<ul><li>作用：指定机器加入到哪一个数据中心中</li></ul></li></ul><h3 id="搭建集群"><a href="#搭建集群" class="headerlink" title="搭建集群"></a>搭建集群</h3><p><img src="/image/consul-0-1.png" alt=""></p><p>此部分过程同redis集群、zookeeper集群类似</p><h3 id="问题踩坑"><a href="#问题踩坑" class="headerlink" title="问题踩坑"></a>问题踩坑</h3><ul><li>google</li><li>github issues</li></ul><h3 id="参考博文："><a href="#参考博文：" class="headerlink" title="参考博文："></a>参考博文：</h3><ul><li>Consul官方文档：<a href="https://www.consul.io/intro/getting-started/install.html" target="_blank" rel="noopener">https://www.consul.io/intro/getting-started/install.html</a></li><li>Consul 系列博文：<a href="http://www.cnblogs.com/java-zhao/archive/2016/04/13/5387105.html" target="_blank" rel="noopener">http://www.cnblogs.com/java-zhao/archive/2016/04/13/5387105.html</a></li><li>使用consul实现分布式服务注册和发现：<a href="http://www.tuicool.com/articles/M3QFven" target="_blank" rel="noopener">http://www.tuicool.com/articles/M3QFven</a></li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Wed Nov 21 2018 15:08:20 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;由于公司目前工作当中微服务用到了consul集群来作为分布式系统中间件，用到了他内置了服务注册与发现框 架、分布一致性协议实
      
    
    </summary>
    
      <category term="consul" scheme="http://www.fufan.me/categories/consul/"/>
    
    
      <category term="consul" scheme="http://www.fufan.me/tags/consul/"/>
    
  </entry>
  
  <entry>
    <title>Redis学习笔记（二）——redis安装</title>
    <link href="http://www.fufan.me/2018/02/15/Redis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94redis%E5%AE%89%E8%A3%85/"/>
    <id>http://www.fufan.me/2018/02/15/Redis学习笔记（二）——redis安装/</id>
    <published>2018-02-15T03:05:00.000Z</published>
    <updated>2018-11-08T10:13:22.293Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Nov 21 2018 15:08:21 GMT+0800 (China Standard Time) --><p>由于公司开发笔记本用的是mac，所以我这里介绍一下在mac和linux环境下的redis安装</p><h2 id="Linux"><a href="#Linux" class="headerlink" title="Linux"></a>Linux</h2><p>1、下载源码，解压缩后编译源码。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> $ wget http://download.redis.io/releases/redis-2.8.3.tar.gz</span><br><span class="line">$ tar xzf redis-2.8.3.tar.gz</span><br><span class="line">$ cd redis-2.8.3</span><br><span class="line">$ make</span><br></pre></td></tr></table></figure><p>2、编译完成后，在Src目录下，有四个可执行文件redis-server、redis-benchmark、redis-cli和redis.conf。然后拷贝到一个目录下。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mkdir /usr/redis</span><br><span class="line">cp redis-server  /usr/redis</span><br><span class="line">cp redis-benchmark /usr/redis</span><br><span class="line">cp redis-cli  /usr/redis</span><br><span class="line">cp redis.conf  /usr/redis</span><br><span class="line">cd /usr/redis</span><br></pre></td></tr></table></figure><p>3、启动Redis服务。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ redis-server   redis.conf</span><br></pre></td></tr></table></figure><p></p><p>4、然后用客户端测试一下是否启动成功。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ redis-cli</span><br><span class="line">redis&gt; set foo bar</span><br><span class="line">OK</span><br><span class="line">redis&gt; get foo</span><br><span class="line">&quot;bar&quot;</span><br></pre></td></tr></table></figure><p></p><h2 id="Mac"><a href="#Mac" class="headerlink" title="Mac"></a>Mac</h2><h3 id="brew-安装"><a href="#brew-安装" class="headerlink" title="brew 安装"></a>brew 安装</h3><p>1、使用brew命令安装redis<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install redis</span><br></pre></td></tr></table></figure><p></p><p>2、启动redis</p><p>后台方式启动，brew services start redis。这样启动的好处是把控制台关掉后，redis仍然是启动的。当然，如果没有这样的需求，也可以这样启动<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">redis-server /usr/local/etc/redis.conf</span><br></pre></td></tr></table></figure><p></p><p>3、关闭redis<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew services stop redis</span><br></pre></td></tr></table></figure><p></p><p>4、使用控制台连接redis</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">redis-cli</span><br><span class="line"></span><br><span class="line">redis-cli -h 127.0.0.1 -p</span><br></pre></td></tr></table></figure><h2 id="redis-conf配置说明"><a href="#redis-conf配置说明" class="headerlink" title="redis.conf配置说明"></a>redis.conf配置说明</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br></pre></td><td class="code"><pre><span class="line"># Redis配置文件样例</span><br><span class="line"></span><br><span class="line"># Note on units: when memory size is needed, it is possible to specifiy</span><br><span class="line"># it in the usual form of 1k 5GB 4M and so forth:</span><br><span class="line">#</span><br><span class="line"># 1k =&gt; 1000 bytes</span><br><span class="line"># 1kb =&gt; 1024 bytes</span><br><span class="line"># 1m =&gt; 1000000 bytes</span><br><span class="line"># 1mb =&gt; 1024*1024 bytes</span><br><span class="line"># 1g =&gt; 1000000000 bytes</span><br><span class="line"># 1gb =&gt; 1024*1024*1024 bytes</span><br><span class="line">#</span><br><span class="line"># units are case insensitive so 1GB 1Gb 1gB are all the same.</span><br><span class="line"></span><br><span class="line"># Redis默认不是以守护进程的方式运行，可以通过该配置项修改，使用yes启用守护进程</span><br><span class="line"># 启用守护进程后，Redis会把pid写到一个pidfile中，在/var/run/redis.pid</span><br><span class="line">daemonize no</span><br><span class="line"></span><br><span class="line"># 当Redis以守护进程方式运行时，Redis默认会把pid写入/var/run/redis.pid文件，可以通过pidfile指定</span><br><span class="line">pidfile /var/run/redis.pid</span><br><span class="line"></span><br><span class="line"># 指定Redis监听端口，默认端口为6379</span><br><span class="line"># 如果指定0端口，表示Redis不监听TCP连接</span><br><span class="line">port 6379</span><br><span class="line"></span><br><span class="line"># 绑定的主机地址</span><br><span class="line"># 你可以绑定单一接口，如果没有绑定，所有接口都会监听到来的连接</span><br><span class="line"># bind 127.0.0.1</span><br><span class="line"></span><br><span class="line"># Specify the path for the unix socket that will be used to listen for</span><br><span class="line"># incoming connections. There is no default, so Redis will not listen</span><br><span class="line"># on a unix socket when not specified.</span><br><span class="line">#</span><br><span class="line"># unixsocket /tmp/redis.sock</span><br><span class="line"># unixsocketperm 755</span><br><span class="line"></span><br><span class="line"># 当客户端闲置多长时间后关闭连接，如果指定为0，表示关闭该功能</span><br><span class="line">timeout 0</span><br><span class="line"></span><br><span class="line"># 指定日志记录级别，Redis总共支持四个级别：debug、verbose、notice、warning，默认为verbose</span><br><span class="line"># debug (很多信息, 对开发／测试比较有用)</span><br><span class="line"># verbose (many rarely useful info, but not a mess like the debug level)</span><br><span class="line"># notice (moderately verbose, what you want in production probably)</span><br><span class="line"># warning (only very important / critical messages are logged)</span><br><span class="line">loglevel verbose</span><br><span class="line"></span><br><span class="line"># 日志记录方式，默认为标准输出，如果配置为redis为守护进程方式运行，而这里又配置为标准输出，则日志将会发送给/dev/null</span><br><span class="line">logfile stdout</span><br><span class="line"></span><br><span class="line"># To enable logging to the system logger, just set &apos;syslog-enabled&apos; to yes,</span><br><span class="line"># and optionally update the other syslog parameters to suit your needs.</span><br><span class="line"># syslog-enabled no</span><br><span class="line"></span><br><span class="line"># Specify the syslog identity.</span><br><span class="line"># syslog-ident redis</span><br><span class="line"></span><br><span class="line"># Specify the syslog facility.  Must be USER or between LOCAL0-LOCAL7.</span><br><span class="line"># syslog-facility local0</span><br><span class="line"></span><br><span class="line"># 设置数据库的数量，默认数据库为0，可以使用select &lt;dbid&gt;命令在连接上指定数据库id</span><br><span class="line"># dbid是从0到‘databases’-1的数目</span><br><span class="line">databases 16</span><br><span class="line"></span><br><span class="line">################################ SNAPSHOTTING  #################################</span><br><span class="line"># 指定在多长时间内，有多少次更新操作，就将数据同步到数据文件，可以多个条件配合</span><br><span class="line"># Save the DB on disk:</span><br><span class="line">#</span><br><span class="line">#   save &lt;seconds&gt; &lt;changes&gt;</span><br><span class="line">#</span><br><span class="line">#   Will save the DB if both the given number of seconds and the given</span><br><span class="line">#   number of write operations against the DB occurred.</span><br><span class="line">#</span><br><span class="line">#   满足以下条件将会同步数据:</span><br><span class="line">#   900秒（15分钟）内有1个更改</span><br><span class="line">#   300秒（5分钟）内有10个更改</span><br><span class="line">#   60秒内有10000个更改</span><br><span class="line">#   Note: 可以把所有“save”行注释掉，这样就取消同步操作了</span><br><span class="line"></span><br><span class="line">save 900 1</span><br><span class="line">save 300 10</span><br><span class="line">save 60 10000</span><br><span class="line"></span><br><span class="line"># 指定存储至本地数据库时是否压缩数据，默认为yes，Redis采用LZF压缩，如果为了节省CPU时间，可以关闭该选项，但会导致数据库文件变的巨大</span><br><span class="line">rdbcompression yes</span><br><span class="line"></span><br><span class="line"># 指定本地数据库文件名，默认值为dump.rdb</span><br><span class="line">dbfilename dump.rdb</span><br><span class="line"></span><br><span class="line"># 工作目录.</span><br><span class="line"># 指定本地数据库存放目录，文件名由上一个dbfilename配置项指定</span><br><span class="line"># </span><br><span class="line"># Also the Append Only File will be created inside this directory.</span><br><span class="line"># </span><br><span class="line"># 注意，这里只能指定一个目录，不能指定文件名</span><br><span class="line">dir ./</span><br><span class="line"></span><br><span class="line">################################# REPLICATION #################################</span><br><span class="line"></span><br><span class="line"># 主从复制。使用slaveof从 Redis服务器复制一个Redis实例。注意，该配置仅限于当前slave有效</span><br><span class="line"># so for example it is possible to configure the slave to save the DB with a</span><br><span class="line"># different interval, or to listen to another port, and so on.</span><br><span class="line"># 设置当本机为slav服务时，设置master服务的ip地址及端口，在Redis启动时，它会自动从master进行数据同步</span><br><span class="line"># slaveof &lt;masterip&gt; &lt;masterport&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 当master服务设置了密码保护时，slav服务连接master的密码</span><br><span class="line"># 下文的“requirepass”配置项可以指定密码</span><br><span class="line"># masterauth &lt;master-password&gt;</span><br><span class="line"></span><br><span class="line"># When a slave lost the connection with the master, or when the replication</span><br><span class="line"># is still in progress, the slave can act in two different ways:</span><br><span class="line">#</span><br><span class="line"># 1) if slave-serve-stale-data is set to &apos;yes&apos; (the default) the slave will</span><br><span class="line">#    still reply to client requests, possibly with out of data data, or the</span><br><span class="line">#    data set may just be empty if this is the first synchronization.</span><br><span class="line">#</span><br><span class="line"># 2) if slave-serve-stale data is set to &apos;no&apos; the slave will reply with</span><br><span class="line">#    an error &quot;SYNC with master in progress&quot; to all the kind of commands</span><br><span class="line">#    but to INFO and SLAVEOF.</span><br><span class="line">#</span><br><span class="line">slave-serve-stale-data yes</span><br><span class="line"></span><br><span class="line"># Slaves send PINGs to server in a predefined interval. It&apos;s possible to change</span><br><span class="line"># this interval with the repl_ping_slave_period option. The default value is 10</span><br><span class="line"># seconds.</span><br><span class="line">#</span><br><span class="line"># repl-ping-slave-period 10</span><br><span class="line"></span><br><span class="line"># The following option sets a timeout for both Bulk transfer I/O timeout and</span><br><span class="line"># master data or ping response timeout. The default value is 60 seconds.</span><br><span class="line">#</span><br><span class="line"># It is important to make sure that this value is greater than the value</span><br><span class="line"># specified for repl-ping-slave-period otherwise a timeout will be detected</span><br><span class="line"># every time there is low traffic between the master and the slave.</span><br><span class="line">#</span><br><span class="line"># repl-timeout 60</span><br><span class="line"></span><br><span class="line">################################## SECURITY ###################################</span><br><span class="line"></span><br><span class="line"># Warning: since Redis is pretty fast an outside user can try up to</span><br><span class="line"># 150k passwords per second against a good box. This means that you should</span><br><span class="line"># use a very strong password otherwise it will be very easy to break.</span><br><span class="line"># 设置Redis连接密码，如果配置了连接密码，客户端在连接Redis时需要通过auth &lt;password&gt;命令提供密码，默认关闭</span><br><span class="line"># requirepass foobared</span><br><span class="line"></span><br><span class="line"># Command renaming.</span><br><span class="line">#</span><br><span class="line"># It is possilbe to change the name of dangerous commands in a shared</span><br><span class="line"># environment. For instance the CONFIG command may be renamed into something</span><br><span class="line"># of hard to guess so that it will be still available for internal-use</span><br><span class="line"># tools but not available for general clients.</span><br><span class="line">#</span><br><span class="line"># Example:</span><br><span class="line">#</span><br><span class="line"># rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52</span><br><span class="line">#</span><br><span class="line"># It is also possilbe to completely kill a command renaming it into</span><br><span class="line"># an empty string:</span><br><span class="line">#</span><br><span class="line"># rename-command CONFIG &quot;&quot;</span><br><span class="line"></span><br><span class="line">################################### LIMITS ####################################</span><br><span class="line"></span><br><span class="line"># 设置同一时间最大客户端连接数，默认无限制，Redis可以同时打开的客户端连接数为Redis进程可以打开的最大文件描述符数，</span><br><span class="line"># 如果设置maxclients 0，表示不作限制。当客户端连接数到达限制时，Redis会关闭新的连接并向客户端返回max Number of clients reached错误信息</span><br><span class="line"># maxclients 128</span><br><span class="line"></span><br><span class="line"># Don&apos;t use more memory than the specified amount of bytes.</span><br><span class="line"># When the memory limit is reached Redis will try to remove keys with an</span><br><span class="line"># EXPIRE set. It will try to start freeing keys that are going to expire</span><br><span class="line"># in little time and preserve keys with a longer time to live.</span><br><span class="line"># Redis will also try to remove objects from free lists if possible.</span><br><span class="line">#</span><br><span class="line"># If all this fails, Redis will start to reply with errors to commands</span><br><span class="line"># that will use more memory, like SET, LPUSH, and so on, and will continue</span><br><span class="line"># to reply to most read-only commands like GET.</span><br><span class="line">#</span><br><span class="line"># WARNING: maxmemory can be a good idea mainly if you want to use Redis as a</span><br><span class="line"># &apos;state&apos; server or cache, not as a real DB. When Redis is used as a real</span><br><span class="line"># database the memory usage will grow over the weeks, it will be obvious if</span><br><span class="line"># it is going to use too much memory in the long run, and you&apos;ll have the time</span><br><span class="line"># to upgrade. With maxmemory after the limit is reached you&apos;ll start to get</span><br><span class="line"># errors for write operations, and this may even lead to DB inconsistency.</span><br><span class="line"># 指定Redis最大内存限制，Redis在启动时会把数据加载到内存中，达到最大内存后，Redis会先尝试清除已到期或即将到期的Key，</span><br><span class="line"># 当此方法处理后，仍然到达最大内存设置，将无法再进行写入操作，但仍然可以进行读取操作。</span><br><span class="line"># Redis新的vm机制，会把Key存放内存，Value会存放在swap区</span><br><span class="line"># maxmemory &lt;bytes&gt;</span><br><span class="line"></span><br><span class="line"># MAXMEMORY POLICY: how Redis will select what to remove when maxmemory</span><br><span class="line"># is reached? You can select among five behavior:</span><br><span class="line"># </span><br><span class="line"># volatile-lru -&gt; remove the key with an expire set using an LRU algorithm</span><br><span class="line"># allkeys-lru -&gt; remove any key accordingly to the LRU algorithm</span><br><span class="line"># volatile-random -&gt; remove a random key with an expire set</span><br><span class="line"># allkeys-&gt;random -&gt; remove a random key, any key</span><br><span class="line"># volatile-ttl -&gt; remove the key with the nearest expire time (minor TTL)</span><br><span class="line"># noeviction -&gt; don&apos;t expire at all, just return an error on write operations</span><br><span class="line"># </span><br><span class="line"># Note: with all the kind of policies, Redis will return an error on write</span><br><span class="line">#       operations, when there are not suitable keys for eviction.</span><br><span class="line">#</span><br><span class="line">#       At the date of writing this commands are: set setnx setex append</span><br><span class="line">#       incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd</span><br><span class="line">#       sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby</span><br><span class="line">#       zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby</span><br><span class="line">#       getset mset msetnx exec sort</span><br><span class="line">#</span><br><span class="line"># The default is:</span><br><span class="line">#</span><br><span class="line"># maxmemory-policy volatile-lru</span><br><span class="line"></span><br><span class="line"># LRU and minimal TTL algorithms are not precise algorithms but approximated</span><br><span class="line"># algorithms (in order to save memory), so you can select as well the sample</span><br><span class="line"># size to check. For instance for default Redis will check three keys and</span><br><span class="line"># pick the one that was used less recently, you can change the sample size</span><br><span class="line"># using the following configuration directive.</span><br><span class="line">#</span><br><span class="line"># maxmemory-samples 3</span><br><span class="line"></span><br><span class="line">############################## APPEND ONLY MODE ###############################</span><br><span class="line"></span><br><span class="line"># </span><br><span class="line"># Note that you can have both the async dumps and the append only file if you</span><br><span class="line"># like (you have to comment the &quot;save&quot; statements above to disable the dumps).</span><br><span class="line"># Still if append only mode is enabled Redis will load the data from the</span><br><span class="line"># log file at startup ignoring the dump.rdb file.</span><br><span class="line"># 指定是否在每次更新操作后进行日志记录，Redis在默认情况下是异步的把数据写入磁盘，如果不开启，可能会在断电时导致一段时间内的数据丢失。</span><br><span class="line"># 因为redis本身同步数据文件是按上面save条件来同步的，所以有的数据会在一段时间内只存在于内存中。默认为no</span><br><span class="line"># IMPORTANT: Check the BGREWRITEAOF to check how to rewrite the append</span><br><span class="line"># log file in background when it gets too big.</span><br><span class="line"></span><br><span class="line">appendonly no</span><br><span class="line"></span><br><span class="line"># 指定更新日志文件名，默认为appendonly.aof</span><br><span class="line"># appendfilename appendonly.aof</span><br><span class="line"></span><br><span class="line"># The fsync() call tells the Operating System to actually write data on disk</span><br><span class="line"># instead to wait for more data in the output buffer. Some OS will really flush </span><br><span class="line"># data on disk, some other OS will just try to do it ASAP.</span><br><span class="line"></span><br><span class="line"># 指定更新日志条件，共有3个可选值：</span><br><span class="line"># no:表示等操作系统进行数据缓存同步到磁盘（快）</span><br><span class="line"># always:表示每次更新操作后手动调用fsync()将数据写到磁盘（慢，安全）</span><br><span class="line"># everysec:表示每秒同步一次（折衷，默认值）</span><br><span class="line"></span><br><span class="line">appendfsync everysec</span><br><span class="line"># appendfsync no</span><br><span class="line"></span><br><span class="line"># When the AOF fsync policy is set to always or everysec, and a background</span><br><span class="line"># saving process (a background save or AOF log background rewriting) is</span><br><span class="line"># performing a lot of I/O against the disk, in some Linux configurations</span><br><span class="line"># Redis may block too long on the fsync() call. Note that there is no fix for</span><br><span class="line"># this currently, as even performing fsync in a different thread will block</span><br><span class="line"># our synchronous write(2) call.</span><br><span class="line">#</span><br><span class="line"># In order to mitigate this problem it&apos;s possible to use the following option</span><br><span class="line"># that will prevent fsync() from being called in the main process while a</span><br><span class="line"># BGSAVE or BGREWRITEAOF is in progress.</span><br><span class="line">#</span><br><span class="line"># This means that while another child is saving the durability of Redis is</span><br><span class="line"># the same as &quot;appendfsync none&quot;, that in pratical terms means that it is</span><br><span class="line"># possible to lost up to 30 seconds of log in the worst scenario (with the</span><br><span class="line"># default Linux settings).</span><br><span class="line"># </span><br><span class="line"># If you have latency problems turn this to &quot;yes&quot;. Otherwise leave it as</span><br><span class="line"># &quot;no&quot; that is the safest pick from the point of view of durability.</span><br><span class="line">no-appendfsync-on-rewrite no</span><br><span class="line"></span><br><span class="line"># Automatic rewrite of the append only file.</span><br><span class="line"># Redis is able to automatically rewrite the log file implicitly calling</span><br><span class="line"># BGREWRITEAOF when the AOF log size will growth by the specified percentage.</span><br><span class="line"># </span><br><span class="line"># This is how it works: Redis remembers the size of the AOF file after the</span><br><span class="line"># latest rewrite (or if no rewrite happened since the restart, the size of</span><br><span class="line"># the AOF at startup is used).</span><br><span class="line">#</span><br><span class="line"># This base size is compared to the current size. If the current size is</span><br><span class="line"># bigger than the specified percentage, the rewrite is triggered. Also</span><br><span class="line"># you need to specify a minimal size for the AOF file to be rewritten, this</span><br><span class="line"># is useful to avoid rewriting the AOF file even if the percentage increase</span><br><span class="line"># is reached but it is still pretty small.</span><br><span class="line">#</span><br><span class="line"># Specify a precentage of zero in order to disable the automatic AOF</span><br><span class="line"># rewrite feature.</span><br><span class="line"></span><br><span class="line">auto-aof-rewrite-percentage 100</span><br><span class="line">auto-aof-rewrite-min-size 64mb</span><br><span class="line"></span><br><span class="line">################################## SLOW LOG ###################################</span><br><span class="line"></span><br><span class="line"># The Redis Slow Log is a system to log queries that exceeded a specified</span><br><span class="line"># execution time. The execution time does not include the I/O operations</span><br><span class="line"># like talking with the client, sending the reply and so forth,</span><br><span class="line"># but just the time needed to actually execute the command (this is the only</span><br><span class="line"># stage of command execution where the thread is blocked and can not serve</span><br><span class="line"># other requests in the meantime).</span><br><span class="line"># </span><br><span class="line"># You can configure the slow log with two parameters: one tells Redis</span><br><span class="line"># what is the execution time, in microseconds, to exceed in order for the</span><br><span class="line"># command to get logged, and the other parameter is the length of the</span><br><span class="line"># slow log. When a new command is logged the oldest one is removed from the</span><br><span class="line"># queue of logged commands.</span><br><span class="line"></span><br><span class="line"># The following time is expressed in microseconds, so 1000000 is equivalent</span><br><span class="line"># to one second. Note that a negative number disables the slow log, while</span><br><span class="line"># a value of zero forces the logging of every command.</span><br><span class="line">slowlog-log-slower-than 10000</span><br><span class="line"></span><br><span class="line"># There is no limit to this length. Just be aware that it will consume memory.</span><br><span class="line"># You can reclaim memory used by the slow log with SLOWLOG RESET.</span><br><span class="line">slowlog-max-len 1024</span><br><span class="line"></span><br><span class="line">################################ VIRTUAL MEMORY ###############################</span><br><span class="line"></span><br><span class="line">### WARNING! Virtual Memory is deprecated in Redis 2.4</span><br><span class="line">### The use of Virtual Memory is strongly discouraged.</span><br><span class="line"></span><br><span class="line">### WARNING! Virtual Memory is deprecated in Redis 2.4</span><br><span class="line">### The use of Virtual Memory is strongly discouraged.</span><br><span class="line"></span><br><span class="line"># Virtual Memory allows Redis to work with datasets bigger than the actual</span><br><span class="line"># amount of RAM needed to hold the whole dataset in memory.</span><br><span class="line"># In order to do so very used keys are taken in memory while the other keys</span><br><span class="line"># are swapped into a swap file, similarly to what operating systems do</span><br><span class="line"># with memory pages.</span><br><span class="line"># 指定是否启用虚拟内存机制，默认值为no，</span><br><span class="line"># VM机制将数据分页存放，由Redis将访问量较少的页即冷数据swap到磁盘上，访问多的页面由磁盘自动换出到内存中</span><br><span class="line"># 把vm-enabled设置为yes，根据需要设置好接下来的三个VM参数，就可以启动VM了</span><br><span class="line">vm-enabled no</span><br><span class="line"># vm-enabled yes</span><br><span class="line"></span><br><span class="line"># This is the path of the Redis swap file. As you can guess, swap files</span><br><span class="line"># can&apos;t be shared by different Redis instances, so make sure to use a swap</span><br><span class="line"># file for every redis process you are running. Redis will complain if the</span><br><span class="line"># swap file is already in use.</span><br><span class="line">#</span><br><span class="line"># Redis交换文件最好的存储是SSD（固态硬盘）</span><br><span class="line"># 虚拟内存文件路径，默认值为/tmp/redis.swap，不可多个Redis实例共享</span><br><span class="line"># *** WARNING *** if you are using a shared hosting the default of putting</span><br><span class="line"># the swap file under /tmp is not secure. Create a dir with access granted</span><br><span class="line"># only to Redis user and configure Redis to create the swap file there.</span><br><span class="line">vm-swap-file /tmp/redis.swap</span><br><span class="line"></span><br><span class="line"># With vm-max-memory 0 the system will swap everything it can. Not a good</span><br><span class="line"># default, just specify the max amount of RAM you can in bytes, but it&apos;s</span><br><span class="line"># better to leave some margin. For instance specify an amount of RAM</span><br><span class="line"># that&apos;s more or less between 60 and 80% of your free RAM.</span><br><span class="line"># 将所有大于vm-max-memory的数据存入虚拟内存，无论vm-max-memory设置多少，所有索引数据都是内存存储的（Redis的索引数据就是keys）</span><br><span class="line"># 也就是说当vm-max-memory设置为0的时候，其实是所有value都存在于磁盘。默认值为0</span><br><span class="line">vm-max-memory 0</span><br><span class="line"></span><br><span class="line"># Redis swap文件分成了很多的page，一个对象可以保存在多个page上面，但一个page上不能被多个对象共享，vm-page-size是要根据存储的数据大小来设定的。</span><br><span class="line"># 建议如果存储很多小对象，page大小最后设置为32或64bytes；如果存储很大的对象，则可以使用更大的page，如果不确定，就使用默认值</span><br><span class="line">vm-page-size 32</span><br><span class="line"></span><br><span class="line"># 设置swap文件中的page数量由于页表（一种表示页面空闲或使用的bitmap）是存放在内存中的，在磁盘上每8个pages将消耗1byte的内存</span><br><span class="line"># swap空间总容量为 vm-page-size * vm-pages</span><br><span class="line">#</span><br><span class="line"># With the default of 32-bytes memory pages and 134217728 pages Redis will</span><br><span class="line"># use a 4 GB swap file, that will use 16 MB of RAM for the page table.</span><br><span class="line">#</span><br><span class="line"># It&apos;s better to use the smallest acceptable value for your application,</span><br><span class="line"># but the default is large in order to work in most conditions.</span><br><span class="line">vm-pages 134217728</span><br><span class="line"></span><br><span class="line"># Max number of VM I/O threads running at the same time.</span><br><span class="line"># This threads are used to read/write data from/to swap file, since they</span><br><span class="line"># also encode and decode objects from disk to memory or the reverse, a bigger</span><br><span class="line"># number of threads can help with big objects even if they can&apos;t help with</span><br><span class="line"># I/O itself as the physical device may not be able to couple with many</span><br><span class="line"># reads/writes operations at the same time.</span><br><span class="line"># 设置访问swap文件的I/O线程数，最后不要超过机器的核数，如果设置为0，那么所有对swap文件的操作都是串行的，可能会造成比较长时间的延迟，默认值为4</span><br><span class="line">vm-max-threads 4</span><br><span class="line"></span><br><span class="line">############################### ADVANCED CONFIG ###############################</span><br><span class="line"></span><br><span class="line"># Hashes are encoded in a special way (much more memory efficient) when they</span><br><span class="line"># have at max a given numer of elements, and the biggest element does not</span><br><span class="line"># exceed a given threshold. You can configure this limits with the following</span><br><span class="line"># configuration directives.</span><br><span class="line"># 指定在超过一定的数量或者最大的元素超过某一临界值时，采用一种特殊的哈希算法</span><br><span class="line">hash-max-zipmap-entries 512</span><br><span class="line">hash-max-zipmap-value 64</span><br><span class="line"></span><br><span class="line"># Similarly to hashes, small lists are also encoded in a special way in order</span><br><span class="line"># to save a lot of space. The special representation is only used when</span><br><span class="line"># you are under the following limits:</span><br><span class="line">list-max-ziplist-entries 512</span><br><span class="line">list-max-ziplist-value 64</span><br><span class="line"></span><br><span class="line"># Sets have a special encoding in just one case: when a set is composed</span><br><span class="line"># of just strings that happens to be integers in radix 10 in the range</span><br><span class="line"># of 64 bit signed integers.</span><br><span class="line"># The following configuration setting sets the limit in the size of the</span><br><span class="line"># set in order to use this special memory saving encoding.</span><br><span class="line">set-max-intset-entries 512</span><br><span class="line"></span><br><span class="line"># Similarly to hashes and lists, sorted sets are also specially encoded in</span><br><span class="line"># order to save a lot of space. This encoding is only used when the length and</span><br><span class="line"># elements of a sorted set are below the following limits:</span><br><span class="line">zset-max-ziplist-entries 128</span><br><span class="line">zset-max-ziplist-value 64</span><br><span class="line"></span><br><span class="line"># Active rehashing uses 1 millisecond every 100 milliseconds of CPU time in</span><br><span class="line"># order to help rehashing the main Redis hash table (the one mapping top-level</span><br><span class="line"># keys to values). The hash table implementation redis uses (see dict.c)</span><br><span class="line"># performs a lazy rehashing: the more operation you run into an hash table</span><br><span class="line"># that is rhashing, the more rehashing &quot;steps&quot; are performed, so if the</span><br><span class="line"># server is idle the rehashing is never complete and some more memory is used</span><br><span class="line"># by the hash table.</span><br><span class="line"># </span><br><span class="line"># The default is to use this millisecond 10 times every second in order to</span><br><span class="line"># active rehashing the main dictionaries, freeing memory when possible.</span><br><span class="line">#</span><br><span class="line"># If unsure:</span><br><span class="line"># use &quot;activerehashing no&quot; if you have hard latency requirements and it is</span><br><span class="line"># not a good thing in your environment that Redis can reply form time to time</span><br><span class="line"># to queries with 2 milliseconds delay.</span><br><span class="line"># 指定是否激活重置哈希，默认为开启</span><br><span class="line">activerehashing yes</span><br><span class="line"></span><br><span class="line">################################## INCLUDES ###################################</span><br><span class="line"></span><br><span class="line"># 指定包含其他的配置文件，可以在同一主机上多个Redis实例之间使用同一份配置文件，而同时各实例又拥有自己的特定配置文件</span><br><span class="line"># include /path/to/local.conf</span><br><span class="line"># include /path/to/other.conf</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Wed Nov 21 2018 15:08:21 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;由于公司开发笔记本用的是mac，所以我这里介绍一下在mac和linux环境下的redis安装&lt;/p&gt;&lt;h2 id=&quot;Linu
      
    
    </summary>
    
    
      <category term="redis" scheme="http://www.fufan.me/tags/redis/"/>
    
  </entry>
  
  <entry>
    <title>Mac开发提升效率工具（一）</title>
    <link href="http://www.fufan.me/2018/02/09/Mac%E5%BC%80%E5%8F%91%E6%8F%90%E5%8D%87%E6%95%88%E7%8E%87%E5%B7%A5%E5%85%B7%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>http://www.fufan.me/2018/02/09/Mac开发提升效率工具（一）/</id>
    <published>2018-02-09T06:07:00.000Z</published>
    <updated>2018-11-09T06:17:17.315Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Nov 21 2018 15:08:21 GMT+0800 (China Standard Time) --><h3 id="Markdown编辑器"><a href="#Markdown编辑器" class="headerlink" title="Markdown编辑器"></a>Markdown编辑器</h3><p>目前可供选择的markdown编辑器至少以下有六种:</p><ol><li><a href="http://www.typora.io/" target="_blank" rel="noopener">Typora</a>, 简洁轻便免费, 独有的所见即所得, 可在预览状态下编辑, 快捷键丰富, 脚本高亮功能出彩, 导出为pdf后, 排版同样正常, 这点非常难得, 笔者使用的就是该款.</li><li><a href="http://www.ulyssesapp.com/" target="_blank" rel="noopener">Ulysses</a>, 功能强大, 快捷键丰富, 支持目录导入, 支持多终端同步.</li><li><a href="http://www.pc6.com/mac/161974.html" target="_blank" rel="noopener">MWeb Lite</a>, <a href="http://zh.mweb.im/" target="_blank" rel="noopener">MWeb</a>的微型版, 不收费, 支持目录导入.</li><li><a href="http://macdown.uranusjr.com/" target="_blank" rel="noopener">macdown</a>, 基于mou开发, 轻量, 不支持目录导入.</li><li><a href="http://25.io/mou/" target="_blank" rel="noopener">mou</a> 历史悠久, 据说有少量的bug, 具体请参考 <a href="http://www.jianshu.com/p/6c157af09e84" target="_blank" rel="noopener">Mac 下两款 Markdown 编辑器 Mou/MacDown 大 PK - 简书</a> .</li><li><a href="http://markeditor.com/app/markeditor" target="_blank" rel="noopener">markeditor</a>, 新出的markdown编辑器, 注重视觉感受, 界面不错, 但运行较慢.</li></ol><p>以上, 推荐开发使用 Typora, PM等使用 Ulysses.</p><h3 id="Mac重度依赖者"><a href="#Mac重度依赖者" class="headerlink" title="Mac重度依赖者"></a>Mac重度依赖者</h3><h4 id="开发工具"><a href="#开发工具" class="headerlink" title="开发工具"></a>开发工具</h4><ul><li><a href="https://www.charlesproxy.com/" target="_blank" rel="noopener">Charles</a> 网络封包分析应用, mac必备.</li><li><a href="http://www.trankynam.com/atext/" target="_blank" rel="noopener">aText</a> 输入增强应用, 比 <a href="http://www.pc6.com/mac/146924.html" target="_blank" rel="noopener"><code>TextExpander</code></a> 要人性化许多，并且对中文和第三方输入法的支持都要更好.</li><li><a href="https://kapeli.com/dash" target="_blank" rel="noopener">Dash</a> mac上api集合应用, 几乎包含各种语言的api文档.</li><li><a href="http://www.renfei.org/snippets-lab/" target="_blank" rel="noopener">SnippetsLab</a> 优秀的代码片段管理工具, 轻量, 可基于菜单栏操作.</li></ul><h4 id="提高效率"><a href="#提高效率" class="headerlink" title="提高效率"></a>提高效率</h4><ul><li><a href="http://www.sdifenzhou.com/alfred3.html" target="_blank" rel="noopener">Alfred 3</a> 神奇的魔法帽, 支持 ① 快速打开application; ② 支持Finder, Calculator, Contacts, Clipboard, iTunes, System, Terminal 等原生应用的各种便捷功能; ③ 支持workflow(工作流).</li><li><a href="http://www.iterm2.com/" target="_blank" rel="noopener">iterm2</a> 增强版的终端应用, 功能强大, 支持分屏, 历史记录, 选中即复制等.</li><li><a href="http://www.pc6.com/mac/124262.html" target="_blank" rel="noopener">Sip</a> 全屏取色应用, 支持快捷键调出(前端福音, 寻找多年, 终于发掘出来了).</li><li><a href="http://www.kekaosx.com/en/" target="_blank" rel="noopener">Keka</a> 压缩或解压缩应用, 开源免费, 压缩比高, 操作便捷, 支持rar等解压, 压缩中文目录后, 在windows下打开不会存在乱码等现象.</li><li><a href="https://github.com/oldj/SwitchHosts/releases" target="_blank" rel="noopener">SwitchHosts</a> 域名host解析必备神器, 支持 windows和Mac的开源工具, mac下只有几百K大小.</li><li><a href="http://pilotmoon.com/scrollreverser/" target="_blank" rel="noopener">Scroll Reverser</a> mac滚动方向自定义应用, 可分别设置鼠标和触摸板的上下左右的滚动效果.</li><li><a href="http://www.irradiatedsoftware.com/sizeup/" target="_blank" rel="noopener">Size up</a> 分屏应用, 类似Moon的一款应用, 支持上下左右居中、4个角落快速分屏及多屏幕切换.</li><li><a href="http://www.pc6.com/mac/124992.html" target="_blank" rel="noopener">Divvy</a> 另一款分屏应用, 可将屏幕分成多宫格的形式, 然后为每个格子定义快捷键, 遗憾的是不支持多屏幕切换.</li><li><a href="http://www.graphviz.org/" target="_blank" rel="noopener">Graphviz</a> 贝尔实验室开发的有向图/无向图自动布局应用, 支持dot脚本绘制结构图, 流程图等. 可参考教程 <a href="http://www.cnblogs.com/sld666666/archive/2010/06/25/1765510.html" target="_blank" rel="noopener">利用Graphviz 画结构图</a> 及 <a href="http://www.cnblogs.com/CoolJie/archive/2012/07/17/graphviz.html" target="_blank" rel="noopener">使用graphviz绘制流程图</a> .</li><li><a href="http://www.xmindchina.net/" target="_blank" rel="noopener">XMind</a> 思维导图应用, 适合业务及思路梳理.</li><li><a href="http://www.pc6.com/mac/129882.html" target="_blank" rel="noopener">iThoughtsX</a> 另一款思维导图应用, 更加简洁和轻量.</li><li><a href="http://www.pc6.com/mac/136806.html" target="_blank" rel="noopener">Pomodoro One</a> 番茄工作法的一款应用.</li></ul><h4 id="博主必备"><a href="#博主必备" class="headerlink" title="博主必备"></a>博主必备</h4><ul><li><a href="http://screenflow.en.softonic.com/mac" target="_blank" rel="noopener">ScreenFlow</a> 这或许是mac上最好用的屏幕录制应用.</li><li><a href="http://www.waitsun.com/annotate-2-0-5.html" target="_blank" rel="noopener">Annotate</a> 屏幕截图批注应用, 令人惊喜的是, 支持划区域gif制作, 教程以及动图制作者必备.</li><li><a href="http://www.cockos.com/licecap/" target="_blank" rel="noopener">Licecap</a> mac上超强大的且极简的gif录制应用, 使用免费, 支持FPS帧率调整且无录制时间限制(笔者用它录制了很多gif动图).</li><li><a href="http://mac.softpedia.com/get/Utilities/KeyCastr.shtml" target="_blank" rel="noopener">KeyCastr</a> 将mac按键显示在屏幕上，分享演示、录制视频或动图时超赞.</li></ul><h4 id="Mac定制化"><a href="#Mac定制化" class="headerlink" title="Mac定制化"></a>Mac定制化</h4><ul><li><a href="https://www.macbartender.com/" target="_blank" rel="noopener">Bartender 2</a> 菜单栏管理应用, 支持隐藏所有菜单栏图标, 还您一个干净的菜单栏.</li><li><a href="http://www.pc6.com/mac/161158.html" target="_blank" rel="noopener">CDock</a> 任务栏定制应用, 可设置Dock全透明, 还您一个清爽的任务栏.</li><li><a href="https://www.macstories.net/mac/textbar-puts-your-text-into-the-menu-bar/" target="_blank" rel="noopener">TextBar</a> 自定义菜单栏输出, 支持script运行, 支持H5渲染.</li><li><a href="http://growl.info/" target="_blank" rel="noopener">Growl</a> 自定义通知样式, 支持多种主题以及颜色, 大小, 渐隐时间等各项参数的自定义.</li><li><a href="https://pqrs.org/osx/karabiner/" target="_blank" rel="noopener">Karabiner</a> 键盘映射修改神器.</li><li><a href="https://www.keyboardmaestro.com/main/" target="_blank" rel="noopener">Keyboard Maestro</a> 键盘大师, mac下功能最为丰富的键盘增强应用.</li><li><a href="https://www.boastr.net/" target="_blank" rel="noopener">BetterTouchTool</a> mac触摸板增强神器.</li><li><a href="http://sspai.com/28020" target="_blank" rel="noopener">Übersicht</a> 华丽的桌面自定义应用, 类似于windows的 <a href="http://rainmeter.cn/cms/" target="_blank" rel="noopener"><code>rainmeter</code></a>. 支持H5.</li><li><a href="http://www.waerfa.com/today-scripts-for-yosemite-today-view" target="_blank" rel="noopener">Today Scripts</a> 个性化通知栏插件, 支持bash脚本(最新的OSX系统不支持).</li><li><a href="http://tweaksapp.com/app/mountain-tweaks/" target="_blank" rel="noopener">Mountain Tweaks</a> mac隐藏功能开启应用.</li></ul><h4 id="折腾党玩转Mac"><a href="#折腾党玩转Mac" class="headerlink" title="折腾党玩转Mac"></a>折腾党玩转Mac</h4><ul><li><a href="http://www.pc6.com/mac/144495.html" target="_blank" rel="noopener">TripMode</a> 移动热点流量管家, 出差达人的福音.</li><li><a href="http://www.pc6.com/mac/121734.html" target="_blank" rel="noopener">Caffeine</a> 点亮mac, 避免长时间演示ppt而进入到休眠状态.</li><li><a href="http://www.yingdev.com/projects/tickeys" target="_blank" rel="noopener">Tickeys</a> 键盘打字风格模拟应用, 支持 Cherry轴等多种风格.</li><li><a href="http://www.pc6.com/mac/116332.html" target="_blank" rel="noopener">keycue</a> 快捷键辅助应用, 帮助记忆快捷键.</li><li><a href="http://www.airserver.com/" target="_blank" rel="noopener">AirServer</a> IOS连接mac必备.</li><li><a href="http://www.beyondcompare.cc/" target="_blank" rel="noopener">Beyond Compare</a> 文件比较应用, 支持文件, 目录, FTP远程地址比较等.</li><li><a href="http://www.pc6.com/mac/129593.html" target="_blank" rel="noopener">Debookee</a> 网络抓包及数据分析应用.</li><li><a href="http://www.waerfa.com/easyfind" target="_blank" rel="noopener">EasyFind</a> 小而强大的文件搜索应用, 媲美windows下的Everything.</li><li><a href="https://filezilla-project.org/" target="_blank" rel="noopener">FileZilla</a> 免费开源的FTP应用.</li><li><a href="http://newping.cn/322" target="_blank" rel="noopener">OmniDiskSweeper</a> 硬盘空间扫描应用, 帮助mac减肥.</li><li><a href="http://www.pc6.com/mac/113361.html" target="_blank" rel="noopener">Kaleidoscope</a> 文件和图像比较应用, 支持图片比较, 能与 git, svn 等版本控制工具完美结合.</li><li><a href="http://freemacsoft.net/appcleaner/" target="_blank" rel="noopener">AppCleaner</a> mac应用卸载工具, 结合 <a href="https://github.com/Louiszhai/tool/blob/master/workflow/AppCleaner.alfredworkflow?raw=true" target="_blank" rel="noopener"><code>AppCleaner</code></a> 的workflow, 使用效果更佳.</li><li><a href="http://www.pc6.com/mac/115425.html" target="_blank" rel="noopener">TeamViewer</a> 远程开发或协助必备应用.</li><li><a href="http://www.pc6.com/mac/428096.html" target="_blank" rel="noopener">Script Debugger</a> 强大的AppleScript编辑器.</li><li><a href="http://www.pc6.com/mac/158839.html" target="_blank" rel="noopener">Reeder</a> 界面优美的RSS订阅应用.</li><li><a href="https://bahoom.com/hyperswitch" target="_blank" rel="noopener">HyperSwitch</a> 带有预览图的快速切换, 作用同Command+Tab.</li><li><a href="https://github.com/Swordfish90/cool-retro-term" target="_blank" rel="noopener">Cool retro term</a> 终端变身复古显示器.</li><li><a href="http://www.pc6.com/mac/119197.html" target="_blank" rel="noopener">Fruit Juice</a> 电池管理应用, 帮助延迟电池的使用时间.</li></ul><h3 id="终端命令"><a href="#终端命令" class="headerlink" title="终端命令"></a>终端命令</h3><ul><li><p><a href="http://ohmyz.sh/" target="_blank" rel="noopener">ohmyzsh</a> shell有很多种, 常用的bash就是之一. 而zsh是shell中目前最强大的, 没有之一. ohmyzsh屏蔽了zsh复杂的配置, 真正达到了一键上手zsh的目的.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Mac下自动安装&amp;设置</span></span><br><span class="line">wget https://github.com/robbyrussell/oh-my-zsh/raw/master/tools/install.sh -O - | sh</span><br><span class="line"><span class="comment"># 设置 shell 默认使用 zsh</span></span><br><span class="line">chsh -s /bin/zsh</span><br><span class="line"><span class="comment">#在 dock 栏右键退出终端, 然后重启终端~安装完成</span></span><br></pre></td></tr></table></figure></li><li><p><a href="https://github.com/wting/autojump" target="_blank" rel="noopener">autojump</a> 支持快速跳转到曾经打开过的目录下,安装方法: brew install autojump .</p></li><li><p><a href="http://tmux.github.io/" target="_blank" rel="noopener">tmux</a> 终端复用工具, 支持在终端中创建不依赖于终端的窗口, 安装方法: brew install tmux. 使用请参考：<a href="http://louiszhai.github.io/2017/09/30/tmux/" target="_blank" rel="noopener">Tmux使用手册</a>.</p></li></ul><h3 id="Chrome-Extension篇"><a href="#Chrome-Extension篇" class="headerlink" title="Chrome Extension篇"></a>Chrome Extension篇</h3><h4 id="自制"><a href="#自制" class="headerlink" title="自制"></a>自制</h4><ul><li><a href="https://github.com/Louiszhai/IHeader" target="_blank" rel="noopener">Iheader</a> 监听和修改http/https请求/响应头，可用于渗透测试（笔者修改请求头用于跨域调试，特别好用）。</li><li><a href="https://chrome.google.com/webstore/detail/qrcode/cmpjmgpafdgofigbhbneckneoakpdhag?utm_source=chrome-ntp-icon" target="_blank" rel="noopener">Qrcode</a> URL生成二维码，如果网页中包含选中文本，则生成选中文本的二维码。</li></ul><h4 id="前端有关"><a href="#前端有关" class="headerlink" title="前端有关"></a>前端有关</h4><ul><li><a href="https://chrome.google.com/webstore/detail/react-developer-tools/fmkadmapgofadopljbjfkapdkoienihi" target="_blank" rel="noopener">React Developer Tools</a> React开发者工具.</li><li><a href="https://chrome.google.com/webstore/detail/redux-devtools/lmhkpmbekcpmknklioeibfkpmmfibljd" target="_blank" rel="noopener">Redux DevTools</a> Redux开发者工具.</li><li><a href="https://chrome.google.com/webstore/detail/web%E5%89%8D%E7%AB%AF%E5%8A%A9%E6%89%8Bfehelper/pkgccpejnmalmdinmhkkfafefagiiiad0" target="_blank" rel="noopener">FE助手</a> 百度推出的前端助手, 具有很多便捷的小功能.</li><li><a href="https://chrome.google.com/webstore/detail/yslow/ninejjcohidippngpapiilnmkgllmakh" target="_blank" rel="noopener">YSlow</a> 雅虎性能分析工具.</li><li><a href="https://chrome.google.com/webstore/detail/postman/fhbjgbiflinjbdggehcddcbncdddomop" target="_blank" rel="noopener">Postman</a> 接口调试工具, 几乎支持所有类型的http(s)请求.</li><li><a href="https://chrome.google.com/webstore/detail/editthiscookie/fngmhnnpilhplaeedifhccceomclgfbg?utm_source=chrome-ntp-icon" target="_blank" rel="noopener">EditThisCookie</a> cookie编辑工具, 可用于获取或设置http only等cookie的值.</li><li><a href="https://chrome.google.com/webstore/detail/jsonview/chklaanhfefbnpoihckbnefhakgolnmc" target="_blank" rel="noopener">JSONView</a> json预览工具, 接口调试必备.</li><li><a href="https://chrome.google.com/webstore/detail/page-ruler/jlpkojjdgbllmedoapgfodplfhcbnbpn" target="_blank" rel="noopener">Page Ruler</a> 页面尺子, 页面重构或者严格按照设计图开发页面时, 将会非常有用.</li><li><a href="https://chrome.google.com/webstore/detail/alexa-traffic-rank/cknebhggccemgcnbidipinkifmmegdel" target="_blank" rel="noopener">Alexa Traffic Rank</a> 网站Alexa排名查看工具.</li></ul><h4 id="工作效率有关"><a href="#工作效率有关" class="headerlink" title="工作效率有关"></a>工作效率有关</h4><ul><li><a href="https://chrome.google.com/webstore/detail/onetab/chphlpgkkbolifaimnlloiipkdnihall" target="_blank" rel="noopener">OneTab</a> 快速关闭并存储浏览器当前窗口所有Tab页, 可用于下次一键全部恢复.</li><li><a href="https://chrome.google.com/webstore/detail/merge-windows/mmpokgfcmbkfdeibafoafkiijdbfblfg" target="_blank" rel="noopener">Merge Windows</a> 合并所有浏览器窗口为同一个窗口.</li><li><a href="https://chrome.google.com/webstore/detail/vimium/dbepggeogbaibhgnhhndojpepiihcmeb" target="_blank" rel="noopener">Vimium</a> 键盘党必备, 使用vim命令管理页面.</li><li><a href="https://chrome.google.com/webstore/detail/vysor/gidgenkbbabolejbgbpnhbimgjbffefm" target="_blank" rel="noopener">Vysor</a> mac上直接操作 Android 手机, 且可远程共享手机操作界面.</li></ul><h4 id="网站有关"><a href="#网站有关" class="headerlink" title="网站有关"></a>网站有关</h4><ul><li><a href="https://chrome.google.com/webstore/detail/octotree/bkhaagjahfmjljalopjnoealnfndnagc" target="_blank" rel="noopener">Octotree</a> Github重度依赖者必备, 提供左侧边栏, 快速浏览仓库内容.</li><li><a href="https://chrome.google.com/webstore/detail/adblock/gighmmpiobklfepjocnamgkkbiglidom" target="_blank" rel="noopener">AdBlock</a> 超强去广告工具, 最受欢迎的Chrome扩展, 拥有超过4000万用户.</li><li><a href="https://chrome.google.com/webstore/detail/reader-view/iibolhpkjjmoepndefdmdlmbpfhlgjpl" target="_blank" rel="noopener">阅读模式</a> 快速开启阅读模式, 进入沉浸式阅读, 并非支持所有网页.</li><li><a href="https://chrome.google.com/webstore/detail/blipshot-one-click-full-p/mdaboflcmhejfihjcbmdiebgfchigjcf?utm_source=chrome-ntp-icon" target="_blank" rel="noopener">Blipshot</a> 全网页截图工具, 支持自动垂直滚动, 截取网页的所有内容为一张图片.</li></ul><h4 id="Chrome-Extension开发"><a href="#Chrome-Extension开发" class="headerlink" title="Chrome Extension开发"></a>Chrome Extension开发</h4><p>相关文章</p><ul><li><a href="https://developer.chrome.com/extensions/samples" target="_blank" rel="noopener">Sample Extensions - Google Chrome</a></li><li><a href="http://www.ituring.com.cn/minibook/950" target="_blank" rel="noopener">图灵社区: 合集 : Chrome扩展及应用开发</a></li><li><a href="http://www.cnblogs.com/champagne/tag/Google%20Chrome%E6%89%A9%E5%B1%95/" target="_blank" rel="noopener">Google Chrome扩展开发系列</a></li></ul><h3 id="Alfred-workflow"><a href="#Alfred-workflow" class="headerlink" title="Alfred workflow"></a>Alfred workflow</h3><p>我曾经耗费巨大的精力, 试图在计算机的使用效率上找到一条优化的策略, 一直以来都收效甚微. 直到遇上Alfred, 它强大的工作流机制, 才让我明白原来计算机可以这么玩. 因为<strong>它彻底解决了输入输出的痛点, 极大的减少了程序之间的切换成本以及按键成本</strong>.</p><p>传统意义上, 使用mac时, 为了查询一个单词, 或者翻译一个单词, 我们要么经历五步: ① 手动打开浏览器 ② 进入谷歌首页 ③ 选中输入框 ④ 输入或粘贴查询单词, 然后空格并加上”翻译” 两个字, 然后再回车 ⑤ 等待浏览器展示查询结果; 要么经历四步: ① 打开翻译应用(比如自带词典) ② 输入或粘贴查询单词 ③ 翻译应用输出查询结果 ④ 查询过后, 一般都需要Command+Q退出应用(否则Dock栏将会全是未关闭的应用).</p><p>查询单词这个场景中, 我们至少需要兴师动众, 切换或打开一个应用两次, 定位输入框一次, 输入或复制粘贴一次. 且查询结果页也会挡住当前的工作区, 使得我们分心, 甚至忘记自己刚刚在做啥. 五个字 — 体验不流畅.</p><p>而 Alfred 的工作流正是为了解决这个问题而设计的. 如果我们使用网友开发的 <a href="https://github.com/Louiszhai/tool/blob/master/workflow/youdao.alfredworkflow?raw=true" target="_blank" rel="noopener"><code>有道词典</code></a> 的 workflow, <strong>最快只需通过两次按键便可获取单词的查询结果</strong>. 假如: 为了查询单词”workflow”, 我会选中单词所在区域, 然后按住 Option+Y 键(我已将有道翻译的快捷键设置为 Option+Y), 单词查询结果就出来了, 而且不需要切换应用, 同时查询结果也较少的挡住工作区了.</p><p>以上 Alfred 界面使用了少数派的主题.</p><p>有关其他的workflow 内容, 请移步 <a href="https://github.com/Louiszhai/alfred-workflows" target="_blank" rel="noopener"><code>Alfred Workflows</code></a> , 那里会有更多非常不错的 workflow 供您选用.</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Wed Nov 21 2018 15:08:21 GMT+0800 (China Standard Time) --&gt;&lt;h3 id=&quot;Markdown编辑器&quot;&gt;&lt;a href=&quot;#Markdown编辑器&quot; class=&quot;headerlink&quot; ti
      
    
    </summary>
    
      <category term="mac" scheme="http://www.fufan.me/categories/mac/"/>
    
    
      <category term="mac" scheme="http://www.fufan.me/tags/mac/"/>
    
      <category term="工具" scheme="http://www.fufan.me/tags/%E5%B7%A5%E5%85%B7/"/>
    
  </entry>
  
  <entry>
    <title>Redis学习笔记（一）——初识redis</title>
    <link href="http://www.fufan.me/2018/02/07/Redis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94%E5%88%9D%E8%AF%86redis/"/>
    <id>http://www.fufan.me/2018/02/07/Redis学习笔记（一）——初识redis/</id>
    <published>2018-02-07T13:02:00.000Z</published>
    <updated>2018-11-08T10:11:15.679Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Nov 21 2018 15:08:20 GMT+0800 (China Standard Time) --><p>由于加入公司一段时间了，公司的爬虫项目太过于依赖redis这个中间件，包括线上的几次重大异常都是由redis来引起的（redis分布式锁问题，阿里云redis主从和分布式服务异常问题等）。所以找时间专门学习一下关于redis的一些使用。</p><p>Redis学习链接：<br><a href="http://www.runoob.com/redis/redis-tutorial.html" target="_blank" rel="noopener">菜鸟redis教程</a><br><a href="http://www.redis.net.cn/tutorial/3501.html" target="_blank" rel="noopener">官网教程</a></p><p><img src="/image/redis-0-0.png" alt=""></p><h2 id="Redis简介"><a href="#Redis简介" class="headerlink" title="Redis简介"></a>Redis简介</h2><p>Redis是一个速度极快的非关系数据库，也就是我们所说的NoSQL数据库(non-relational database)，它可以存储键(key)与5种不同类型的值(value)之间的映射(mapping)，可以将存储在内存的键值对数据持久化到硬盘，可以使用复制特性来扩展读性能，还可以使用客户端分片来扩展性能，并且它还提供了多种语言的API。</p><ul><li>Redis 是完全开源免费的，遵守BSD协议，是一个高性能的key-value数据库。</li><li>Redis支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用。</li><li>Redis支持数据的备份，即master-slave模式的数据备份。</li></ul><h2 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h2><ul><li>性能极高： Redis能读的速度是110000次/s,写的速度是81000次/s</li><li>丰富的数据类型：Redis支持二进制案例的 Strings, Lists, Hashes, Sets 及 Ordered Sets 数据类型操作。</li><li>原子性：Redis的所有操作都是原子性的，同时Redis还支持对几个操作全并后的原子性执行。</li><li>丰富的特性：支持 publish/subscribe, 通知, key 过期等等特性。</li><li>分布式锁：很多分布式系统中可以用redis的setnx和getset来做分布式锁</li></ul><h2 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h2><h3 id="字符串-String"><a href="#字符串-String" class="headerlink" title="字符串 - String"></a>字符串 - String</h3><ul><li>string类型是二进制安全的。意思是redis的string可以包含任何数据。比如jpg图片或者序列化的对象 。</li><li>string类型是Redis最基本的数据类型，一个键最大能存储512MB。</li></ul><p>api示例：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:6379&gt; set username fufan</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; get username</span><br><span class="line">&quot;fufan&quot;</span><br><span class="line">127.0.0.1:6379&gt;</span><br></pre></td></tr></table></figure><p></p><h3 id="哈希-hash"><a href="#哈希-hash" class="headerlink" title="哈希 - hash"></a>哈希 - hash</h3><ul><li>Redis hash 是一个键值对集合。</li><li>Redis hash是一个string类型的field和value的映射表，hash特别适合用于存储对象。</li><li>每个 hash 可以存储 232 - 1 键值对（40多亿）。</li></ul><p>api示例：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:6379&gt; HSET user name fufan password 121314</span><br><span class="line">(integer) 2</span><br><span class="line">127.0.0.1:6379&gt; HGETALL user</span><br><span class="line">1) &quot;name&quot;</span><br><span class="line">2) &quot;fufan&quot;</span><br><span class="line">3) &quot;password&quot;</span><br><span class="line">4) &quot;121314&quot;</span><br></pre></td></tr></table></figure><p></p><h3 id="列表-list"><a href="#列表-list" class="headerlink" title="列表 - list"></a>列表 - list</h3><ul><li>Redis 列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素导列表的头部（左边）或者尾部（右边）。</li><li>列表最多可存储 2^32 - 1 元素 (4294967295, 每个列表可存储40多亿)。</li></ul><p>api示例：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:6379&gt; LPUSH userlist fufan</span><br><span class="line">(integer) 1</span><br><span class="line">127.0.0.1:6379&gt; LPUSH userlist yajun</span><br><span class="line">(integer) 2</span><br><span class="line">127.0.0.1:6379&gt; LPUSH userlist luwei</span><br><span class="line">(integer) 3</span><br><span class="line">127.0.0.1:6379&gt; LPOP userlist</span><br><span class="line">&quot;luwei&quot;</span><br><span class="line">127.0.0.1:6379&gt; RPOP userlist</span><br><span class="line">&quot;fufan&quot;</span><br><span class="line">127.0.0.1:6379&gt;</span><br></pre></td></tr></table></figure><p></p><h3 id="集合-set"><a href="#集合-set" class="headerlink" title="集合 - set"></a>集合 - set</h3><ul><li>Redis的Set是string类型的无序集合。</li><li>集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是O(1)</li><li>根据集合内元素的唯一性，第二次插入的元素将被忽略。</li><li>集合中最大的成员数为 2^32 -1(4294967295,每个集合可存储40多亿个成员)<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:6379&gt; sadd product phone</span><br><span class="line">(integer) 1</span><br><span class="line">127.0.0.1:6379&gt; sadd product pad</span><br><span class="line">(integer) 1</span><br><span class="line">127.0.0.1:6379&gt; sadd product tv</span><br><span class="line">(integer) 1</span><br><span class="line">127.0.0.1:6379&gt; SMEMBERS product</span><br><span class="line">1) &quot;tv&quot;</span><br><span class="line">2) &quot;phone&quot;</span><br><span class="line">3) &quot;pad&quot;</span><br></pre></td></tr></table></figure></li></ul><h3 id="有序集合-sorted-set"><a href="#有序集合-sorted-set" class="headerlink" title="有序集合 - sorted set"></a>有序集合 - sorted set</h3><ul><li>Redis zset 和 set一样也是string类型元素的集合,且不允许重复的成员。</li><li>不同的是每个元素都会关联一个double类型的分数。redis正是通过分数来为集合中的成员进行从小到大的排序。</li><li>zset的成员是唯一的,但分数(score)却可以重复。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:6379&gt; ZADD fufan 100 chinese</span><br><span class="line">(integer) 1</span><br><span class="line">127.0.0.1:6379&gt; ZADD fufan 100 math</span><br><span class="line">(integer) 1</span><br><span class="line">127.0.0.1:6379&gt; ZADD fufan 135 english</span><br><span class="line">(integer) 1</span><br><span class="line">127.0.0.1:6379&gt; ZRANGE fufan 0 10 withscores</span><br><span class="line">1) &quot;chinese&quot;</span><br><span class="line">2) &quot;100&quot;</span><br><span class="line">3) &quot;math&quot;</span><br><span class="line">4) &quot;100&quot;</span><br><span class="line">5) &quot;english&quot;</span><br><span class="line">6) &quot;135&quot;</span><br><span class="line">127.0.0.1:6379&gt; ZRANGEBYSCORE fufan 100 100</span><br><span class="line">1) &quot;chinese&quot;</span><br><span class="line">2) &quot;math&quot;</span><br><span class="line">127.0.0.1:6379&gt;</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Wed Nov 21 2018 15:08:20 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;由于加入公司一段时间了，公司的爬虫项目太过于依赖redis这个中间件，包括线上的几次重大异常都是由redis来引起的（red
      
    
    </summary>
    
      <category term="redis" scheme="http://www.fufan.me/categories/redis/"/>
    
    
      <category term="redis" scheme="http://www.fufan.me/tags/redis/"/>
    
  </entry>
  
  <entry>
    <title>JVM专题（三）——GC算法 垃圾收集器</title>
    <link href="http://www.fufan.me/2017/11/21/JVM%E4%B8%93%E9%A2%98%EF%BC%88%E4%B8%89%EF%BC%89%E2%80%94%E2%80%94GC%E7%AE%97%E6%B3%95-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8/"/>
    <id>http://www.fufan.me/2017/11/21/JVM专题（三）——GC算法-垃圾收集器/</id>
    <published>2017-11-20T16:30:00.000Z</published>
    <updated>2018-11-08T10:38:47.127Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Nov 21 2018 15:08:21 GMT+0800 (China Standard Time) --><p>这篇文件将给大家介绍GC都有哪几种算法，以及JVM都有那些垃圾回收器，它们的工作原理。</p><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>垃圾收集 Garbage Collection 通常被称为“GC”，它诞生于1960年 MIT 的 Lisp 语言，经过半个多世纪，目前已经十分成熟了。 jvm 中，程序计数器、虚拟机栈、本地方法栈都是随线程而生随线程而灭，栈帧随着方法的进入和退出做入栈和出栈操作，实现了自动的内存清理，因此，我们的内存垃圾回收主要集中于 java 堆和方法区中，在程序运行期间，这部分内存的分配和使用都是动态的.</p><h2 id="对象存活判断"><a href="#对象存活判断" class="headerlink" title="对象存活判断"></a>对象存活判断</h2><p>判断对象是否存活一般有两种方式：</p><p>引用计数：每个对象有一个引用计数属性，新增一个引用时计数加1，引用释放时计数减1，计数为0时可以回收。此方法简单，无法解决对象相互循环引用的问题。<br>可达性分析（Reachability Analysis）：从GC Roots开始向下搜索，搜索所走过的路径称为引用链。当一个对象到GC Roots没有任何引用链相连时，则证明此对象是不可用的。不可达对象。</p><p>在Java语言中，GC Roots包括：</p><ul><li>虚拟机栈中引用的对象。</li><li>方法区中类静态属性实体引用的对象。</li><li>方法区中常量引用的对象。</li><li>本地方法栈中JNI引用的对象。</li></ul><h2 id="垃圾收集算法"><a href="#垃圾收集算法" class="headerlink" title="垃圾收集算法"></a>垃圾收集算法</h2><h3 id="标记-清除算法"><a href="#标记-清除算法" class="headerlink" title="标记 -清除算法"></a>标记 -清除算法</h3><p>“标记-清除”（Mark-Sweep）算法，如它的名字一样，算法分为“标记”和“清除”两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收掉所有被标记的对象。之所以说它是最基础的收集算法，是因为后续的收集算法都是基于这种思路并对其缺点进行改进而得到的。</p><p>它的主要缺点有两个：一个是效率问题，标记和清除过程的效率都不高；另外一个是空间问题，标记清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致，当程序在以后的运行过程中需要分配较大对象时无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。</p><p><img src="/image/jvm-3-0.png" alt=""></p><h3 id="复制算法"><a href="#复制算法" class="headerlink" title="复制算法"></a>复制算法</h3><p>“复制”（Copying）的收集算法，它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。</p><p>这样使得每次都是对其中的一块进行内存回收，内存分配时也就不用考虑内存碎片等复杂情况，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效。只是这种算法的代价是将内存缩小为原来的一半，持续复制长生存期的对象则导致效率降低。<br><img src="/image/jvm-3-1.png" alt=""></p><h3 id="标记-压缩算法"><a href="#标记-压缩算法" class="headerlink" title="标记-压缩算法"></a>标记-压缩算法</h3><p>复制收集算法在对象存活率较高时就要执行较多的复制操作，效率将会变低。更关键的是，如果不想浪费50%的空间，就需要有额外的空间进行分配担保，以应对被使用的内存中所有对象都100%存活的极端情况，所以在老年代一般不能直接选用这种算法。</p><p>根据老年代的特点，有人提出了另外一种“标记-整理”（Mark-Compact）算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存<br><img src="/image/jvm-3-2.png" alt=""></p><h3 id="分代收集算法"><a href="#分代收集算法" class="headerlink" title="分代收集算法"></a>分代收集算法</h3><p>GC分代的基本假设：绝大部分对象的生命周期都非常短暂，存活时间短。</p><p>“分代收集”（Generational Collection）算法，把Java堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。在新生代中，每次垃圾收集时都发现有大批对象死去，只有少量存活，那就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集。而老年代中因为对象存活率高、没有额外空间对它进行分配担保，就必须使用“标记-清理”或“标记-整理”算法来进行回收。</p><h3 id="垃圾收集器"><a href="#垃圾收集器" class="headerlink" title="垃圾收集器"></a>垃圾收集器</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">如果说收集算法是内存回收的方法论，垃圾收集器就是内存回收的具体实现</span><br></pre></td></tr></table></figure><h3 id="Serial收集器"><a href="#Serial收集器" class="headerlink" title="Serial收集器"></a>Serial收集器</h3><p>串行收集器是最古老，最稳定以及效率高的收集器，可能会产生较长的停顿，只使用一个线程去回收。新生代、老年代使用串行回收；新生代复制算法、老年代标记-压缩；垃圾收集的过程中会Stop The World（服务暂停）</p><p>参数控制：-XX:+UseSerialGC 串行收集器<br><img src="/image/jvm-3-3.png" alt=""></p><h3 id="ParNew收集器"><a href="#ParNew收集器" class="headerlink" title="ParNew收集器"></a>ParNew收集器</h3><p>ParNew收集器其实就是Serial收集器的多线程版本。新生代并行，老年代串行；新生代复制算法、老年代标记-压缩</p><p>参数控制：</p><p>-XX:+UseParNewGC ParNew收集器<br>-XX:ParallelGCThreads 限制线程数量</p><p><img src="/image/jvm-3-4.png" alt=""></p><h3 id="Parallel收集器"><a href="#Parallel收集器" class="headerlink" title="Parallel收集器"></a>Parallel收集器</h3><p>Parallel Scavenge收集器类似ParNew收集器，Parallel收集器更关注系统的吞吐量。可以通过参数来打开自适应调节策略，虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或最大的吞吐量；也可以通过参数控制GC的时间不大于多少毫秒或者比例；新生代复制算法、老年代标记-压缩</p><p>参数控制：-XX:+UseParallelGC 使用Parallel收集器+ 老年代串行</p><h3 id="Parallel-Old-收集器"><a href="#Parallel-Old-收集器" class="headerlink" title="Parallel Old 收集器"></a>Parallel Old 收集器</h3><p>Parallel Old是Parallel Scavenge收集器的老年代版本，使用多线程和“标记－整理”算法。这个收集器是在JDK 1.6中才开始提供</p><p>参数控制： -XX:+UseParallelOldGC 使用Parallel收集器+ 老年代并行</p><h3 id="CMS收集器"><a href="#CMS收集器" class="headerlink" title="CMS收集器"></a>CMS收集器</h3><p>CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。目前很大一部分的Java应用都集中在互联网站或B/S系统的服务端上，这类应用尤其重视服务的响应速度，希望系统停顿时间最短，以给用户带来较好的体验。</p><p>从名字（包含“Mark Sweep”）上就可以看出CMS收集器是基于“标记-清除”算法实现的，它的运作过程相对于前面几种收集器来说要更复杂一些，整个过程分为4个步骤，包括：</p><ul><li>初始标记（CMS initial mark）</li><li>并发标记（CMS concurrent mark）</li><li>重新标记（CMS remark）</li><li>并发清除（CMS concurrent sweep）</li></ul><p>其中初始标记、重新标记这两个步骤仍然需要“Stop The World”。初始标记仅仅只是标记一下GC Roots能直接关联到的对象，速度很快，并发标记阶段就是进行GC Roots Tracing的过程，而重新标记阶段则是为了修正并发标记期间，因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段稍长一些，但远比并发标记的时间短。</p><p>由于整个过程中耗时最长的并发标记和并发清除过程中，收集器线程都可以与用户线程一起工作，所以总体上来说，CMS收集器的内存回收过程是与用户线程一起并发地执行。老年代收集器（新生代使用ParNew）</p><p>优点: 并发收集、低停顿<br>缺点: 产生大量空间碎片、并发阶段会降低吞吐量</p><p>参数控制：</p><ul><li>-XX:+UseConcMarkSweepGC 使用CMS收集器</li><li>-XX:+ UseCMSCompactAtFullCollection Full GC后，进行一次碎片整理；整理过程是独占的，会引起停顿时间变长</li><li>-XX:+CMSFullGCsBeforeCompaction 设置进行几次Full GC后，进行一次碎片整理</li><li>-XX:ParallelCMSThreads 设定CMS的线程数量（一般情况约等于可用CPU数量）</li></ul><p><img src="/image/jvm-3-5.png" alt=""></p><h3 id="G1收集器"><a href="#G1收集器" class="headerlink" title="G1收集器"></a>G1收集器</h3><p>G1是目前技术发展的最前沿成果之一，HotSpot开发团队赋予它的使命是未来可以替换掉JDK1.5中发布的CMS收集器。与CMS收集器相比G1收集器有以下特点：</p><ol><li>空间整合，G1收集器采用标记整理算法，不会产生内存空间碎片。分配大对象时不会因为无法找到连续空间而提前触发下一次GC。</li><li>可预测停顿，这是G1的另一大优势，降低停顿时间是G1和CMS的共同关注点，但G1除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为N毫秒的时间片段内，消耗在垃圾收集上的时间不得超过N毫秒，这几乎已经是实时Java（RTSJ）的垃圾收集器的特征了。</li></ol><p><img src="/image/jvm-3-6.png" alt=""></p><p>上面提到的垃圾收集器，收集的范围都是整个新生代或者老年代，而G1不再是这样。使用G1收集器时，Java堆的内存布局与其他收集器有很大差别，它将整个Java堆划分为多个大小相等的独立区域（Region），虽然还保留有新生代和老年代的概念，但新生代和老年代不再是物理隔阂了，它们都是一部分（可以不连续）Region的集合。</p><p>G1的新生代收集跟ParNew类似，当新生代占用达到一定比例的时候，开始出发收集。和CMS类似，G1收集器收集老年代对象会有短暂停顿。</p><p>收集步骤：</p><ul><li>1、标记阶段，首先初始标记(Initial-Mark),这个阶段是停顿的(Stop the World Event)，并且会触发一次普通Mintor GC。对应GC log:GC pause (young) (inital-mark)</li><li>2、Root Region Scanning，程序运行过程中会回收survivor区(存活到老年代)，这一过程必须在young GC之前完成。</li><li>3、Concurrent Marking，在整个堆中进行并发标记(和应用程序并发执行)，此过程可能被young GC中断。在并发标记阶段，若发现区域对象中的所有对象都是垃圾，那个这个区域会被立即回收(图中打X)。同时，并发标记过程中，会计算每个区域的对象活性(区域中存活对象的比例)。<br><img src="/image/jvm-3-7.png" alt=""></li><li>4、Remark, 再标记，会有短暂停顿(STW)。再标记阶段是用来收集 并发标记阶段 产生新的垃圾(并发阶段和应用程序一同运行)；G1中采用了比CMS更快的初始快照算法:snapshot-at-the-beginning (SATB)。</li><li>5、Copy/Clean up，多线程清除失活对象，会有STW。G1将回收区域的存活对象拷贝到新区域，清除Remember Sets，并发清空回收区域并把它返回到空闲区域链表中。</li></ul><p><img src="/image/jvm-3-8.png" alt=""></p><p>6、复制/清除过程后。回收区域的活性对象已经被集中回收到深蓝色和深绿色区域。</p><p><img src="/image/jvm-3-9.png" alt=""></p><h3 id="常用的收集器组合"><a href="#常用的收集器组合" class="headerlink" title="常用的收集器组合"></a>常用的收集器组合</h3><table><thead><tr><th>服务器</th><th>新生代GC策略</th><th>老年老代GC策略</th><th>说明</th></tr></thead><tbody><tr><td>组合1</td><td>Serial</td><td>Serial Old</td><td>Serial和Serial Old都是单线程进行GC，特点就是GC时暂停所有应用线程。</td></tr><tr><td>组合2</td><td>Serial</td><td>CMS+Serial Old</td><td>CMS（Concurrent Mark Sweep）是并发GC，实现GC线程和应用线程并发工作，不需要暂停所有应用线程。另外，当CMS进行GC失败时，会自动使用Serial Old策略进行GC。</td></tr><tr><td>组合3</td><td>ParNew</td><td>CMS</td><td>使用-XX:+UseParNewGC选项来开启。ParNew是Serial的并行版本，可以指定GC线程数，默认GC线程数为CPU的数量。可以使用-XX:ParallelGCThreads选项指定GC的线程数。如果指定了选项-XX:+UseConcMarkSweepGC选项，则新生代默认使用ParNew GC策略。</td></tr><tr><td>组合4</td><td>ParNew</td><td>Serial Old</td><td>使用-XX:+UseParNewGC选项来开启。新生代使用ParNew GC策略，年老代默认使用Serial Old GC策略。</td></tr><tr><td>组合5</td><td>Parallel Scavenge</td><td>Serial Old</td><td>Parallel Scavenge策略主要是关注一个可控的吞吐量：应用程序运行时间 / (应用程序运行时间 + GC时间)，可见这会使得CPU的利用率尽可能的高，适用于后台持久运行的应用程序，而不适用于交互较多的应用程序。</td></tr><tr><td>组合6</td><td>Parallel Scavenge</td><td>Parallel Old</td><td>Parallel Old是Serial Old的并行版本</td></tr><tr><td>组合7</td><td>G1GC</td><td>G1GC</td><td>-XX:+UnlockExperimentalVMOptions -XX:+UseG1GC #开启；-XX:MaxGCPauseMillis =50 #暂停时间目标；-XX:GCPauseIntervalMillis =200 #暂停间隔目标；-XX:+G1YoungGenSize=512m #年轻代大小；-XX:SurvivorRatio=6 #幸存区比例</td></tr></tbody></table><h3 id="系统吞吐量和系统并发数以及响时间的关系理解"><a href="#系统吞吐量和系统并发数以及响时间的关系理解" class="headerlink" title="系统吞吐量和系统并发数以及响时间的关系理解"></a>系统吞吐量和系统并发数以及响时间的关系理解</h3><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Wed Nov 21 2018 15:08:21 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;这篇文件将给大家介绍GC都有哪几种算法，以及JVM都有那些垃圾回收器，它们的工作原理。&lt;/p&gt;&lt;h2 id=&quot;概述&quot;&gt;&lt;a 
      
    
    </summary>
    
      <category term="java虚拟机" scheme="http://www.fufan.me/categories/java%E8%99%9A%E6%8B%9F%E6%9C%BA/"/>
    
    
      <category term="java虚拟机" scheme="http://www.fufan.me/tags/java%E8%99%9A%E6%8B%9F%E6%9C%BA/"/>
    
  </entry>
  
</feed>
